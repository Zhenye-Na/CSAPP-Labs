combine1:
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbp
	movq	%rsi, %rbp
	pushq	%rbx
	xorl	%ebx, %ebx
	subq	$16, %rsp
	movl	$0, (%rsi)
	jmp	.L2
.L3:
	leaq	12(%rsp), %rdx
	movq	%rbx, %rsi
	movq	%r12, %rdi
	addq	$1, %rbx
	call	get_vec_element
	movl	12(%rsp), %eax
	addl	%eax, 0(%rbp)
.L2:
	movq	%r12, %rdi
	call	vec_length
	cmpq	%rax, %rbx
	jl	.L3
	addq	$16, %rsp
	popq	%rbx
	popq	%rbp
	popq	%r12
	ret

combine2:
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbp
	movq	%rsi, %rbp
	pushq	%rbx
	xorl	%ebx, %ebx
	subq	$24, %rsp
	call	vec_length
	testq	%rax, %rax
	movq	%rax, %r12
	movl	$0, 0(%rbp)
	jle	.L11
.L10:
	leaq	12(%rsp), %rdx
	movq	%rbx, %rsi
	movq	%r13, %rdi
	addq	$1, %rbx
	call	get_vec_element
	movl	12(%rsp), %edx
	addl	%edx, 0(%rbp)
	cmpq	%r12, %rbx
	jne	.L10
.L11:
	addq	$24, %rsp
	popq	%rbx
	popq	%rbp
	popq	%r12
	popq	%r13
	ret

combine4b:
	pushq	%rbp
	movq	%rsi, %rbp
	pushq	%rbx
	movq	%rdi, %rbx
	subq	$8, %rsp
	call	vec_length
	xorl	%ecx, %ecx
	testq	%rax, %rax
	jle	.L14
	xorl	%edx, %edx
.L16:
	cmpq	%rdx, (%rbx)
	jle	.L15
	movq	8(%rbx), %rdi
	addl	(%rdi,%rdx,4), %ecx
.L15:
	addq	$1, %rdx
	cmpq	%rax, %rdx
	jne	.L16
.L14:
	movl	%ecx, 0(%rbp)
	addq	$8, %rsp
	popq	%rbx
	popq	%rbp
	ret

combine3:
	pushq	%r12
	pushq	%rbp
	movq	%rdi, %rbp
	pushq	%rbx
	movq	%rsi, %rbx
	call	vec_length
	movq	%rbp, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%r12, %r12
	movl	$0, (%rbx)
	jle	.L23
	movq	%rax, %rdx
	leaq	(%rax,%r12,4), %rcx
	xorl	%eax, %eax
.L22:
	addl	(%rdx), %eax
	addq	$4, %rdx
	cmpq	%rcx, %rdx
	movl	%eax, (%rbx)
	jne	.L22
.L23:
	popq	%rbx
	popq	%rbp
	popq	%r12
	ret

combine3w:
	pushq	%r12
	pushq	%rbp
	movq	%rdi, %rbp
	pushq	%rbx
	movq	%rsi, %rbx
	call	vec_length
	movq	%rbp, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%r12, %r12
	movl	$0, (%rbx)
	jle	.L29
	movq	%rax, %rdx
	leaq	(%rax,%r12,4), %rcx
	xorl	%eax, %eax
.L28:
	addl	(%rdx), %eax
	addq	$4, %rdx
	cmpq	%rcx, %rdx
	movl	%eax, (%rbx)
	jne	.L28
.L29:
	popq	%rbx
	popq	%rbp
	popq	%r12
	ret

combine4:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%rbx, %rbx
	jle	.L44
	movq	%rax, %rcx
	andl	$31, %eax
	movq	%rbx, %rdi
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpq	%rbx, %rax
	cmova	%rbx, %rax
	cmpq	$17, %rbx
	ja	.L69
.L33:
	cmpq	$1, %rdi
	movl	(%rcx), %edx
	jbe	.L47
	addl	4(%rcx), %edx
	cmpq	$2, %rdi
	jbe	.L48
	addl	8(%rcx), %edx
	cmpq	$3, %rdi
	jbe	.L49
	addl	12(%rcx), %edx
	cmpq	$4, %rdi
	jbe	.L50
	addl	16(%rcx), %edx
	cmpq	$5, %rdi
	jbe	.L51
	addl	20(%rcx), %edx
	cmpq	$6, %rdi
	jbe	.L52
	addl	24(%rcx), %edx
	cmpq	$7, %rdi
	jbe	.L53
	addl	28(%rcx), %edx
	cmpq	$8, %rdi
	jbe	.L54
	addl	32(%rcx), %edx
	cmpq	$9, %rdi
	jbe	.L55
	movl	36(%rcx), %eax
	addl	%edx, %eax
	cmpq	$10, %rdi
	movl	%eax, %edx
	jbe	.L56
	addl	40(%rcx), %eax
	cmpq	$11, %rdi
	movl	%eax, %edx
	jbe	.L57
	addl	44(%rcx), %edx
	cmpq	$12, %rdi
	jbe	.L58
	addl	48(%rcx), %edx
	cmpq	$13, %rdi
	jbe	.L59
	addl	52(%rcx), %edx
	cmpq	$14, %rdi
	jbe	.L60
	addl	56(%rcx), %edx
	cmpq	$15, %rdi
	jbe	.L61
	addl	60(%rcx), %edx
	cmpq	$16, %rdi
	jbe	.L62
	addl	64(%rcx), %edx
	movl	$17, %eax
.L35:
	cmpq	%rdi, %rbx
	je	.L32
.L34:
	movq	%rbx, %rsi
	subq	%rdi, %rsi
	movq	%rsi, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L37
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %r8
	xorl	%edi, %edi
.L43:
	addq	$1, %rdi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rdi, %r10
	ja	.L43
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %edi
	addl	%edi, %edx
	cmpq	%r9, %rsi
	je	.L67
	vzeroupper
.L37:
	leaq	1(%rax), %rsi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rsi, %rbx
	jle	.L32
	addl	(%rcx,%rsi,4), %edx
	leaq	2(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L32
	addl	(%rcx,%rsi,4), %edx
	leaq	3(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L32
	addl	(%rcx,%rsi,4), %edx
	leaq	4(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L32
	addl	(%rcx,%rsi,4), %edx
	leaq	5(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L32
	addq	$6, %rax
	addl	(%rcx,%rsi,4), %edx
	cmpq	%rax, %rbx
	jle	.L32
	addl	(%rcx,%rax,4), %edx
.L32:
	movl	%edx, 0(%r13)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L69:
	testq	%rax, %rax
	jne	.L70
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.L34
.L67:
	vzeroupper
	movl	%edx, 0(%r13)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L44:
	xorl	%edx, %edx
	jmp	.L32
.L53:
	movl	$7, %eax
	jmp	.L35
.L54:
	movl	$8, %eax
	jmp	.L35
.L61:
	movl	$15, %eax
	jmp	.L35
.L62:
	movl	$16, %eax
	jmp	.L35
.L51:
	movl	$5, %eax
	jmp	.L35
.L52:
	movl	$6, %eax
	jmp	.L35
.L47:
	movl	$1, %eax
	jmp	.L35
.L48:
	movl	$2, %eax
	jmp	.L35
.L55:
	movl	$9, %eax
	jmp	.L35
.L56:
	movl	$10, %eax
	jmp	.L35
.L57:
	movl	$11, %eax
	jmp	.L35
.L58:
	movl	$12, %eax
	jmp	.L35
.L59:
	movl	$13, %eax
	jmp	.L35
.L60:
	movl	$14, %eax
	jmp	.L35
.L49:
	movl	$3, %eax
	jmp	.L35
.L50:
	movl	$4, %eax
	jmp	.L35
.L70:
	movq	%rax, %rdi
	jmp	.L33
combine4p:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%rbx, %rdi
	movq	%rax, %r13
	call	get_vec_start
	leaq	(%rax,%r13,4), %r8
	movq	%rax, %rcx
	cmpq	%r8, %rax
	jae	.L84
	movq	%rcx, %r9
	movq	%rcx, %rdx
	leaq	4(%rax), %rax
	andl	$31, %edx
	notq	%r9
	addq	%r8, %r9
	shrq	$2, %rdx
	shrq	$2, %r9
	negq	%rdx
	addq	$1, %r9
	andl	$7, %edx
	cmpq	%rdx, %r9
	movq	%r9, %rdi
	cmovbe	%r9, %rdx
	cmpq	$17, %r9
	ja	.L93
.L73:
	cmpq	$1, %rdi
	movl	(%rcx), %edx
	jbe	.L75
	addl	4(%rcx), %edx
	cmpq	$2, %rdi
	leaq	8(%rcx), %rax
	jbe	.L75
	addl	8(%rcx), %edx
	cmpq	$3, %rdi
	leaq	12(%rcx), %rax
	jbe	.L75
	addl	12(%rcx), %edx
	cmpq	$4, %rdi
	leaq	16(%rcx), %rax
	jbe	.L75
	addl	16(%rcx), %edx
	cmpq	$5, %rdi
	leaq	20(%rcx), %rax
	jbe	.L75
	addl	20(%rcx), %edx
	cmpq	$6, %rdi
	leaq	24(%rcx), %rax
	jbe	.L75
	addl	24(%rcx), %edx
	cmpq	$7, %rdi
	leaq	28(%rcx), %rax
	jbe	.L75
	addl	28(%rcx), %edx
	cmpq	$8, %rdi
	leaq	32(%rcx), %rax
	jbe	.L75
	addl	32(%rcx), %edx
	cmpq	$9, %rdi
	leaq	36(%rcx), %rax
	jbe	.L75
	movl	36(%rcx), %esi
	leaq	40(%rcx), %rax
	addl	%edx, %esi
	cmpq	$10, %rdi
	movl	%esi, %edx
	jbe	.L75
	addl	40(%rcx), %esi
	cmpq	$11, %rdi
	leaq	44(%rcx), %rax
	movl	%esi, %edx
	jbe	.L75
	addl	44(%rcx), %edx
	cmpq	$12, %rdi
	leaq	48(%rcx), %rax
	jbe	.L75
	addl	48(%rcx), %edx
	cmpq	$13, %rdi
	leaq	52(%rcx), %rax
	jbe	.L75
	addl	52(%rcx), %edx
	cmpq	$14, %rdi
	leaq	56(%rcx), %rax
	jbe	.L75
	addl	56(%rcx), %edx
	cmpq	$15, %rdi
	leaq	60(%rcx), %rax
	jbe	.L75
	addl	60(%rcx), %edx
	cmpq	$16, %rdi
	leaq	64(%rcx), %rax
	jbe	.L75
	addl	64(%rcx), %edx
	leaq	68(%rcx), %rax
.L75:
	cmpq	%rdi, %r9
	je	.L72
.L74:
	subq	%rdi, %r9
	movq	%r9, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %rsi
	testq	%rsi, %rsi
	je	.L77
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %rdi
	xorl	%ecx, %ecx
.L83:
	addq	$1, %rcx
	vpaddd	(%rdi), %ymm0, %ymm0
	addq	$32, %rdi
	cmpq	%rcx, %r10
	ja	.L83
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%rsi,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edi
	vpextrd	$0, %xmm1, %ecx
	addl	%edi, %ecx
	vpextrd	$2, %xmm1, %edi
	addl	%edi, %ecx
	vpextrd	$3, %xmm1, %edi
	addl	%edi, %ecx
	vpextrd	$0, %xmm0, %edi
	addl	%edi, %ecx
	vpextrd	$1, %xmm0, %edi
	addl	%edi, %ecx
	vpextrd	$2, %xmm0, %edi
	addl	%edi, %ecx
	vpextrd	$3, %xmm0, %edi
	addl	%edi, %ecx
	addl	%ecx, %edx
	cmpq	%rsi, %r9
	je	.L91
	vzeroupper
.L77:
	leaq	4(%rax), %rcx
	addl	(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L72
	leaq	8(%rax), %rcx
	addl	4(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L72
	leaq	12(%rax), %rcx
	addl	8(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L72
	leaq	16(%rax), %rcx
	addl	12(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L72
	leaq	20(%rax), %rcx
	addl	16(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L72
	leaq	24(%rax), %rcx
	addl	20(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L72
	addl	24(%rax), %edx
.L72:
	movl	%edx, (%r12)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L93:
	testq	%rdx, %rdx
	jne	.L94
	xorl	%edi, %edi
	movq	%rcx, %rax
	xorl	%edx, %edx
	jmp	.L74
.L91:
	vzeroupper
	movl	%edx, (%r12)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L84:
	xorl	%edx, %edx
	jmp	.L72
.L94:
	movq	%rdx, %rdi
	jmp	.L73
combine5:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-1(%rax), %r12
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r12, %r12
	movq	%rax, %rcx
	jle	.L116
	leaq	-2(%rbx), %rdi
	shrq	%rdi
	addq	$1, %rdi
	movq	%rdi, %r8
	shrq	$2, %r8
	leaq	0(,%r8,4), %rsi
	testq	%rsi, %rsi
	je	.L117
	cmpq	$7, %rdi
	jbe	.L117
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L103:
	movq	%rax, %rdx
	addq	$1, %rax
	salq	$5, %rdx
	cmpq	%rax, %r8
	vmovdqu	(%rcx,%rdx), %xmm1
	vinserti128	$0x1, 16(%rcx,%rdx), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L103
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %r8d
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%r8d, %eax
	addl	%eax, %edx
	cmpq	%rsi, %rdi
	leaq	(%rsi,%rsi), %rax
	je	.L101
.L97:
	leaq	2(%rax), %rsi
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rax,4), %edx
	cmpq	%rsi, %r12
	jle	.L101
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	4(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L101
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	6(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L101
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	8(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L101
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	10(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L101
	addl	(%rcx,%rsi,4), %edx
	addq	$12, %rax
	addl	4(%rcx,%rsi,4), %edx
	cmpq	%rax, %r12
	jle	.L101
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rax,4), %edx
.L101:
	addq	%rdi, %rdi
.L96:
	cmpq	%rdi, %rbx
	jle	.L104
	leaq	(%rcx,%rdi,4), %rsi
	movq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L106
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r10
	jbe	.L126
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r9
	jbe	.L134
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rdi
.L107:
	cmpq	%rax, %r8
	je	.L104
.L106:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L109
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L115:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rsi, %r10
	ja	.L115
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%r9, %rax
	je	.L104
.L109:
	leaq	1(%rdi), %rax
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rax, %rbx
	jle	.L104
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L104
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L104
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L104
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L104
	addq	$6, %rdi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rdi, %rbx
	jle	.L104
	addl	(%rcx,%rdi,4), %edx
.L104:
	movl	%edx, 0(%r13)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L134:
	movq	%r9, %rdi
	jmp	.L107
.L117:
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.L97
.L116:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L96
.L126:
	movq	%r10, %rdi
	jmp	.L107
unroll3a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-2(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	testq	%rbx, %rbx
	movq	%rax, %rdi
	jle	.L145
.L146:
	addl	(%rdi,%rcx,4), %edx
	addl	4(%rdi,%rcx,4), %edx
	addl	8(%rdi,%rcx,4), %edx
	addq	$3, %rcx
	cmpq	%rcx, %rbx
	jg	.L146
.L145:
	cmpq	%rcx, %r12
	jle	.L147
	leaq	(%rdi,%rcx,4), %r9
	movq	%r12, %rax
	subq	%rcx, %rax
	movq	%r9, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L149
	addl	(%r9), %edx
	cmpq	$1, %r8
	leaq	1(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$2, %r8
	leaq	2(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$3, %r8
	leaq	3(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$4, %r8
	leaq	4(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$5, %r8
	leaq	5(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$6, %r8
	leaq	6(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$7, %r8
	leaq	7(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$8, %r8
	leaq	8(%rcx), %r10
	movl	%edx, %esi
	jbe	.L168
	addl	(%rdi,%r10,4), %esi
	cmpq	$9, %r8
	movl	%esi, %edx
	leaq	9(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$10, %r8
	leaq	10(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$11, %r8
	leaq	11(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$12, %r8
	leaq	12(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$13, %r8
	leaq	13(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$14, %r8
	leaq	14(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$15, %r8
	leaq	15(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	cmpq	$16, %r8
	leaq	16(%rcx), %rsi
	jbe	.L176
	addl	(%rdi,%rsi,4), %edx
	addq	$17, %rcx
.L150:
	cmpq	%rax, %r8
	je	.L147
.L149:
	subq	%r8, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L152
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%r8,4), %rsi
	xorl	%r8d, %r8d
.L158:
	addq	$1, %r8
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r8, %r11
	ja	.L158
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rcx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%rax, %r10
	je	.L184
	vzeroupper
.L152:
	leaq	1(%rcx), %rax
	addl	(%rdi,%rcx,4), %edx
	cmpq	%rax, %r12
	jle	.L147
	addl	(%rdi,%rax,4), %edx
	leaq	2(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L147
	addl	(%rdi,%rax,4), %edx
	leaq	3(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L147
	addl	(%rdi,%rax,4), %edx
	leaq	4(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L147
	addl	(%rdi,%rax,4), %edx
	leaq	5(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L147
	addq	$6, %rcx
	addl	(%rdi,%rax,4), %edx
	cmpq	%rcx, %r12
	jle	.L147
	addl	(%rdi,%rcx,4), %edx
.L147:
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L176:
	movq	%rsi, %rcx
	jmp	.L150
.L184:
	vzeroupper
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L168:
	movq	%r10, %rcx
	jmp	.L150
combine5p:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	get_vec_start
	movq	%r13, %rdi
	movq	%rax, %rbx
	call	vec_length
	leaq	(%rbx,%rax,4), %rdi
	leaq	-4(%rdi), %rcx
	cmpq	%rcx, %rbx
	jae	.L207
	movq	%rdi, %rax
	subq	%rbx, %rax
	leaq	-5(%rax), %rsi
	shrq	$3, %rsi
	addq	$1, %rsi
	movq	%rsi, %r9
	shrq	$2, %r9
	leaq	0(,%r9,4), %r8
	testq	%r8, %r8
	je	.L208
	cmpq	$7, %rsi
	jbe	.L208
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L194:
	movq	%rax, %rdx
	addq	$1, %rax
	salq	$5, %rdx
	cmpq	%rax, %r9
	vmovdqu	(%rbx,%rdx), %xmm1
	vinserti128	$0x1, 16(%rbx,%rdx), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L194
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %r9d
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%r9d, %eax
	addl	%eax, %edx
	cmpq	%r8, %rsi
	leaq	(%rbx,%r8,8), %rax
	je	.L192
.L188:
	leaq	8(%rax), %r8
	addl	(%rax), %edx
	addl	4(%rax), %edx
	cmpq	%r8, %rcx
	jbe	.L192
	leaq	16(%rax), %r8
	addl	8(%rax), %edx
	addl	12(%rax), %edx
	cmpq	%r8, %rcx
	jbe	.L192
	leaq	24(%rax), %r8
	addl	16(%rax), %edx
	addl	20(%rax), %edx
	cmpq	%r8, %rcx
	jbe	.L192
	leaq	32(%rax), %r8
	addl	24(%rax), %edx
	addl	28(%rax), %edx
	cmpq	%r8, %rcx
	jbe	.L192
	leaq	40(%rax), %r8
	addl	32(%rax), %edx
	addl	36(%rax), %edx
	cmpq	%r8, %rcx
	jbe	.L192
	leaq	48(%rax), %r8
	addl	40(%rax), %edx
	addl	44(%rax), %edx
	cmpq	%r8, %rcx
	jbe	.L192
	addl	48(%rax), %edx
	addl	52(%rax), %edx
.L192:
	leaq	(%rbx,%rsi,8), %rbx
.L187:
	cmpq	%rbx, %rdi
	jbe	.L195
	leaq	4(%rbx), %rax
	leaq	3(%rdi), %r8
	movq	%rbx, %rcx
	andl	$31, %ecx
	subq	%rax, %r8
	shrq	$2, %rcx
	shrq	$2, %r8
	negq	%rcx
	addq	$1, %r8
	andl	$7, %ecx
	cmpq	%rcx, %r8
	movq	%rcx, %rsi
	movq	%r8, %rcx
	cmovbe	%r8, %rsi
	cmpq	$17, %r8
	ja	.L217
.L196:
	addl	(%rbx), %edx
	cmpq	$1, %rcx
	jbe	.L198
	addl	4(%rbx), %edx
	cmpq	$2, %rcx
	leaq	8(%rbx), %rax
	jbe	.L198
	addl	8(%rbx), %edx
	cmpq	$3, %rcx
	leaq	12(%rbx), %rax
	jbe	.L198
	addl	12(%rbx), %edx
	cmpq	$4, %rcx
	leaq	16(%rbx), %rax
	jbe	.L198
	addl	16(%rbx), %edx
	cmpq	$5, %rcx
	leaq	20(%rbx), %rax
	jbe	.L198
	addl	20(%rbx), %edx
	cmpq	$6, %rcx
	leaq	24(%rbx), %rax
	jbe	.L198
	addl	24(%rbx), %edx
	cmpq	$7, %rcx
	leaq	28(%rbx), %rax
	jbe	.L198
	addl	28(%rbx), %edx
	cmpq	$8, %rcx
	leaq	32(%rbx), %rax
	jbe	.L198
	addl	32(%rbx), %edx
	cmpq	$9, %rcx
	leaq	36(%rbx), %rax
	jbe	.L198
	movl	36(%rbx), %esi
	leaq	40(%rbx), %rax
	addl	%edx, %esi
	cmpq	$10, %rcx
	movl	%esi, %edx
	jbe	.L198
	addl	40(%rbx), %esi
	cmpq	$11, %rcx
	leaq	44(%rbx), %rax
	movl	%esi, %edx
	jbe	.L198
	addl	44(%rbx), %edx
	cmpq	$12, %rcx
	leaq	48(%rbx), %rax
	jbe	.L198
	addl	48(%rbx), %edx
	cmpq	$13, %rcx
	leaq	52(%rbx), %rax
	jbe	.L198
	addl	52(%rbx), %edx
	cmpq	$14, %rcx
	leaq	56(%rbx), %rax
	jbe	.L198
	addl	56(%rbx), %edx
	cmpq	$15, %rcx
	leaq	60(%rbx), %rax
	jbe	.L198
	addl	60(%rbx), %edx
	cmpq	$16, %rcx
	leaq	64(%rbx), %rax
	jbe	.L198
	addl	64(%rbx), %edx
	leaq	68(%rbx), %rax
.L198:
	cmpq	%rcx, %r8
	je	.L195
.L197:
	subq	%rcx, %r8
	movq	%r8, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L200
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rbx,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L206:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rcx, %r10
	ja	.L206
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r9,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%r9, %r8
	je	.L195
.L200:
	leaq	4(%rax), %rcx
	addl	(%rax), %edx
	cmpq	%rcx, %rdi
	jbe	.L195
	leaq	8(%rax), %rcx
	addl	4(%rax), %edx
	cmpq	%rcx, %rdi
	jbe	.L195
	leaq	12(%rax), %rcx
	addl	8(%rax), %edx
	cmpq	%rcx, %rdi
	jbe	.L195
	leaq	16(%rax), %rcx
	addl	12(%rax), %edx
	cmpq	%rcx, %rdi
	jbe	.L195
	leaq	20(%rax), %rcx
	addl	16(%rax), %edx
	cmpq	%rcx, %rdi
	jbe	.L195
	leaq	24(%rax), %rcx
	addl	20(%rax), %edx
	cmpq	%rcx, %rdi
	jbe	.L195
	addl	24(%rax), %edx
.L195:
	movl	%edx, (%r12)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L208:
	movq	%rbx, %rax
	xorl	%edx, %edx
	jmp	.L188
.L217:
	testq	%rsi, %rsi
	jne	.L218
	xorl	%ecx, %ecx
	movq	%rbx, %rax
	jmp	.L197
.L207:
	xorl	%edx, %edx
	jmp	.L187
.L218:
	movq	%rsi, %rcx
	jmp	.L196
unroll2aw_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-1(%rax), %r12
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r12, %r12
	movq	%rax, %rcx
	jle	.L240
	leaq	-2(%rbx), %rdi
	shrq	%rdi
	addq	$1, %rdi
	movq	%rdi, %r8
	shrq	$2, %r8
	leaq	0(,%r8,4), %rsi
	testq	%rsi, %rsi
	je	.L241
	cmpq	$7, %rdi
	jbe	.L241
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L227:
	movq	%rax, %rdx
	addq	$1, %rax
	salq	$5, %rdx
	cmpq	%rax, %r8
	vmovdqu	(%rcx,%rdx), %xmm1
	vinserti128	$0x1, 16(%rcx,%rdx), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L227
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %r8d
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%r8d, %eax
	addl	%eax, %edx
	cmpq	%rsi, %rdi
	leaq	(%rsi,%rsi), %rax
	je	.L225
.L221:
	leaq	2(%rax), %rsi
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rax,4), %edx
	cmpq	%rsi, %r12
	jle	.L225
	leaq	4(%rax), %r8
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	cmpq	%r8, %r12
	jle	.L225
	leaq	6(%rax), %rsi
	addl	(%rcx,%r8,4), %edx
	addl	4(%rcx,%r8,4), %edx
	cmpq	%rsi, %r12
	jle	.L225
	leaq	8(%rax), %r8
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	cmpq	%r8, %r12
	jle	.L225
	leaq	10(%rax), %rsi
	addl	(%rcx,%r8,4), %edx
	addl	4(%rcx,%r8,4), %edx
	cmpq	%rsi, %r12
	jle	.L225
	addl	(%rcx,%rsi,4), %edx
	addq	$12, %rax
	addl	4(%rcx,%rsi,4), %edx
	cmpq	%rax, %r12
	jle	.L225
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rax,4), %edx
.L225:
	addq	%rdi, %rdi
.L220:
	cmpq	%rdi, %rbx
	jle	.L228
	leaq	(%rcx,%rdi,4), %rsi
	movq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L230
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r10
	jbe	.L250
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r9
	jbe	.L258
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rdi
.L231:
	cmpq	%rax, %r8
	je	.L228
.L230:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L233
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L239:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rsi, %r10
	ja	.L239
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%r9, %rax
	je	.L228
.L233:
	leaq	1(%rdi), %rax
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rax, %rbx
	jle	.L228
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L228
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L228
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L228
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L228
	addq	$6, %rdi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rdi, %rbx
	jle	.L228
	addl	(%rcx,%rdi,4), %edx
.L228:
	movl	%edx, 0(%r13)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L258:
	movq	%r9, %rdi
	jmp	.L231
.L241:
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.L221
.L240:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L220
.L250:
	movq	%r10, %rdi
	jmp	.L231
unroll4a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-3(%rax), %r13
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r13, %r13
	movq	%rax, %rcx
	jle	.L289
	leaq	-4(%rbx), %rdi
	shrq	$2, %rdi
	addq	$1, %rdi
	movq	%rdi, %rdx
	shrq	%rdx
	movq	%rdx, %r8
	addq	%r8, %r8
	je	.L290
	cmpq	$3, %rdi
	jbe	.L290
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L276:
	movq	%rax, %rsi
	addq	$1, %rax
	salq	$5, %rsi
	cmpq	%rax, %rdx
	vmovdqu	(%rcx,%rsi), %xmm1
	vinserti128	$0x1, 16(%rcx,%rsi), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L276
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %esi
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%esi, %eax
	addl	%eax, %edx
	cmpq	%r8, %rdi
	leaq	0(,%r8,4), %rax
	je	.L274
.L270:
	leaq	0(,%rax,4), %rsi
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rax), %r8
	addl	4(%rcx,%rsi), %edx
	addl	8(%rcx,%rsi), %edx
	addl	12(%rcx,%rsi), %edx
	cmpq	%r8, %r13
	jg	.L317
.L274:
	salq	$2, %rdi
.L269:
	cmpq	%rdi, %rbx
	jle	.L277
	leaq	(%rcx,%rdi,4), %rsi
	movq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L279
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r10
	jbe	.L299
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r9
	jbe	.L307
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rdi
.L280:
	cmpq	%rax, %r8
	je	.L277
.L279:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L282
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L288:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%r10, %rsi
	jb	.L288
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%rax, %r9
	je	.L277
.L282:
	leaq	1(%rdi), %rax
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rax, %rbx
	jle	.L277
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L277
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L277
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L277
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L277
	addq	$6, %rdi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rdi, %rbx
	jle	.L277
	addl	(%rcx,%rdi,4), %edx
.L277:
	movl	%edx, (%r12)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L317:
	leaq	0(,%r8,4), %rsi
	addl	(%rcx,%r8,4), %edx
	addq	$8, %rax
	addl	4(%rcx,%rsi), %edx
	addl	8(%rcx,%rsi), %edx
	addl	12(%rcx,%rsi), %edx
	cmpq	%rax, %r13
	jle	.L274
	leaq	0(,%rax,4), %rsi
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rsi), %edx
	addl	8(%rcx,%rsi), %edx
	addl	12(%rcx,%rsi), %edx
	jmp	.L274
.L307:
	movq	%r9, %rdi
	jmp	.L280
.L290:
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.L270
.L289:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L269
.L299:
	movq	%r10, %rdi
	jmp	.L280
unroll5a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-4(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r8
	jle	.L333
	movq	%rax, %rcx
	xorl	%edx, %edx
	xorl	%edi, %edi
.L320:
	addl	(%rcx), %edx
	addq	$5, %rdi
	addq	$20, %rcx
	addl	-16(%rcx), %edx
	addl	-12(%rcx), %edx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	cmpq	%rdi, %rbx
	jg	.L320
.L319:
	cmpq	%rdi, %r12
	jle	.L321
	leaq	(%r8,%rdi,4), %r9
	movq	%r12, %rax
	subq	%rdi, %rax
	movq	%r9, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L323
	addl	(%r9), %edx
	cmpq	$1, %rcx
	leaq	1(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$2, %rcx
	leaq	2(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$3, %rcx
	leaq	3(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$4, %rcx
	leaq	4(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$5, %rcx
	leaq	5(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$6, %rcx
	leaq	6(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$7, %rcx
	leaq	7(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$8, %rcx
	leaq	8(%rdi), %r10
	movl	%edx, %esi
	jbe	.L342
	addl	(%r8,%r10,4), %esi
	cmpq	$9, %rcx
	movl	%esi, %edx
	leaq	9(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$10, %rcx
	leaq	10(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$11, %rcx
	leaq	11(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$12, %rcx
	leaq	12(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$13, %rcx
	leaq	13(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$14, %rcx
	leaq	14(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$15, %rcx
	leaq	15(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	cmpq	$16, %rcx
	leaq	16(%rdi), %rsi
	jbe	.L350
	addl	(%r8,%rsi,4), %edx
	addq	$17, %rdi
.L324:
	cmpq	%rax, %rcx
	je	.L321
.L323:
	subq	%rcx, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L326
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L332:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r11, %rcx
	jb	.L332
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%rax, %r10
	je	.L358
	vzeroupper
.L326:
	leaq	1(%rdi), %rax
	addl	(%r8,%rdi,4), %edx
	cmpq	%rax, %r12
	jle	.L321
	addl	(%r8,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L321
	addl	(%r8,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L321
	addl	(%r8,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L321
	addl	(%r8,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L321
	addq	$6, %rdi
	addl	(%r8,%rax,4), %edx
	cmpq	%rdi, %r12
	jle	.L321
	addl	(%r8,%rdi,4), %edx
.L321:
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L350:
	movq	%rsi, %rdi
	jmp	.L324
.L358:
	vzeroupper
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L333:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L319
.L342:
	movq	%r10, %rdi
	jmp	.L324
unroll6a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-5(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r8
	jle	.L375
	movq	%rax, %rcx
	xorl	%edx, %edx
	xorl	%edi, %edi
.L362:
	addl	(%rcx), %edx
	addq	$6, %rdi
	addq	$24, %rcx
	addl	-20(%rcx), %edx
	addl	-16(%rcx), %edx
	addl	-12(%rcx), %edx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	cmpq	%rdi, %rbx
	jg	.L362
.L361:
	cmpq	%rdi, %r12
	jle	.L363
	leaq	(%r8,%rdi,4), %r9
	movq	%r12, %rax
	subq	%rdi, %rax
	movq	%r9, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L365
	addl	(%r9), %edx
	cmpq	$1, %rcx
	leaq	1(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$2, %rcx
	leaq	2(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$3, %rcx
	leaq	3(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$4, %rcx
	leaq	4(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$5, %rcx
	leaq	5(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$6, %rcx
	leaq	6(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$7, %rcx
	leaq	7(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$8, %rcx
	leaq	8(%rdi), %r10
	movl	%edx, %esi
	jbe	.L384
	addl	(%r8,%r10,4), %esi
	cmpq	$9, %rcx
	movl	%esi, %edx
	leaq	9(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$10, %rcx
	leaq	10(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$11, %rcx
	leaq	11(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$12, %rcx
	leaq	12(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$13, %rcx
	leaq	13(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$14, %rcx
	leaq	14(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$15, %rcx
	leaq	15(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	cmpq	$16, %rcx
	leaq	16(%rdi), %rsi
	jbe	.L392
	addl	(%r8,%rsi,4), %edx
	addq	$17, %rdi
.L366:
	cmpq	%rax, %rcx
	je	.L363
.L365:
	subq	%rcx, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L368
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L374:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r11, %rcx
	jb	.L374
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%rax, %r10
	je	.L400
	vzeroupper
.L368:
	leaq	1(%rdi), %rax
	addl	(%r8,%rdi,4), %edx
	cmpq	%rax, %r12
	jle	.L363
	addl	(%r8,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L363
	addl	(%r8,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L363
	addl	(%r8,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L363
	addl	(%r8,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L363
	addq	$6, %rdi
	addl	(%r8,%rax,4), %edx
	cmpq	%rdi, %r12
	jle	.L363
	addl	(%r8,%rdi,4), %edx
.L363:
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L392:
	movq	%rsi, %rdi
	jmp	.L366
.L400:
	vzeroupper
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L375:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L361
.L384:
	movq	%r10, %rdi
	jmp	.L366
unroll7a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-6(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r8
	jle	.L417
	movq	%rax, %rcx
	xorl	%edx, %edx
	xorl	%edi, %edi
.L404:
	addl	(%rcx), %edx
	addq	$7, %rdi
	addq	$28, %rcx
	addl	-24(%rcx), %edx
	addl	-20(%rcx), %edx
	addl	-16(%rcx), %edx
	addl	-12(%rcx), %edx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	cmpq	%rdi, %rbx
	jg	.L404
.L403:
	cmpq	%rdi, %r12
	jle	.L405
	leaq	(%r8,%rdi,4), %r9
	movq	%r12, %rax
	subq	%rdi, %rax
	movq	%r9, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L407
	addl	(%r9), %edx
	cmpq	$1, %rcx
	leaq	1(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$2, %rcx
	leaq	2(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$3, %rcx
	leaq	3(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$4, %rcx
	leaq	4(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$5, %rcx
	leaq	5(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$6, %rcx
	leaq	6(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$7, %rcx
	leaq	7(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$8, %rcx
	leaq	8(%rdi), %r10
	movl	%edx, %esi
	jbe	.L426
	addl	(%r8,%r10,4), %esi
	cmpq	$9, %rcx
	movl	%esi, %edx
	leaq	9(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$10, %rcx
	leaq	10(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$11, %rcx
	leaq	11(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$12, %rcx
	leaq	12(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$13, %rcx
	leaq	13(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$14, %rcx
	leaq	14(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$15, %rcx
	leaq	15(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	cmpq	$16, %rcx
	leaq	16(%rdi), %rsi
	jbe	.L434
	addl	(%r8,%rsi,4), %edx
	addq	$17, %rdi
.L408:
	cmpq	%rax, %rcx
	je	.L405
.L407:
	subq	%rcx, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L410
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L416:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r11, %rcx
	jb	.L416
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%rax, %r10
	je	.L442
	vzeroupper
.L410:
	leaq	1(%rdi), %rax
	addl	(%r8,%rdi,4), %edx
	cmpq	%rax, %r12
	jle	.L405
	addl	(%r8,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L405
	addl	(%r8,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%r12, %rax
	jge	.L405
	addl	(%r8,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L405
	addl	(%r8,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L405
	addq	$6, %rdi
	addl	(%r8,%rax,4), %edx
	cmpq	%rdi, %r12
	jle	.L405
	addl	(%r8,%rdi,4), %edx
.L405:
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L434:
	movq	%rsi, %rdi
	jmp	.L408
.L442:
	vzeroupper
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L417:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L403
.L426:
	movq	%r10, %rdi
	jmp	.L408
unroll8a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	cmpq	$7, %rbx
	movq	%rax, %rcx
	jle	.L462
	leaq	-8(%rbx), %rdi
	shrq	$3, %rdi
	addq	$1, %rdi
	cmpq	$1, %rdi
	jbe	.L446
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L449:
	movq	%rax, %rdx
	addq	$1, %rax
	salq	$5, %rdx
	cmpq	%rax, %rdi
	vmovdqu	(%rcx,%rdx), %xmm1
	vinserti128	$0x1, 16(%rcx,%rdx), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L449
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %esi
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%esi, %eax
	addl	%eax, %edx
.L448:
	salq	$3, %rdi
.L445:
	cmpq	%rdi, %rbx
	jle	.L450
	leaq	(%rcx,%rdi,4), %rsi
	movq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L452
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r10
	jbe	.L471
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r9
	jbe	.L479
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rdi
.L453:
	cmpq	%rax, %r8
	je	.L450
.L452:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L455
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L461:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%r10, %rsi
	jb	.L461
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%rax, %r9
	je	.L450
.L455:
	leaq	1(%rdi), %rax
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rax, %rbx
	jle	.L450
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L450
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L450
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L450
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L450
	addq	$6, %rdi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rdi, %rbx
	jle	.L450
	addl	(%rcx,%rdi,4), %edx
.L450:
	movl	%edx, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L479:
	movq	%r9, %rdi
	jmp	.L453
.L446:
	movl	4(%rax), %edx
	addl	(%rax), %edx
	addl	8(%rax), %edx
	addl	12(%rax), %edx
	addl	16(%rax), %edx
	addl	20(%rax), %edx
	addl	24(%rax), %edx
	addl	28(%rax), %edx
	jmp	.L448
.L462:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L445
.L471:
	movq	%r10, %rdi
	jmp	.L453
unroll9a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-8(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r8
	jle	.L503
	movq	%rax, %rcx
	xorl	%edx, %edx
	xorl	%edi, %edi
.L490:
	addl	(%rcx), %edx
	addq	$9, %rdi
	addq	$36, %rcx
	addl	-32(%rcx), %edx
	addl	-28(%rcx), %edx
	addl	-24(%rcx), %edx
	addl	-20(%rcx), %edx
	addl	-16(%rcx), %edx
	addl	-12(%rcx), %edx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	cmpq	%rdi, %rbx
	jg	.L490
.L489:
	cmpq	%rdi, %r12
	jle	.L491
	leaq	(%r8,%rdi,4), %r9
	movq	%r12, %rax
	subq	%rdi, %rax
	movq	%r9, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L493
	addl	(%r9), %edx
	cmpq	$1, %rcx
	leaq	1(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$2, %rcx
	leaq	2(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$3, %rcx
	leaq	3(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$4, %rcx
	leaq	4(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$5, %rcx
	leaq	5(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$6, %rcx
	leaq	6(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$7, %rcx
	leaq	7(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$8, %rcx
	leaq	8(%rdi), %r10
	movl	%edx, %esi
	jbe	.L512
	addl	(%r8,%r10,4), %esi
	cmpq	$9, %rcx
	movl	%esi, %edx
	leaq	9(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$10, %rcx
	leaq	10(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$11, %rcx
	leaq	11(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$12, %rcx
	leaq	12(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$13, %rcx
	leaq	13(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$14, %rcx
	leaq	14(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$15, %rcx
	leaq	15(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	cmpq	$16, %rcx
	leaq	16(%rdi), %rsi
	jbe	.L520
	addl	(%r8,%rsi,4), %edx
	addq	$17, %rdi
.L494:
	cmpq	%rax, %rcx
	je	.L491
.L493:
	subq	%rcx, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L496
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L502:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r11, %rcx
	jb	.L502
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%rax, %r10
	je	.L528
	vzeroupper
.L496:
	leaq	1(%rdi), %rax
	addl	(%r8,%rdi,4), %edx
	cmpq	%rax, %r12
	jle	.L491
	addl	(%r8,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L491
	addl	(%r8,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L491
	addl	(%r8,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L491
	addl	(%r8,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L491
	addq	$6, %rdi
	addl	(%r8,%rax,4), %edx
	cmpq	%r12, %rdi
	jge	.L491
	addl	(%r8,%rdi,4), %edx
.L491:
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L520:
	movq	%rsi, %rdi
	jmp	.L494
.L528:
	vzeroupper
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L503:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L489
.L512:
	movq	%r10, %rdi
	jmp	.L494
unroll10a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-9(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r8
	jle	.L545
	movq	%rax, %rcx
	xorl	%edx, %edx
	xorl	%edi, %edi
.L532:
	addl	(%rcx), %edx
	addq	$10, %rdi
	addq	$40, %rcx
	addl	-36(%rcx), %edx
	addl	-32(%rcx), %edx
	addl	-28(%rcx), %edx
	addl	-24(%rcx), %edx
	addl	-20(%rcx), %edx
	addl	-16(%rcx), %edx
	addl	-12(%rcx), %edx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	cmpq	%rdi, %rbx
	jg	.L532
.L531:
	cmpq	%rdi, %r12
	jle	.L533
	leaq	(%r8,%rdi,4), %r9
	movq	%r12, %rax
	subq	%rdi, %rax
	movq	%r9, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L535
	addl	(%r9), %edx
	cmpq	$1, %rcx
	leaq	1(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$2, %rcx
	leaq	2(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$3, %rcx
	leaq	3(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$4, %rcx
	leaq	4(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$5, %rcx
	leaq	5(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$6, %rcx
	leaq	6(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$7, %rcx
	leaq	7(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$8, %rcx
	leaq	8(%rdi), %r10
	movl	%edx, %esi
	jbe	.L554
	addl	(%r8,%r10,4), %esi
	cmpq	$9, %rcx
	movl	%esi, %edx
	leaq	9(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$10, %rcx
	leaq	10(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$11, %rcx
	leaq	11(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$12, %rcx
	leaq	12(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$13, %rcx
	leaq	13(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$14, %rcx
	leaq	14(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$15, %rcx
	leaq	15(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	cmpq	$16, %rcx
	leaq	16(%rdi), %rsi
	jbe	.L562
	addl	(%r8,%rsi,4), %edx
	addq	$17, %rdi
.L536:
	cmpq	%rax, %rcx
	je	.L533
.L535:
	subq	%rcx, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L538
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L544:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r11, %rcx
	jb	.L544
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%rax, %r10
	je	.L570
	vzeroupper
.L538:
	leaq	1(%rdi), %rax
	addl	(%r8,%rdi,4), %edx
	cmpq	%rax, %r12
	jle	.L533
	addl	(%r8,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L533
	addl	(%r8,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L533
	addl	(%r8,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L533
	addl	(%r8,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r12
	jle	.L533
	addq	$6, %rdi
	addl	(%r8,%rax,4), %edx
	cmpq	%rdi, %r12
	jle	.L533
	addl	(%r8,%rdi,4), %edx
.L533:
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L562:
	movq	%rsi, %rdi
	jmp	.L536
.L570:
	vzeroupper
	movl	%edx, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L545:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L531
.L554:
	movq	%r10, %rdi
	jmp	.L536
unroll16a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	cmpq	$15, %rbx
	movq	%rax, %rdi
	jle	.L588
	leaq	-16(%rbx), %rcx
	vpxor	%xmm0, %xmm0, %xmm0
	movq	%rax, %rdx
	shrq	$4, %rcx
	movq	%rcx, %rax
	vmovdqa	%ymm0, %ymm1
	salq	$6, %rax
	leaq	64(%rdi,%rax), %rax
.L575:
	vmovdqu	(%rdx), %xmm2
	addq	$64, %rdx
	vinserti128	$0x1, -48(%rdx), %ymm2, %ymm2
	vpaddd	%ymm2, %ymm1, %ymm1
	vmovdqu	-32(%rdx), %xmm2
	vinserti128	$0x1, -16(%rdx), %ymm2, %ymm2
	cmpq	%rax, %rdx
	vpaddd	%ymm2, %ymm0, %ymm0
	jne	.L575
	vpaddd	%ymm0, %ymm1, %ymm0
	vmovdqa	%xmm0, %xmm2
	addq	$1, %rcx
	vextracti128	$0x1, %ymm0, %xmm0
	salq	$4, %rcx
	vpextrd	$1, %xmm2, %edx
	vpextrd	$0, %xmm2, %eax
	vpextrd	$2, %xmm0, %esi
	addl	%edx, %eax
	vpextrd	$2, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%esi, %eax
	addl	%eax, %edx
.L573:
	cmpq	%rcx, %rbx
	jle	.L576
	leaq	(%rdi,%rcx,4), %rsi
	movq	%rbx, %rax
	subq	%rcx, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L578
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$2, %r8
	leaq	2(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$3, %r8
	leaq	3(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$4, %r8
	leaq	4(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$5, %r8
	leaq	5(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$6, %r8
	leaq	6(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$7, %r8
	leaq	7(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$8, %r8
	leaq	8(%rcx), %r10
	jbe	.L597
	addl	(%rdi,%r10,4), %edx
	cmpq	$9, %r8
	leaq	9(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$10, %r8
	leaq	10(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$11, %r8
	leaq	11(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$12, %r8
	leaq	12(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$13, %r8
	leaq	13(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$14, %r8
	leaq	14(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$15, %r8
	leaq	15(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	cmpq	$16, %r8
	leaq	16(%rcx), %r9
	jbe	.L605
	addl	(%rdi,%r9,4), %edx
	addq	$17, %rcx
.L579:
	cmpq	%rax, %r8
	je	.L576
.L578:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L581
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L587:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%r10, %rsi
	jb	.L587
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rcx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%rax, %r9
	je	.L576
.L581:
	leaq	1(%rcx), %rax
	addl	(%rdi,%rcx,4), %edx
	cmpq	%rax, %rbx
	jle	.L576
	addl	(%rdi,%rax,4), %edx
	leaq	2(%rcx), %rax
	cmpq	%rax, %rbx
	jle	.L576
	addl	(%rdi,%rax,4), %edx
	leaq	3(%rcx), %rax
	cmpq	%rax, %rbx
	jle	.L576
	addl	(%rdi,%rax,4), %edx
	leaq	4(%rcx), %rax
	cmpq	%rax, %rbx
	jle	.L576
	addl	(%rdi,%rax,4), %edx
	leaq	5(%rcx), %rax
	cmpq	%rax, %rbx
	jle	.L576
	addq	$6, %rcx
	addl	(%rdi,%rax,4), %edx
	cmpq	%rcx, %rbx
	jle	.L576
	addl	(%rdi,%rcx,4), %edx
.L576:
	movl	%edx, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L605:
	movq	%r9, %rcx
	jmp	.L579
.L588:
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	jmp	.L573
.L597:
	movq	%r10, %rcx
	jmp	.L579
unroll2_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	movq	%rax, %rcx
	movq	%r12, %rax
	shrq	$63, %rax
	leaq	(%r12,%rax), %r8
	andl	$1, %r8d
	subq	%rax, %r8
	subq	%r8, %r12
	leaq	(%rcx,%r12,4), %rdi
	cmpq	%rdi, %rcx
	jae	.L635
	movq	%rcx, %rsi
	notq	%rsi
	addq	%rdi, %rsi
	shrq	$3, %rsi
	addq	$1, %rsi
	movq	%rsi, %r10
	shrq	$2, %r10
	leaq	0(,%r10,4), %r9
	testq	%r9, %r9
	je	.L636
	cmpq	$7, %rsi
	jbe	.L636
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L622:
	movq	%rax, %rdx
	addq	$1, %rax
	salq	$5, %rdx
	cmpq	%rax, %r10
	vmovdqu	(%rcx,%rdx), %xmm1
	vinserti128	$0x1, 16(%rcx,%rdx), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L622
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %r10d
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%r10d, %eax
	addl	%eax, %edx
	cmpq	%r9, %rsi
	leaq	(%rcx,%r9,8), %rax
	je	.L620
.L616:
	leaq	8(%rax), %r9
	addl	(%rax), %edx
	addl	4(%rax), %edx
	cmpq	%r9, %rdi
	jbe	.L620
	leaq	16(%rax), %r9
	addl	8(%rax), %edx
	addl	12(%rax), %edx
	cmpq	%r9, %rdi
	jbe	.L620
	leaq	24(%rax), %r9
	addl	16(%rax), %edx
	addl	20(%rax), %edx
	cmpq	%r9, %rdi
	jbe	.L620
	leaq	32(%rax), %r9
	addl	24(%rax), %edx
	addl	28(%rax), %edx
	cmpq	%r9, %rdi
	jbe	.L620
	leaq	40(%rax), %r9
	addl	32(%rax), %edx
	addl	36(%rax), %edx
	cmpq	%r9, %rdi
	jbe	.L620
	leaq	48(%rax), %r9
	addl	40(%rax), %edx
	addl	44(%rax), %edx
	cmpq	%r9, %rdi
	jbe	.L620
	addl	48(%rax), %edx
	addl	52(%rax), %edx
.L620:
	leaq	(%rcx,%rsi,8), %rcx
.L615:
	leaq	(%rdi,%r8,4), %r8
	cmpq	%rcx, %r8
	jbe	.L623
	leaq	4(%rcx), %rax
	leaq	3(%r8), %r9
	movq	%rcx, %rdi
	andl	$31, %edi
	subq	%rax, %r9
	shrq	$2, %rdi
	shrq	$2, %r9
	negq	%rdi
	addq	$1, %r9
	andl	$7, %edi
	cmpq	%rdi, %r9
	movq	%rdi, %rsi
	movq	%r9, %rdi
	cmovbe	%r9, %rsi
	cmpq	$17, %r9
	ja	.L645
.L624:
	addl	(%rcx), %edx
	cmpq	$1, %rdi
	jbe	.L626
	addl	4(%rcx), %edx
	cmpq	$2, %rdi
	leaq	8(%rcx), %rax
	jbe	.L626
	addl	8(%rcx), %edx
	cmpq	$3, %rdi
	leaq	12(%rcx), %rax
	jbe	.L626
	addl	12(%rcx), %edx
	cmpq	$4, %rdi
	leaq	16(%rcx), %rax
	jbe	.L626
	addl	16(%rcx), %edx
	cmpq	$5, %rdi
	leaq	20(%rcx), %rax
	jbe	.L626
	addl	20(%rcx), %edx
	cmpq	$6, %rdi
	leaq	24(%rcx), %rax
	jbe	.L626
	addl	24(%rcx), %edx
	cmpq	$7, %rdi
	leaq	28(%rcx), %rax
	jbe	.L626
	addl	28(%rcx), %edx
	cmpq	$8, %rdi
	leaq	32(%rcx), %rax
	jbe	.L626
	addl	32(%rcx), %edx
	cmpq	$9, %rdi
	leaq	36(%rcx), %rax
	jbe	.L626
	movl	36(%rcx), %esi
	leaq	40(%rcx), %rax
	addl	%edx, %esi
	cmpq	$10, %rdi
	movl	%esi, %edx
	jbe	.L626
	addl	40(%rcx), %esi
	cmpq	$11, %rdi
	leaq	44(%rcx), %rax
	movl	%esi, %edx
	jbe	.L626
	addl	44(%rcx), %edx
	cmpq	$12, %rdi
	leaq	48(%rcx), %rax
	jbe	.L626
	addl	48(%rcx), %edx
	cmpq	$13, %rdi
	leaq	52(%rcx), %rax
	jbe	.L626
	addl	52(%rcx), %edx
	cmpq	$14, %rdi
	leaq	56(%rcx), %rax
	jbe	.L626
	addl	56(%rcx), %edx
	cmpq	$15, %rdi
	leaq	60(%rcx), %rax
	jbe	.L626
	addl	60(%rcx), %edx
	cmpq	$16, %rdi
	leaq	64(%rcx), %rax
	jbe	.L626
	addl	64(%rcx), %edx
	leaq	68(%rcx), %rax
.L626:
	cmpq	%rdi, %r9
	je	.L623
.L625:
	subq	%rdi, %r9
	movq	%r9, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L628
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %rsi
	xorl	%ecx, %ecx
.L634:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rcx, %r11
	ja	.L634
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r10,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%r10, %r9
	je	.L623
.L628:
	leaq	4(%rax), %rcx
	addl	(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L623
	leaq	8(%rax), %rcx
	addl	4(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L623
	leaq	12(%rax), %rcx
	addl	8(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L623
	leaq	16(%rax), %rcx
	addl	12(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L623
	leaq	20(%rax), %rcx
	addl	16(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L623
	leaq	24(%rax), %rcx
	addl	20(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L623
	addl	24(%rax), %edx
.L623:
	movl	%edx, (%rbx)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L636:
	movq	%rcx, %rax
	xorl	%edx, %edx
	jmp	.L616
.L645:
	testq	%rsi, %rsi
	jne	.L646
	xorl	%edi, %edi
	movq	%rcx, %rax
	jmp	.L625
.L635:
	xorl	%edx, %edx
	jmp	.L615
.L646:
	movq	%rsi, %rdi
	jmp	.L624
unroll3_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%rbx, %rdi
	movq	%rax, %r13
	call	get_vec_start
	movq	%rax, %rcx
	leaq	-8(%rax,%r13,4), %rax
	xorl	%edx, %edx
	cmpq	%rax, %rcx
	jae	.L648
.L649:
	addl	(%rcx), %edx
	addq	$12, %rcx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	cmpq	%rcx, %rax
	ja	.L649
.L648:
	leaq	8(%rax), %r9
	cmpq	%rcx, %r9
	jbe	.L650
	addq	$7, %rax
	movq	%rcx, %rdi
	subq	%rcx, %rax
	andl	$31, %edi
	shrq	$2, %rax
	shrq	$2, %rdi
	leaq	1(%rax), %r8
	negq	%rdi
	andl	$7, %edi
	cmpq	%r8, %rdi
	movq	%rdi, %rax
	movq	%r8, %rdi
	cmova	%r8, %rax
	cmpq	$17, %r8
	ja	.L671
.L651:
	addl	(%rcx), %edx
	cmpq	$1, %rdi
	leaq	4(%rcx), %rax
	jbe	.L653
	addl	4(%rcx), %edx
	cmpq	$2, %rdi
	leaq	8(%rcx), %rax
	jbe	.L653
	addl	8(%rcx), %edx
	cmpq	$3, %rdi
	leaq	12(%rcx), %rax
	jbe	.L653
	addl	12(%rcx), %edx
	cmpq	$4, %rdi
	leaq	16(%rcx), %rax
	jbe	.L653
	addl	16(%rcx), %edx
	cmpq	$5, %rdi
	leaq	20(%rcx), %rax
	jbe	.L653
	addl	20(%rcx), %edx
	cmpq	$6, %rdi
	leaq	24(%rcx), %rax
	jbe	.L653
	addl	24(%rcx), %edx
	cmpq	$7, %rdi
	leaq	28(%rcx), %rax
	jbe	.L653
	addl	28(%rcx), %edx
	cmpq	$8, %rdi
	leaq	32(%rcx), %rax
	jbe	.L653
	addl	32(%rcx), %edx
	cmpq	$9, %rdi
	leaq	36(%rcx), %rax
	jbe	.L653
	movl	36(%rcx), %esi
	leaq	40(%rcx), %rax
	addl	%edx, %esi
	cmpq	$10, %rdi
	movl	%esi, %edx
	jbe	.L653
	addl	40(%rcx), %esi
	cmpq	$11, %rdi
	leaq	44(%rcx), %rax
	movl	%esi, %edx
	jbe	.L653
	addl	44(%rcx), %edx
	cmpq	$12, %rdi
	leaq	48(%rcx), %rax
	jbe	.L653
	addl	48(%rcx), %edx
	cmpq	$13, %rdi
	leaq	52(%rcx), %rax
	jbe	.L653
	addl	52(%rcx), %edx
	cmpq	$14, %rdi
	leaq	56(%rcx), %rax
	jbe	.L653
	addl	56(%rcx), %edx
	cmpq	$15, %rdi
	leaq	60(%rcx), %rax
	jbe	.L653
	addl	60(%rcx), %edx
	cmpq	$16, %rdi
	leaq	64(%rcx), %rax
	jbe	.L653
	addl	64(%rcx), %edx
	leaq	68(%rcx), %rax
.L653:
	cmpq	%rdi, %r8
	je	.L650
.L652:
	subq	%rdi, %r8
	movq	%r8, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L655
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %rsi
	xorl	%ecx, %ecx
.L661:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rcx, %r11
	ja	.L661
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r10,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%r10, %r8
	je	.L669
	vzeroupper
.L655:
	leaq	4(%rax), %rcx
	addl	(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L650
	leaq	8(%rax), %rcx
	addl	4(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L650
	leaq	12(%rax), %rcx
	addl	8(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L650
	leaq	16(%rax), %rcx
	addl	12(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L650
	leaq	20(%rax), %rcx
	addl	16(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L650
	leaq	24(%rax), %rcx
	addl	20(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L650
	addl	24(%rax), %edx
.L650:
	movl	%edx, (%r12)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L671:
	testq	%rax, %rax
	jne	.L672
	xorl	%edi, %edi
	movq	%rcx, %rax
	jmp	.L652
.L669:
	vzeroupper
	movl	%edx, (%r12)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L672:
	movq	%rax, %rdi
	jmp	.L651
unroll4_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	movq	%rsi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %r13
	call	get_vec_start
	leaq	-12(%rax,%r13,4), %rdi
	movq	%rax, %rcx
	cmpq	%rdi, %rax
	jae	.L694
	movq	%rax, %rsi
	notq	%rsi
	addq	%rdi, %rsi
	shrq	$4, %rsi
	addq	$1, %rsi
	movq	%rsi, %rdx
	shrq	%rdx
	movq	%rdx, %r9
	addq	%r9, %r9
	je	.L695
	cmpq	$3, %rsi
	jbe	.L695
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L681:
	movq	%rax, %r8
	addq	$1, %rax
	salq	$5, %r8
	cmpq	%rax, %rdx
	vmovdqu	(%rcx,%r8), %xmm1
	vinserti128	$0x1, 16(%rcx,%r8), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L681
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edx
	vpextrd	$0, %xmm1, %eax
	vpextrd	$2, %xmm0, %r8d
	addl	%edx, %eax
	vpextrd	$2, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm1, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%r8d, %eax
	addl	%eax, %edx
	movq	%r9, %rax
	salq	$4, %rax
	addq	%rcx, %rax
	cmpq	%r9, %rsi
	je	.L679
.L675:
	addl	(%rax), %edx
	leaq	16(%rax), %r8
	addl	4(%rax), %edx
	addl	8(%rax), %edx
	addl	12(%rax), %edx
	cmpq	%r8, %rdi
	ja	.L704
.L679:
	salq	$4, %rsi
	addq	%rsi, %rcx
.L674:
	leaq	12(%rdi), %r9
	cmpq	%rcx, %r9
	jbe	.L682
	addq	$11, %rdi
	movq	%rcx, %rax
	subq	%rcx, %rdi
	andl	$31, %eax
	shrq	$2, %rdi
	shrq	$2, %rax
	leaq	1(%rdi), %r8
	negq	%rax
	andl	$7, %eax
	cmpq	%r8, %rax
	movq	%r8, %rdi
	cmova	%r8, %rax
	cmpq	$17, %r8
	ja	.L705
.L683:
	addl	(%rcx), %edx
	cmpq	$1, %rdi
	leaq	4(%rcx), %rax
	jbe	.L685
	addl	4(%rcx), %edx
	cmpq	$2, %rdi
	leaq	8(%rcx), %rax
	jbe	.L685
	addl	8(%rcx), %edx
	cmpq	$3, %rdi
	leaq	12(%rcx), %rax
	jbe	.L685
	addl	12(%rcx), %edx
	cmpq	$4, %rdi
	leaq	16(%rcx), %rax
	jbe	.L685
	addl	16(%rcx), %edx
	cmpq	$5, %rdi
	leaq	20(%rcx), %rax
	jbe	.L685
	addl	20(%rcx), %edx
	cmpq	$6, %rdi
	leaq	24(%rcx), %rax
	jbe	.L685
	addl	24(%rcx), %edx
	cmpq	$7, %rdi
	leaq	28(%rcx), %rax
	jbe	.L685
	addl	28(%rcx), %edx
	cmpq	$8, %rdi
	leaq	32(%rcx), %rax
	jbe	.L685
	addl	32(%rcx), %edx
	cmpq	$9, %rdi
	leaq	36(%rcx), %rax
	jbe	.L685
	movl	36(%rcx), %esi
	leaq	40(%rcx), %rax
	addl	%edx, %esi
	cmpq	$10, %rdi
	movl	%esi, %edx
	jbe	.L685
	addl	40(%rcx), %esi
	cmpq	$11, %rdi
	leaq	44(%rcx), %rax
	movl	%esi, %edx
	jbe	.L685
	addl	44(%rcx), %edx
	cmpq	$12, %rdi
	leaq	48(%rcx), %rax
	jbe	.L685
	addl	48(%rcx), %edx
	cmpq	$13, %rdi
	leaq	52(%rcx), %rax
	jbe	.L685
	addl	52(%rcx), %edx
	cmpq	$14, %rdi
	leaq	56(%rcx), %rax
	jbe	.L685
	addl	56(%rcx), %edx
	cmpq	$15, %rdi
	leaq	60(%rcx), %rax
	jbe	.L685
	addl	60(%rcx), %edx
	cmpq	$16, %rdi
	leaq	64(%rcx), %rax
	jbe	.L685
	addl	64(%rcx), %edx
	leaq	68(%rcx), %rax
.L685:
	cmpq	%rdi, %r8
	je	.L682
.L684:
	subq	%rdi, %r8
	movq	%r8, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L687
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %rsi
	xorl	%ecx, %ecx
.L693:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rcx, %r11
	ja	.L693
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r10,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%r10, %r8
	je	.L682
.L687:
	leaq	4(%rax), %rcx
	addl	(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L682
	leaq	8(%rax), %rcx
	addl	4(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L682
	leaq	12(%rax), %rcx
	addl	8(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L682
	leaq	16(%rax), %rcx
	addl	12(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L682
	leaq	20(%rax), %rcx
	addl	16(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L682
	leaq	24(%rax), %rcx
	addl	20(%rax), %edx
	cmpq	%rcx, %r9
	jbe	.L682
	addl	24(%rax), %edx
.L682:
	movl	%edx, (%rbx)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L704:
	addl	16(%rax), %edx
	leaq	32(%rax), %r8
	addl	20(%rax), %edx
	addl	24(%rax), %edx
	addl	28(%rax), %edx
	cmpq	%r8, %rdi
	jbe	.L679
	addl	32(%rax), %edx
	addl	36(%rax), %edx
	addl	40(%rax), %edx
	addl	44(%rax), %edx
	jmp	.L679
.L695:
	movq	%rcx, %rax
	xorl	%edx, %edx
	jmp	.L675
.L705:
	testq	%rax, %rax
	jne	.L706
	xorl	%edi, %edi
	movq	%rcx, %rax
	jmp	.L684
.L694:
	xorl	%edx, %edx
	jmp	.L674
.L706:
	movq	%rax, %rdi
	jmp	.L683
unroll8_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r13, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	movq	%rbx, %rdx
	movq	%rax, %rcx
	sarq	$63, %rdx
	shrq	$61, %rdx
	leaq	(%rbx,%rdx), %rdi
	andl	$7, %edi
	subq	%rdx, %rdi
	subq	%rdi, %rbx
	leaq	(%rax,%rbx,4), %rax
	cmpq	%rax, %rcx
	jae	.L725
	movq	%rcx, %rdx
	notq	%rdx
	addq	%rax, %rdx
	shrq	$5, %rdx
	leaq	1(%rdx), %rsi
	cmpq	$1, %rsi
	jbe	.L709
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
.L712:
	movq	%rdx, %r8
	addq	$1, %rdx
	salq	$5, %r8
	cmpq	%rdx, %rsi
	vmovdqu	(%rcx,%r8), %xmm1
	vinserti128	$0x1, 16(%rcx,%r8), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L712
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %edx
	vpextrd	$2, %xmm0, %r9d
	addl	%r8d, %edx
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edx
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edx
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %edx
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %edx
	vpextrd	$3, %xmm0, %r8d
	addl	%r9d, %edx
	addl	%r8d, %edx
.L711:
	salq	$5, %rsi
	addq	%rsi, %rcx
.L708:
	leaq	(%rax,%rdi,4), %r8
	cmpq	%rcx, %r8
	jbe	.L713
	leaq	4(%rcx), %rax
	leaq	3(%r8), %r9
	movq	%rcx, %rdi
	andl	$31, %edi
	subq	%rax, %r9
	shrq	$2, %rdi
	shrq	$2, %r9
	negq	%rdi
	addq	$1, %r9
	andl	$7, %edi
	cmpq	%rdi, %r9
	movq	%rdi, %rsi
	movq	%r9, %rdi
	cmovbe	%r9, %rsi
	cmpq	$17, %r9
	ja	.L733
.L714:
	addl	(%rcx), %edx
	cmpq	$1, %rdi
	jbe	.L716
	addl	4(%rcx), %edx
	cmpq	$2, %rdi
	leaq	8(%rcx), %rax
	jbe	.L716
	addl	8(%rcx), %edx
	cmpq	$3, %rdi
	leaq	12(%rcx), %rax
	jbe	.L716
	addl	12(%rcx), %edx
	cmpq	$4, %rdi
	leaq	16(%rcx), %rax
	jbe	.L716
	addl	16(%rcx), %edx
	cmpq	$5, %rdi
	leaq	20(%rcx), %rax
	jbe	.L716
	addl	20(%rcx), %edx
	cmpq	$6, %rdi
	leaq	24(%rcx), %rax
	jbe	.L716
	addl	24(%rcx), %edx
	cmpq	$7, %rdi
	leaq	28(%rcx), %rax
	jbe	.L716
	addl	28(%rcx), %edx
	cmpq	$8, %rdi
	leaq	32(%rcx), %rax
	jbe	.L716
	addl	32(%rcx), %edx
	cmpq	$9, %rdi
	leaq	36(%rcx), %rax
	jbe	.L716
	movl	36(%rcx), %esi
	leaq	40(%rcx), %rax
	addl	%edx, %esi
	cmpq	$10, %rdi
	movl	%esi, %edx
	jbe	.L716
	addl	40(%rcx), %esi
	cmpq	$11, %rdi
	leaq	44(%rcx), %rax
	movl	%esi, %edx
	jbe	.L716
	addl	44(%rcx), %edx
	cmpq	$12, %rdi
	leaq	48(%rcx), %rax
	jbe	.L716
	addl	48(%rcx), %edx
	cmpq	$13, %rdi
	leaq	52(%rcx), %rax
	jbe	.L716
	addl	52(%rcx), %edx
	cmpq	$14, %rdi
	leaq	56(%rcx), %rax
	jbe	.L716
	addl	56(%rcx), %edx
	cmpq	$15, %rdi
	leaq	60(%rcx), %rax
	jbe	.L716
	addl	60(%rcx), %edx
	cmpq	$16, %rdi
	leaq	64(%rcx), %rax
	jbe	.L716
	addl	64(%rcx), %edx
	leaq	68(%rcx), %rax
.L716:
	cmpq	%rdi, %r9
	je	.L713
.L715:
	subq	%rdi, %r9
	movq	%r9, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L718
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %rsi
	xorl	%ecx, %ecx
.L724:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rcx, %r11
	ja	.L724
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r10,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%r10, %r9
	je	.L713
.L718:
	leaq	4(%rax), %rcx
	addl	(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L713
	leaq	8(%rax), %rcx
	addl	4(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L713
	leaq	12(%rax), %rcx
	addl	8(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L713
	leaq	16(%rax), %rcx
	addl	12(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L713
	leaq	20(%rax), %rcx
	addl	16(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L713
	leaq	24(%rax), %rcx
	addl	20(%rax), %edx
	cmpq	%rcx, %r8
	jbe	.L713
	addl	24(%rax), %edx
.L713:
	movl	%edx, (%r12)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L709:
	movl	(%rcx), %edx
	addl	4(%rcx), %edx
	addl	8(%rcx), %edx
	addl	12(%rcx), %edx
	addl	16(%rcx), %edx
	addl	20(%rcx), %edx
	addl	24(%rcx), %edx
	addl	28(%rcx), %edx
	jmp	.L711
.L733:
	testq	%rsi, %rsi
	jne	.L734
	xorl	%edi, %edi
	movq	%rcx, %rax
	jmp	.L715
.L725:
	xorl	%edx, %edx
	jmp	.L708
.L734:
	movq	%rsi, %rdi
	jmp	.L714
unroll16_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	movq	%rsi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	movq	%r12, %rdx
	movq	%rax, %rcx
	sarq	$63, %rdx
	shrq	$60, %rdx
	leaq	(%r12,%rdx), %rdi
	andl	$15, %edi
	subq	%rdx, %rdi
	subq	%rdi, %r12
	leaq	(%rax,%r12,4), %rsi
	cmpq	%rsi, %rax
	jae	.L751
	notq	%rax
	vpxor	%xmm0, %xmm0, %xmm0
	addq	%rsi, %rax
	movq	%rcx, %rdx
	shrq	$6, %rax
	leaq	1(%rax), %r8
	salq	$6, %rax
	vmovdqa	%ymm0, %ymm1
	leaq	64(%rcx,%rax), %rax
.L738:
	vmovdqu	(%rdx), %xmm2
	addq	$64, %rdx
	vinserti128	$0x1, -48(%rdx), %ymm2, %ymm2
	vpaddd	%ymm2, %ymm1, %ymm1
	vmovdqu	-32(%rdx), %xmm2
	vinserti128	$0x1, -16(%rdx), %ymm2, %ymm2
	cmpq	%rax, %rdx
	vpaddd	%ymm2, %ymm0, %ymm0
	jne	.L738
	vpaddd	%ymm0, %ymm1, %ymm0
	vmovdqa	%xmm0, %xmm2
	salq	$6, %r8
	vextracti128	$0x1, %ymm0, %xmm0
	addq	%r8, %rcx
	vpextrd	$1, %xmm2, %edx
	vpextrd	$0, %xmm2, %eax
	vpextrd	$2, %xmm0, %r9d
	addl	%edx, %eax
	vpextrd	$2, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%r9d, %eax
	addl	%edx, %eax
.L736:
	leaq	(%rsi,%rdi,4), %r8
	cmpq	%rcx, %r8
	jbe	.L739
	leaq	4(%rcx), %rdx
	leaq	3(%r8), %r9
	movq	%rcx, %rdi
	andl	$31, %edi
	subq	%rdx, %r9
	shrq	$2, %rdi
	shrq	$2, %r9
	negq	%rdi
	addq	$1, %r9
	andl	$7, %edi
	cmpq	%rdi, %r9
	movq	%rdi, %rsi
	movq	%r9, %rdi
	cmovbe	%r9, %rsi
	cmpq	$17, %r9
	ja	.L759
.L740:
	addl	(%rcx), %eax
	cmpq	$1, %rdi
	jbe	.L742
	addl	4(%rcx), %eax
	cmpq	$2, %rdi
	leaq	8(%rcx), %rdx
	jbe	.L742
	addl	8(%rcx), %eax
	cmpq	$3, %rdi
	leaq	12(%rcx), %rdx
	jbe	.L742
	addl	12(%rcx), %eax
	cmpq	$4, %rdi
	leaq	16(%rcx), %rdx
	jbe	.L742
	addl	16(%rcx), %eax
	cmpq	$5, %rdi
	leaq	20(%rcx), %rdx
	jbe	.L742
	addl	20(%rcx), %eax
	cmpq	$6, %rdi
	leaq	24(%rcx), %rdx
	jbe	.L742
	addl	24(%rcx), %eax
	cmpq	$7, %rdi
	leaq	28(%rcx), %rdx
	jbe	.L742
	addl	28(%rcx), %eax
	cmpq	$8, %rdi
	leaq	32(%rcx), %rdx
	jbe	.L742
	addl	32(%rcx), %eax
	cmpq	$9, %rdi
	leaq	36(%rcx), %rdx
	jbe	.L742
	addl	36(%rcx), %eax
	cmpq	$10, %rdi
	leaq	40(%rcx), %rdx
	jbe	.L742
	addl	40(%rcx), %eax
	cmpq	$11, %rdi
	leaq	44(%rcx), %rdx
	jbe	.L742
	addl	44(%rcx), %eax
	cmpq	$12, %rdi
	leaq	48(%rcx), %rdx
	jbe	.L742
	addl	48(%rcx), %eax
	cmpq	$13, %rdi
	leaq	52(%rcx), %rdx
	jbe	.L742
	addl	52(%rcx), %eax
	cmpq	$14, %rdi
	leaq	56(%rcx), %rdx
	jbe	.L742
	addl	56(%rcx), %eax
	cmpq	$15, %rdi
	leaq	60(%rcx), %rdx
	jbe	.L742
	addl	60(%rcx), %eax
	cmpq	$16, %rdi
	leaq	64(%rcx), %rdx
	jbe	.L742
	addl	64(%rcx), %eax
	leaq	68(%rcx), %rdx
.L742:
	cmpq	%r9, %rdi
	je	.L739
.L741:
	subq	%rdi, %r9
	movq	%r9, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L744
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdi,4), %rsi
	xorl	%ecx, %ecx
.L750:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r11, %rcx
	jb	.L750
	vmovdqa	%xmm0, %xmm1
	leaq	(%rdx,%r10,4), %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %eax
	cmpq	%r9, %r10
	je	.L739
.L744:
	leaq	4(%rdx), %rcx
	addl	(%rdx), %eax
	cmpq	%rcx, %r8
	jbe	.L739
	leaq	8(%rdx), %rcx
	addl	4(%rdx), %eax
	cmpq	%rcx, %r8
	jbe	.L739
	leaq	12(%rdx), %rcx
	addl	8(%rdx), %eax
	cmpq	%rcx, %r8
	jbe	.L739
	leaq	16(%rdx), %rcx
	addl	12(%rdx), %eax
	cmpq	%rcx, %r8
	jbe	.L739
	leaq	20(%rdx), %rcx
	addl	16(%rdx), %eax
	cmpq	%rcx, %r8
	jbe	.L739
	leaq	24(%rdx), %rcx
	addl	20(%rdx), %eax
	cmpq	%rcx, %r8
	jbe	.L739
	addl	24(%rdx), %eax
.L739:
	movl	%eax, (%rbx)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L759:
	testq	%rsi, %rsi
	jne	.L760
	xorl	%edi, %edi
	movq	%rcx, %rdx
	jmp	.L741
.L751:
	xorl	%eax, %eax
	jmp	.L736
.L760:
	movq	%rsi, %rdi
	jmp	.L740
combine6:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-1(%rax), %r12
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r12, %r12
	movq	%rax, %rcx
	jle	.L781
	leaq	-2(%rbx), %rdi
	shrq	%rdi
	addq	$1, %rdi
	movq	%rdi, %rsi
	shrq	$2, %rsi
	leaq	0(,%rsi,4), %r9
	testq	%r9, %r9
	je	.L782
	cmpq	$13, %rdi
	jbe	.L782
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
.L768:
	movq	%rax, %rdx
	addq	$1, %rax
	salq	$5, %rdx
	cmpq	%rax, %rsi
	vmovdqu	(%rcx,%rdx), %xmm1
	vinserti128	$0x1, 16(%rcx,%rdx), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L768
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$0, %xmm1, %r11d
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$2, %xmm1, %r8d
	vpextrd	$3, %xmm1, %esi
	vpextrd	$0, %xmm0, %edx
	vpextrd	$1, %xmm0, %eax
	addl	%r11d, %r8d
	addl	%r10d, %esi
	addl	%edx, %r8d
	vpextrd	$2, %xmm0, %edx
	addl	%eax, %esi
	vpextrd	$3, %xmm0, %eax
	addl	%r8d, %edx
	leaq	(%r9,%r9), %r8
	addl	%esi, %eax
	cmpq	%r9, %rdi
	je	.L766
.L767:
	addl	(%rcx,%r8,4), %edx
	addl	4(%rcx,%r8,4), %eax
	addq	$2, %r8
	cmpq	%r8, %r12
	jg	.L767
.L766:
	addq	%rdi, %rdi
.L762:
	cmpq	%rdi, %rbx
	jle	.L769
	leaq	(%rcx,%rdi,4), %rsi
	movq	%rbx, %r9
	subq	%rdi, %r9
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%r9, %r8
	cmova	%r9, %r8
	cmpq	$17, %r9
	cmovbe	%r9, %r8
	testq	%r8, %r8
	je	.L771
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r11
	jbe	.L791
	addl	(%rcx,%r11,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r10
	jbe	.L799
	addl	(%rcx,%r10,4), %edx
	addq	$17, %rdi
.L772:
	cmpq	%r9, %r8
	je	.L769
.L771:
	subq	%r8, %r9
	movq	%r9, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L774
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L780:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rsi, %r11
	ja	.L780
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%r10, %r9
	je	.L769
.L774:
	leaq	1(%rdi), %rsi
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rsi, %rbx
	jle	.L769
	addl	(%rcx,%rsi,4), %edx
	leaq	2(%rdi), %rsi
	cmpq	%rsi, %rbx
	jle	.L769
	addl	(%rcx,%rsi,4), %edx
	leaq	3(%rdi), %rsi
	cmpq	%rsi, %rbx
	jle	.L769
	addl	(%rcx,%rsi,4), %edx
	leaq	4(%rdi), %rsi
	cmpq	%rsi, %rbx
	jle	.L769
	addl	(%rcx,%rsi,4), %edx
	leaq	5(%rdi), %rsi
	cmpq	%rsi, %rbx
	jle	.L769
	addq	$6, %rdi
	addl	(%rcx,%rsi,4), %edx
	cmpq	%rdi, %rbx
	jle	.L769
	addl	(%rcx,%rdi,4), %edx
.L769:
	addl	%eax, %edx
	movl	%edx, 0(%r13)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L799:
	movq	%r10, %rdi
	jmp	.L772
.L782:
	xorl	%eax, %eax
	xorl	%edx, %edx
	xorl	%r8d, %r8d
	jmp	.L767
.L781:
	xorl	%eax, %eax
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L762
.L791:
	movq	%r11, %rdi
	jmp	.L772
unroll4x2a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-3(%rax), %rbx
	movq	%r14, %rdi
	movq	%rax, %r12
	call	get_vec_start
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	testq	%rbx, %rbx
	movq	%rax, %rdi
	jle	.L810
.L812:
	addl	(%rdi,%rdx,4), %ecx
	addl	4(%rdi,%rdx,4), %r8d
	addl	8(%rdi,%rdx,4), %ecx
	addl	12(%rdi,%rdx,4), %r8d
	addq	$4, %rdx
	cmpq	%rdx, %rbx
	jg	.L812
	leaq	-4(%r12), %rax
	shrq	$2, %rax
	leaq	4(,%rax,4), %rdx
.L810:
	cmpq	%rdx, %r12
	jle	.L813
	leaq	(%rdi,%rdx,4), %rsi
	movq	%r12, %rax
	subq	%rdx, %rax
	movq	%rsi, %r9
	andl	$31, %r9d
	shrq	$2, %r9
	negq	%r9
	andl	$7, %r9d
	cmpq	%rax, %r9
	cmova	%rax, %r9
	cmpq	$17, %rax
	cmovbe	%rax, %r9
	testq	%r9, %r9
	je	.L815
	addl	(%rsi), %ecx
	cmpq	$1, %r9
	leaq	1(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$2, %r9
	leaq	2(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$3, %r9
	leaq	3(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$4, %r9
	leaq	4(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$5, %r9
	leaq	5(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$6, %r9
	leaq	6(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$7, %r9
	leaq	7(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$8, %r9
	leaq	8(%rdx), %r11
	jbe	.L834
	addl	(%rdi,%r11,4), %ecx
	cmpq	$9, %r9
	leaq	9(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$10, %r9
	leaq	10(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$11, %r9
	leaq	11(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$12, %r9
	leaq	12(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$13, %r9
	leaq	13(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$14, %r9
	leaq	14(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$15, %r9
	leaq	15(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	cmpq	$16, %r9
	leaq	16(%rdx), %r10
	jbe	.L842
	addl	(%rdi,%r10,4), %ecx
	addq	$17, %rdx
.L816:
	cmpq	%rax, %r9
	je	.L813
.L815:
	subq	%r9, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L818
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r9,4), %r9
	xorl	%esi, %esi
.L824:
	addq	$1, %rsi
	vpaddd	(%r9), %ymm0, %ymm0
	addq	$32, %r9
	cmpq	%r11, %rsi
	jb	.L824
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$0, %xmm1, %esi
	addl	%r9d, %esi
	vpextrd	$2, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$0, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$1, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$2, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm0, %r9d
	addl	%r9d, %esi
	addl	%esi, %ecx
	cmpq	%rax, %r10
	je	.L850
	vzeroupper
.L818:
	leaq	1(%rdx), %rax
	addl	(%rdi,%rdx,4), %ecx
	cmpq	%rax, %r12
	jle	.L813
	addl	(%rdi,%rax,4), %ecx
	leaq	2(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L813
	addl	(%rdi,%rax,4), %ecx
	leaq	3(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L813
	addl	(%rdi,%rax,4), %ecx
	leaq	4(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L813
	addl	(%rdi,%rax,4), %ecx
	leaq	5(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L813
	addq	$6, %rdx
	addl	(%rdi,%rax,4), %ecx
	cmpq	%rdx, %r12
	jle	.L813
	addl	(%rdi,%rdx,4), %ecx
.L813:
	addl	%r8d, %ecx
	movl	%ecx, 0(%r13)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L842:
	movq	%r10, %rdx
	jmp	.L816
.L850:
	vzeroupper
	jmp	.L813
.L834:
	movq	%r11, %rdx
	jmp	.L816
unroll8x2a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-7(%rax), %rbx
	movq	%r14, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r9
	jle	.L868
	movq	%rax, %rcx
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%r8d, %r8d
.L855:
	addl	(%rcx), %edx
	addl	4(%rcx), %edi
	addq	$8, %r8
	addl	8(%rcx), %edx
	addl	12(%rcx), %edi
	addq	$32, %rcx
	addl	-16(%rcx), %edx
	addl	-12(%rcx), %edi
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edi
	cmpq	%r8, %rbx
	jg	.L855
	leaq	-8(%r12), %rax
	shrq	$3, %rax
	leaq	8(,%rax,8), %rcx
.L853:
	cmpq	%rcx, %r12
	jle	.L856
	leaq	(%r9,%rcx,4), %rsi
	movq	%r12, %rax
	subq	%rcx, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L858
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$2, %r8
	leaq	2(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$3, %r8
	leaq	3(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$4, %r8
	leaq	4(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$5, %r8
	leaq	5(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$6, %r8
	leaq	6(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$7, %r8
	leaq	7(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$8, %r8
	leaq	8(%rcx), %r11
	jbe	.L877
	addl	(%r9,%r11,4), %edx
	cmpq	$9, %r8
	leaq	9(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$10, %r8
	leaq	10(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$11, %r8
	leaq	11(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$12, %r8
	leaq	12(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$13, %r8
	leaq	13(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$14, %r8
	leaq	14(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$15, %r8
	leaq	15(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	cmpq	$16, %r8
	leaq	16(%rcx), %r10
	jbe	.L885
	addl	(%r9,%r10,4), %edx
	addq	$17, %rcx
.L859:
	cmpq	%rax, %r8
	je	.L856
.L858:
	subq	%r8, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L861
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L867:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%r11, %rsi
	jb	.L867
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rcx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%rax, %r10
	je	.L893
	vzeroupper
.L861:
	leaq	1(%rcx), %rax
	addl	(%r9,%rcx,4), %edx
	cmpq	%rax, %r12
	jle	.L856
	addl	(%r9,%rax,4), %edx
	leaq	2(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L856
	addl	(%r9,%rax,4), %edx
	leaq	3(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L856
	addl	(%r9,%rax,4), %edx
	leaq	4(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L856
	addl	(%r9,%rax,4), %edx
	leaq	5(%rcx), %rax
	cmpq	%rax, %r12
	jle	.L856
	addq	$6, %rcx
	addl	(%r9,%rax,4), %edx
	cmpq	%rcx, %r12
	jle	.L856
	addl	(%r9,%rcx,4), %edx
.L856:
	addl	%edi, %edx
	movl	%edx, 0(%r13)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L885:
	movq	%r10, %rcx
	jmp	.L859
.L893:
	vzeroupper
	jmp	.L856
.L868:
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	jmp	.L853
.L877:
	movq	%r11, %rcx
	jmp	.L859
unroll3x3a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-2(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	testq	%rbx, %rbx
	movq	%rax, %rdi
	jle	.L896
.L897:
	addl	(%rdi,%rdx,4), %ecx
	addl	4(%rdi,%rdx,4), %r9d
	addl	8(%rdi,%rdx,4), %r8d
	addq	$3, %rdx
	cmpq	%rdx, %rbx
	jg	.L897
.L896:
	cmpq	%rdx, %r12
	jle	.L898
	leaq	(%rdi,%rdx,4), %rsi
	movq	%r12, %rax
	subq	%rdx, %rax
	movq	%rsi, %r10
	andl	$31, %r10d
	shrq	$2, %r10
	negq	%r10
	andl	$7, %r10d
	cmpq	%rax, %r10
	cmova	%rax, %r10
	cmpq	$17, %rax
	cmovbe	%rax, %r10
	testq	%r10, %r10
	je	.L900
	addl	(%rsi), %ecx
	cmpq	$1, %r10
	leaq	1(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$2, %r10
	leaq	2(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$3, %r10
	leaq	3(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$4, %r10
	leaq	4(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$5, %r10
	leaq	5(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$6, %r10
	leaq	6(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$7, %r10
	leaq	7(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$8, %r10
	leaq	8(%rdx), %rbx
	jbe	.L919
	addl	(%rdi,%rbx,4), %ecx
	cmpq	$9, %r10
	leaq	9(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$10, %r10
	leaq	10(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$11, %r10
	leaq	11(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$12, %r10
	leaq	12(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$13, %r10
	leaq	13(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$14, %r10
	leaq	14(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$15, %r10
	leaq	15(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	cmpq	$16, %r10
	leaq	16(%rdx), %r11
	jbe	.L927
	addl	(%rdi,%r11,4), %ecx
	addq	$17, %rdx
.L901:
	cmpq	%rax, %r10
	je	.L898
.L900:
	subq	%r10, %rax
	movq	%rax, %rbx
	shrq	$3, %rbx
	leaq	0(,%rbx,8), %r11
	testq	%r11, %r11
	je	.L903
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r10,4), %r10
	xorl	%esi, %esi
.L909:
	addq	$1, %rsi
	vpaddd	(%r10), %ymm0, %ymm0
	addq	$32, %r10
	cmpq	%rsi, %rbx
	ja	.L909
	vmovdqa	%xmm0, %xmm1
	addq	%r11, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$0, %xmm1, %esi
	addl	%r10d, %esi
	vpextrd	$2, %xmm1, %r10d
	addl	%r10d, %esi
	vpextrd	$3, %xmm1, %r10d
	addl	%r10d, %esi
	vpextrd	$0, %xmm0, %r10d
	addl	%r10d, %esi
	vpextrd	$1, %xmm0, %r10d
	addl	%r10d, %esi
	vpextrd	$2, %xmm0, %r10d
	addl	%r10d, %esi
	vpextrd	$3, %xmm0, %r10d
	addl	%r10d, %esi
	addl	%esi, %ecx
	cmpq	%rax, %r11
	je	.L935
	vzeroupper
.L903:
	leaq	1(%rdx), %rax
	addl	(%rdi,%rdx,4), %ecx
	cmpq	%rax, %r12
	jle	.L898
	addl	(%rdi,%rax,4), %ecx
	leaq	2(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L898
	addl	(%rdi,%rax,4), %ecx
	leaq	3(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L898
	addl	(%rdi,%rax,4), %ecx
	leaq	4(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L898
	addl	(%rdi,%rax,4), %ecx
	leaq	5(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L898
	addq	$6, %rdx
	addl	(%rdi,%rax,4), %ecx
	cmpq	%rdx, %r12
	jle	.L898
	addl	(%rdi,%rdx,4), %ecx
.L898:
	addl	%r9d, %ecx
	addl	%ecx, %r8d
	movl	%r8d, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L927:
	movq	%r11, %rdx
	jmp	.L901
.L935:
	vzeroupper
	jmp	.L898
.L919:
	movq	%rbx, %rdx
	jmp	.L901
unroll4x4a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-3(%rax), %r13
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r13, %r13
	movq	%rax, %rcx
	jle	.L957
	leaq	-4(%rbx), %rdi
	shrq	$2, %rdi
	addq	$1, %rdi
	movq	%rdi, %rax
	shrq	%rax
	movq	%rax, %r11
	addq	%r11, %r11
	je	.L958
	cmpq	$19, %rdi
	jbe	.L958
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%edx, %edx
.L944:
	movq	%rdx, %r8
	addq	$1, %rdx
	salq	$5, %r8
	cmpq	%rdx, %rax
	vmovdqu	(%rcx,%r8), %xmm1
	vinserti128	$0x1, 16(%rcx,%r8), %ymm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L944
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$0, %xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm0, %eax
	vpextrd	$2, %xmm1, %r10d
	vpextrd	$3, %xmm1, %r9d
	addl	%eax, %edx
	vpextrd	$1, %xmm0, %eax
	addl	%eax, %esi
	vpextrd	$2, %xmm0, %eax
	addl	%eax, %r10d
	vpextrd	$3, %xmm0, %eax
	addl	%eax, %r9d
	cmpq	%r11, %rdi
	leaq	0(,%r11,4), %rax
	je	.L942
.L939:
	leaq	(%rcx,%rax,4), %r8
.L943:
	addq	$4, %rax
	addl	(%r8), %edx
	addl	4(%r8), %esi
	addl	8(%r8), %r10d
	addl	12(%r8), %r9d
	addq	$16, %r8
	cmpq	%rax, %r13
	jg	.L943
.L942:
	salq	$2, %rdi
	addl	%r10d, %r9d
.L938:
	cmpq	%rdi, %rbx
	jle	.L945
	leaq	(%rcx,%rdi,4), %r11
	movq	%rbx, %rax
	subq	%rdi, %rax
	movq	%r11, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L947
	addl	(%r11), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r13
	jbe	.L967
	addl	(%rcx,%r13,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r10
	jbe	.L975
	addl	(%rcx,%r10,4), %edx
	addq	$17, %rdi
.L948:
	cmpq	%rax, %r8
	je	.L945
.L947:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r13
	testq	%r13, %r13
	je	.L950
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r11,%r8,4), %r11
	xorl	%r8d, %r8d
.L956:
	addq	$1, %r8
	vpaddd	(%r11), %ymm0, %ymm0
	addq	$32, %r11
	cmpq	%r10, %r8
	jb	.L956
	vmovdqa	%xmm0, %xmm1
	addq	%r13, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$0, %xmm1, %r8d
	addl	%r10d, %r8d
	vpextrd	$2, %xmm1, %r10d
	addl	%r10d, %r8d
	vpextrd	$3, %xmm1, %r10d
	addl	%r10d, %r8d
	vpextrd	$0, %xmm0, %r10d
	addl	%r10d, %r8d
	vpextrd	$1, %xmm0, %r10d
	addl	%r10d, %r8d
	vpextrd	$2, %xmm0, %r10d
	addl	%r10d, %r8d
	vpextrd	$3, %xmm0, %r10d
	addl	%r10d, %r8d
	addl	%r8d, %edx
	cmpq	%rax, %r13
	je	.L945
.L950:
	leaq	1(%rdi), %rax
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rax, %rbx
	jle	.L945
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L945
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L945
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L945
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L945
	addq	$6, %rdi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rdi, %rbx
	jle	.L945
	addl	(%rcx,%rdi,4), %edx
.L945:
	addl	%esi, %edx
	addl	%edx, %r9d
	movl	%r9d, (%r12)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L975:
	movq	%r10, %rdi
	jmp	.L948
.L958:
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%esi, %esi
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.L939
.L957:
	xorl	%r9d, %r9d
	xorl	%esi, %esi
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L938
.L967:
	movq	%r13, %rdi
	jmp	.L948
unroll8x4a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-7(%rax), %rbx
	movq	%r14, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r11
	jle	.L1001
	movq	%rax, %rdx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%ecx, %ecx
	xorl	%r10d, %r10d
.L988:
	addl	(%rdx), %ecx
	addl	4(%rdx), %r9d
	addq	$8, %r10
	addl	8(%rdx), %r8d
	addl	12(%rdx), %edi
	addq	$32, %rdx
	addl	-16(%rdx), %ecx
	addl	-12(%rdx), %r9d
	addl	-8(%rdx), %r8d
	addl	-4(%rdx), %edi
	cmpq	%r10, %rbx
	jg	.L988
	leaq	-8(%r12), %rax
	shrq	$3, %rax
	leaq	8(,%rax,8), %rdx
.L986:
	cmpq	%rdx, %r12
	jle	.L989
	leaq	(%r11,%rdx,4), %rsi
	movq	%r12, %rax
	subq	%rdx, %rax
	movq	%rsi, %r10
	andl	$31, %r10d
	shrq	$2, %r10
	negq	%r10
	andl	$7, %r10d
	cmpq	%rax, %r10
	cmova	%rax, %r10
	cmpq	$17, %rax
	cmovbe	%rax, %r10
	testq	%r10, %r10
	je	.L991
	addl	(%rsi), %ecx
	cmpq	$1, %r10
	leaq	1(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$2, %r10
	leaq	2(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$3, %r10
	leaq	3(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$4, %r10
	leaq	4(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$5, %r10
	leaq	5(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$6, %r10
	leaq	6(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$7, %r10
	leaq	7(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$8, %r10
	leaq	8(%rdx), %r14
	movl	%ecx, %ebx
	jbe	.L1010
	addl	(%r11,%r14,4), %ebx
	cmpq	$9, %r10
	movl	%ebx, %ecx
	leaq	9(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$10, %r10
	leaq	10(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$11, %r10
	leaq	11(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$12, %r10
	leaq	12(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$13, %r10
	leaq	13(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$14, %r10
	leaq	14(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$15, %r10
	leaq	15(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	cmpq	$16, %r10
	leaq	16(%rdx), %rbx
	jbe	.L1018
	addl	(%r11,%rbx,4), %ecx
	addq	$17, %rdx
.L992:
	cmpq	%rax, %r10
	je	.L989
.L991:
	subq	%r10, %rax
	movq	%rax, %r14
	shrq	$3, %r14
	leaq	0(,%r14,8), %rbx
	testq	%rbx, %rbx
	je	.L994
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r10,4), %r10
	xorl	%esi, %esi
.L1000:
	addq	$1, %rsi
	vpaddd	(%r10), %ymm0, %ymm0
	addq	$32, %r10
	cmpq	%r14, %rsi
	jb	.L1000
	vmovdqa	%xmm0, %xmm1
	addq	%rbx, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$0, %xmm1, %esi
	addl	%r10d, %esi
	vpextrd	$2, %xmm1, %r10d
	addl	%r10d, %esi
	vpextrd	$3, %xmm1, %r10d
	addl	%r10d, %esi
	vpextrd	$0, %xmm0, %r10d
	addl	%r10d, %esi
	vpextrd	$1, %xmm0, %r10d
	addl	%r10d, %esi
	vpextrd	$2, %xmm0, %r10d
	addl	%r10d, %esi
	vpextrd	$3, %xmm0, %r10d
	addl	%r10d, %esi
	addl	%esi, %ecx
	cmpq	%rax, %rbx
	je	.L1026
	vzeroupper
.L994:
	leaq	1(%rdx), %rax
	addl	(%r11,%rdx,4), %ecx
	cmpq	%rax, %r12
	jle	.L989
	addl	(%r11,%rax,4), %ecx
	leaq	2(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L989
	addl	(%r11,%rax,4), %ecx
	leaq	3(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L989
	addl	(%r11,%rax,4), %ecx
	leaq	4(%rdx), %rax
	cmpq	%r12, %rax
	jge	.L989
	addl	(%r11,%rax,4), %ecx
	leaq	5(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L989
	addq	$6, %rdx
	addl	(%r11,%rax,4), %ecx
	cmpq	%rdx, %r12
	jle	.L989
	addl	(%r11,%rdx,4), %ecx
.L989:
	addl	%r9d, %ecx
	addl	%ecx, %r8d
	addl	%r8d, %edi
	movl	%edi, 0(%r13)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1018:
	movq	%rbx, %rdx
	jmp	.L992
.L1026:
	vzeroupper
	jmp	.L989
.L1001:
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	jmp	.L986
.L1010:
	movq	%r14, %rdx
	jmp	.L992
unroll12x6a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-11(%rax), %r15
	movq	%rbx, %rdi
	movq	%rax, %r13
	call	get_vec_start
	testq	%r15, %r15
	movq	%rax, %r12
	jle	.L1044
	movq	%rax, %rdx
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
.L1031:
	addl	(%rdx), %ecx
	addl	4(%rdx), %r8d
	addq	$12, %rdi
	addl	8(%rdx), %ebx
	addl	12(%rdx), %r11d
	addq	$48, %rdx
	addl	-32(%rdx), %r10d
	addl	-28(%rdx), %r9d
	addl	-24(%rdx), %ecx
	addl	-20(%rdx), %r8d
	addl	-16(%rdx), %ebx
	addl	-12(%rdx), %r11d
	addl	-8(%rdx), %r10d
	addl	-4(%rdx), %r9d
	cmpq	%rdi, %r15
	jg	.L1031
	addl	%ebx, %r11d
	addl	%r10d, %r9d
.L1029:
	cmpq	%rdi, %r13
	jle	.L1032
	leaq	(%r12,%rdi,4), %rsi
	movq	%r13, %rax
	subq	%rdi, %rax
	movq	%rsi, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1034
	addl	(%rsi), %ecx
	cmpq	$1, %rdx
	leaq	1(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rdi), %rbx
	jbe	.L1053
	addl	(%r12,%rbx,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rdi), %r10
	jbe	.L1061
	addl	(%r12,%r10,4), %ecx
	addq	$17, %rdi
.L1035:
	cmpq	%rdx, %rax
	je	.L1032
.L1034:
	subq	%rdx, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %rbx
	testq	%rbx, %rbx
	je	.L1037
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%rdx,4), %rsi
	xorl	%edx, %edx
.L1043:
	addq	$1, %rdx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r10, %rdx
	jb	.L1043
	vmovdqa	%xmm0, %xmm1
	addq	%rbx, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %edx
	addl	%esi, %edx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %edx
	addl	%edx, %ecx
	cmpq	%rax, %rbx
	je	.L1069
	vzeroupper
.L1037:
	leaq	1(%rdi), %rax
	addl	(%r12,%rdi,4), %ecx
	cmpq	%rax, %r13
	jle	.L1032
	addl	(%r12,%rax,4), %ecx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1032
	addl	(%r12,%rax,4), %ecx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1032
	addl	(%r12,%rax,4), %ecx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1032
	addl	(%r12,%rax,4), %ecx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1032
	addq	$6, %rdi
	addl	(%r12,%rax,4), %ecx
	cmpq	%rdi, %r13
	jle	.L1032
	addl	(%r12,%rdi,4), %ecx
.L1032:
	addl	%r8d, %ecx
	addl	%ecx, %r11d
	addl	%r11d, %r9d
	movl	%r9d, (%r14)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1061:
	movq	%r10, %rdi
	jmp	.L1035
.L1069:
	vzeroupper
	jmp	.L1032
.L1044:
	xorl	%r9d, %r9d
	xorl	%r11d, %r11d
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
	jmp	.L1029
.L1053:
	movq	%rbx, %rdi
	jmp	.L1035
unroll12x12a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	movq	%rsi, (%rsp)
	call	vec_length
	movq	%rax, 8(%rsp)
	subq	$11, %rax
	movq	%rbx, %rdi
	movq	%rax, %r14
	movq	%rax, 24(%rsp)
	call	get_vec_start
	testq	%r14, %r14
	movq	%rax, 16(%rsp)
	jle	.L1087
	movq	%rax, %rdx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r12d, %r12d
	xorl	%r13d, %r13d
	xorl	%r14d, %r14d
	xorl	%r15d, %r15d
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	xorl	%esi, %esi
.L1074:
	addl	(%rdx), %ecx
	addl	24(%rdx), %ebx
	addq	$12, %rsi
	addl	4(%rdx), %eax
	addl	28(%rdx), %r11d
	addq	$48, %rdx
	addl	-40(%rdx), %r15d
	addl	-16(%rdx), %r10d
	addl	-36(%rdx), %r14d
	addl	-12(%rdx), %r9d
	addl	-32(%rdx), %r13d
	addl	-8(%rdx), %r8d
	addl	-28(%rdx), %r12d
	addl	-4(%rdx), %edi
	cmpq	%rsi, 24(%rsp)
	jg	.L1074
	addl	%r13d, %r12d
	leal	(%r8,%rdi), %r13d
	addl	%r15d, %r14d
	addl	%ebx, %r11d
	addl	%r10d, %r9d
.L1072:
	movq	8(%rsp), %rdi
	cmpq	%rsi, %rdi
	jle	.L1075
	movq	16(%rsp), %r15
	subq	%rsi, %rdi
	leaq	(%r15,%rsi,4), %r8
	movq	%r8, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rdi, %rdx
	cmova	%rdi, %rdx
	cmpq	$17, %rdi
	cmovbe	%rdi, %rdx
	testq	%rdx, %rdx
	je	.L1077
	addl	(%r8), %ecx
	cmpq	$1, %rdx
	leaq	1(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rsi), %rbx
	jbe	.L1096
	addl	(%r15,%rbx,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rsi), %r10
	jbe	.L1104
	addl	(%r15,%r10,4), %ecx
	addq	$17, %rsi
.L1078:
	cmpq	%rdx, %rdi
	je	.L1075
.L1077:
	subq	%rdx, %rdi
	movq	%rdi, %rbx
	shrq	$3, %rbx
	leaq	0(,%rbx,8), %r10
	testq	%r10, %r10
	je	.L1080
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r8,%rdx,4), %r8
	xorl	%edx, %edx
.L1086:
	addq	$1, %rdx
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rbx, %rdx
	jb	.L1086
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rsi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %edx
	addl	%r8d, %edx
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edx
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edx
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %edx
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %edx
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %edx
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %edx
	addl	%edx, %ecx
	cmpq	%rdi, %r10
	je	.L1113
	vzeroupper
.L1080:
	movq	16(%rsp), %rdi
	movq	8(%rsp), %rbx
	leaq	1(%rsi), %rdx
	addl	(%rdi,%rsi,4), %ecx
	cmpq	%rdx, %rbx
	jle	.L1075
	addl	(%rdi,%rdx,4), %ecx
	leaq	2(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1075
	addl	(%rdi,%rdx,4), %ecx
	leaq	3(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1075
	addl	(%rdi,%rdx,4), %ecx
	leaq	4(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1075
	addl	(%rdi,%rdx,4), %ecx
	leaq	5(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1075
	addq	$6, %rsi
	addl	(%rdi,%rdx,4), %ecx
	cmpq	%rsi, %rbx
	jle	.L1075
	movq	16(%rsp), %rdi
	addl	(%rdi,%rsi,4), %ecx
.L1075:
	addl	%eax, %ecx
	movq	(%rsp), %rax
	addl	%ecx, %r14d
	addl	%r14d, %r12d
	addl	%r12d, %r11d
	addl	%r11d, %r9d
	addl	%r9d, %r13d
	movl	%r13d, (%rax)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1104:
	movq	%r10, %rsi
	jmp	.L1078
.L1087:
	xorl	%r13d, %r13d
	xorl	%r9d, %r9d
	xorl	%r11d, %r11d
	xorl	%r12d, %r12d
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	jmp	.L1072
.L1096:
	movq	%rbx, %rsi
	jmp	.L1078
.L1113:
	vzeroupper
	jmp	.L1075
unroll5x5a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-4(%rax), %r12
	movq	%rbx, %rdi
	movq	%rax, %r13
	call	get_vec_start
	testq	%r12, %r12
	movq	%rax, %rbx
	jle	.L1131
	movq	%rax, %rdx
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
.L1118:
	addq	$5, %rdi
	addl	(%rdx), %ecx
	addl	4(%rdx), %r8d
	addl	8(%rdx), %r11d
	addl	12(%rdx), %r10d
	addq	$20, %rdx
	addl	-4(%rdx), %r9d
	cmpq	%rdi, %r12
	jg	.L1118
	addl	%r11d, %r10d
	addl	%r10d, %r9d
.L1116:
	cmpq	%rdi, %r13
	jle	.L1119
	leaq	(%rbx,%rdi,4), %rsi
	movq	%r13, %rax
	subq	%rdi, %rax
	movq	%rsi, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1121
	addl	(%rsi), %ecx
	cmpq	$1, %rdx
	leaq	1(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rdi), %r11
	jbe	.L1140
	addl	(%rbx,%r11,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rdi), %r10
	jbe	.L1148
	addl	(%rbx,%r10,4), %ecx
	addq	$17, %rdi
.L1122:
	cmpq	%rdx, %rax
	je	.L1119
.L1121:
	subq	%rdx, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r12
	testq	%r12, %r12
	je	.L1124
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%rdx,4), %rsi
	xorl	%edx, %edx
.L1130:
	addq	$1, %rdx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r10, %rdx
	jb	.L1130
	vmovdqa	%xmm0, %xmm1
	addq	%r12, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %edx
	addl	%esi, %edx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %edx
	addl	%edx, %ecx
	cmpq	%rax, %r12
	je	.L1156
	vzeroupper
.L1124:
	leaq	1(%rdi), %rax
	addl	(%rbx,%rdi,4), %ecx
	cmpq	%rax, %r13
	jle	.L1119
	addl	(%rbx,%rax,4), %ecx
	leaq	2(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1119
	addl	(%rbx,%rax,4), %ecx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1119
	addl	(%rbx,%rax,4), %ecx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1119
	addl	(%rbx,%rax,4), %ecx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1119
	addq	$6, %rdi
	addl	(%rbx,%rax,4), %ecx
	cmpq	%rdi, %r13
	jle	.L1119
	addl	(%rbx,%rdi,4), %ecx
.L1119:
	addl	%r8d, %ecx
	addl	%ecx, %r9d
	movl	%r9d, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1148:
	movq	%r10, %rdi
	jmp	.L1122
.L1156:
	vzeroupper
	jmp	.L1119
.L1131:
	xorl	%r9d, %r9d
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
	jmp	.L1116
.L1140:
	movq	%r11, %rdi
	jmp	.L1122
unroll6x6a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-5(%rax), %r15
	movq	%rbx, %rdi
	movq	%rax, %r13
	call	get_vec_start
	testq	%r15, %r15
	movq	%rax, %r12
	jle	.L1174
	movq	%rax, %rdx
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
.L1161:
	addq	$6, %rdi
	addl	(%rdx), %ecx
	addl	4(%rdx), %r8d
	addl	8(%rdx), %ebx
	addl	12(%rdx), %r11d
	addq	$24, %rdx
	addl	-8(%rdx), %r10d
	addl	-4(%rdx), %r9d
	cmpq	%rdi, %r15
	jg	.L1161
	addl	%ebx, %r11d
	addl	%r10d, %r9d
.L1159:
	cmpq	%rdi, %r13
	jle	.L1162
	leaq	(%r12,%rdi,4), %rsi
	movq	%r13, %rax
	subq	%rdi, %rax
	movq	%rsi, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1164
	addl	(%rsi), %ecx
	cmpq	$1, %rdx
	leaq	1(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rdi), %rbx
	jbe	.L1183
	addl	(%r12,%rbx,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rdi), %r10
	jbe	.L1191
	addl	(%r12,%r10,4), %ecx
	addq	$17, %rdi
.L1165:
	cmpq	%rdx, %rax
	je	.L1162
.L1164:
	subq	%rdx, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %rbx
	testq	%rbx, %rbx
	je	.L1167
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%rdx,4), %rsi
	xorl	%edx, %edx
.L1173:
	addq	$1, %rdx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r10, %rdx
	jb	.L1173
	vmovdqa	%xmm0, %xmm1
	addq	%rbx, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %edx
	addl	%esi, %edx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %edx
	addl	%edx, %ecx
	cmpq	%rax, %rbx
	je	.L1199
	vzeroupper
.L1167:
	leaq	1(%rdi), %rax
	addl	(%r12,%rdi,4), %ecx
	cmpq	%rax, %r13
	jle	.L1162
	addl	(%r12,%rax,4), %ecx
	leaq	2(%rdi), %rax
	cmpq	%r13, %rax
	jge	.L1162
	addl	(%r12,%rax,4), %ecx
	leaq	3(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1162
	addl	(%r12,%rax,4), %ecx
	leaq	4(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1162
	addl	(%r12,%rax,4), %ecx
	leaq	5(%rdi), %rax
	cmpq	%rax, %r13
	jle	.L1162
	addq	$6, %rdi
	addl	(%r12,%rax,4), %ecx
	cmpq	%rdi, %r13
	jle	.L1162
	addl	(%r12,%rdi,4), %ecx
.L1162:
	addl	%r8d, %ecx
	addl	%ecx, %r11d
	addl	%r11d, %r9d
	movl	%r9d, (%r14)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1191:
	movq	%r10, %rdi
	jmp	.L1165
.L1199:
	vzeroupper
	jmp	.L1162
.L1174:
	xorl	%r9d, %r9d
	xorl	%r11d, %r11d
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
	jmp	.L1159
.L1183:
	movq	%rbx, %rdi
	jmp	.L1165
unroll7x7a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	movq	%rsi, %r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-6(%rax), %r13
	movq	%rbx, %rdi
	movq	%rax, %r14
	call	get_vec_start
	testq	%r13, %r13
	jle	.L1217
	movq	%rax, %rdx
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r12d, %r12d
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
.L1204:
	addq	$7, %rdi
	addl	(%rdx), %ecx
	addl	4(%rdx), %r8d
	addl	8(%rdx), %r12d
	addl	12(%rdx), %ebx
	addq	$28, %rdx
	addl	-12(%rdx), %r11d
	addl	-8(%rdx), %r10d
	addl	-4(%rdx), %r9d
	cmpq	%rdi, %r13
	jg	.L1204
	addl	%r11d, %r10d
	addl	%r12d, %ebx
	addl	%r10d, %r9d
.L1202:
	cmpq	%rdi, %r14
	jle	.L1205
	leaq	(%rax,%rdi,4), %r13
	movq	%r14, %rsi
	subq	%rdi, %rsi
	movq	%r13, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rsi, %rdx
	cmova	%rsi, %rdx
	cmpq	$17, %rsi
	cmovbe	%rsi, %rdx
	testq	%rdx, %rdx
	je	.L1207
	addl	0(%r13), %ecx
	cmpq	$1, %rdx
	leaq	1(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rdi), %r11
	jbe	.L1226
	addl	(%rax,%r11,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rdi), %r10
	jbe	.L1234
	addl	(%rax,%r10,4), %ecx
	addq	$17, %rdi
.L1208:
	cmpq	%rdx, %rsi
	je	.L1205
.L1207:
	subq	%rdx, %rsi
	movq	%rsi, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r12
	testq	%r12, %r12
	je	.L1210
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	0(%r13,%rdx,4), %r11
	xorl	%edx, %edx
.L1216:
	addq	$1, %rdx
	vpaddd	(%r11), %ymm0, %ymm0
	addq	$32, %r11
	cmpq	%r10, %rdx
	jb	.L1216
	vmovdqa	%xmm0, %xmm1
	addq	%r12, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$0, %xmm1, %edx
	addl	%r10d, %edx
	vpextrd	$2, %xmm1, %r10d
	addl	%r10d, %edx
	vpextrd	$3, %xmm1, %r10d
	addl	%r10d, %edx
	vpextrd	$0, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$1, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$2, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$3, %xmm0, %r10d
	addl	%r10d, %edx
	addl	%edx, %ecx
	cmpq	%rsi, %r12
	je	.L1242
	vzeroupper
.L1210:
	leaq	1(%rdi), %rdx
	addl	(%rax,%rdi,4), %ecx
	cmpq	%rdx, %r14
	jle	.L1205
	addl	(%rax,%rdx,4), %ecx
	leaq	2(%rdi), %rdx
	cmpq	%rdx, %r14
	jle	.L1205
	addl	(%rax,%rdx,4), %ecx
	leaq	3(%rdi), %rdx
	cmpq	%rdx, %r14
	jle	.L1205
	addl	(%rax,%rdx,4), %ecx
	leaq	4(%rdi), %rdx
	cmpq	%r14, %rdx
	jge	.L1205
	addl	(%rax,%rdx,4), %ecx
	leaq	5(%rdi), %rdx
	cmpq	%rdx, %r14
	jle	.L1205
	addq	$6, %rdi
	addl	(%rax,%rdx,4), %ecx
	cmpq	%rdi, %r14
	jle	.L1205
	addl	(%rax,%rdi,4), %ecx
.L1205:
	addl	%r8d, %ecx
	addl	%ecx, %ebx
	addl	%ebx, %r9d
	movl	%r9d, (%r15)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1234:
	movq	%r10, %rdi
	jmp	.L1208
.L1242:
	vzeroupper
	jmp	.L1205
.L1217:
	xorl	%r9d, %r9d
	xorl	%ebx, %ebx
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
	jmp	.L1202
.L1226:
	movq	%r11, %rdi
	jmp	.L1208
unroll8x8a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	movq	%rsi, %r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-7(%rax), %r13
	movq	%rbx, %rdi
	movq	%rax, %r14
	call	get_vec_start
	testq	%r13, %r13
	jle	.L1260
	movq	%rax, %rdx
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r12d, %r12d
	xorl	%esi, %esi
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edi, %edi
.L1247:
	addq	$8, %rdi
	addl	(%rdx), %ecx
	addl	4(%rdx), %r8d
	addl	8(%rdx), %esi
	addl	12(%rdx), %r12d
	addq	$32, %rdx
	addl	-16(%rdx), %ebx
	addl	-12(%rdx), %r11d
	addl	-8(%rdx), %r10d
	addl	-4(%rdx), %r9d
	cmpq	%rdi, %r13
	jg	.L1247
	leaq	-8(%r14), %rdx
	addl	%ebx, %r11d
	addl	%r10d, %r9d
	leal	(%r11,%r9), %r10d
	addl	%esi, %r12d
	shrq	$3, %rdx
	leaq	8(,%rdx,8), %rdx
.L1245:
	cmpq	%rdx, %r14
	jle	.L1248
	leaq	(%rax,%rdx,4), %rsi
	movq	%r14, %r13
	subq	%rdx, %r13
	movq	%rsi, %rdi
	andl	$31, %edi
	shrq	$2, %rdi
	negq	%rdi
	andl	$7, %edi
	cmpq	%r13, %rdi
	cmova	%r13, %rdi
	cmpq	$17, %r13
	cmovbe	%r13, %rdi
	testq	%rdi, %rdi
	je	.L1250
	addl	(%rsi), %ecx
	cmpq	$1, %rdi
	leaq	1(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$2, %rdi
	leaq	2(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$3, %rdi
	leaq	3(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$4, %rdi
	leaq	4(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$5, %rdi
	leaq	5(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$6, %rdi
	leaq	6(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$7, %rdi
	leaq	7(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$8, %rdi
	leaq	8(%rdx), %r11
	jbe	.L1269
	addl	(%rax,%r11,4), %ecx
	cmpq	$9, %rdi
	leaq	9(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$10, %rdi
	leaq	10(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$11, %rdi
	leaq	11(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$12, %rdi
	leaq	12(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$13, %rdi
	leaq	13(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$14, %rdi
	leaq	14(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$15, %rdi
	leaq	15(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	cmpq	$16, %rdi
	leaq	16(%rdx), %r9
	jbe	.L1277
	addl	(%rax,%r9,4), %ecx
	addq	$17, %rdx
.L1251:
	cmpq	%rdi, %r13
	je	.L1248
.L1250:
	subq	%rdi, %r13
	movq	%r13, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r9
	testq	%r9, %r9
	je	.L1253
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%rdi,4), %rdi
	xorl	%esi, %esi
.L1259:
	addq	$1, %rsi
	vpaddd	(%rdi), %ymm0, %ymm0
	addq	$32, %rdi
	cmpq	%r11, %rsi
	jb	.L1259
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edi
	vpextrd	$0, %xmm1, %esi
	addl	%edi, %esi
	vpextrd	$2, %xmm1, %edi
	addl	%edi, %esi
	vpextrd	$3, %xmm1, %edi
	addl	%edi, %esi
	vpextrd	$0, %xmm0, %edi
	addl	%edi, %esi
	vpextrd	$1, %xmm0, %edi
	addl	%edi, %esi
	vpextrd	$2, %xmm0, %edi
	addl	%edi, %esi
	vpextrd	$3, %xmm0, %edi
	addl	%edi, %esi
	addl	%esi, %ecx
	cmpq	%r13, %r9
	je	.L1285
	vzeroupper
.L1253:
	leaq	1(%rdx), %rsi
	addl	(%rax,%rdx,4), %ecx
	cmpq	%rsi, %r14
	jle	.L1248
	addl	(%rax,%rsi,4), %ecx
	leaq	2(%rdx), %rsi
	cmpq	%rsi, %r14
	jle	.L1248
	addl	(%rax,%rsi,4), %ecx
	leaq	3(%rdx), %rsi
	cmpq	%rsi, %r14
	jle	.L1248
	addl	(%rax,%rsi,4), %ecx
	leaq	4(%rdx), %rsi
	cmpq	%rsi, %r14
	jle	.L1248
	addl	(%rax,%rsi,4), %ecx
	leaq	5(%rdx), %rsi
	cmpq	%r14, %rsi
	jge	.L1248
	addq	$6, %rdx
	addl	(%rax,%rsi,4), %ecx
	cmpq	%r14, %rdx
	jge	.L1248
	addl	(%rax,%rdx,4), %ecx
.L1248:
	addl	%r8d, %ecx
	addl	%ecx, %r12d
	addl	%r12d, %r10d
	movl	%r10d, (%r15)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1277:
	movq	%r9, %rdx
	jmp	.L1251
.L1285:
	vzeroupper
	jmp	.L1248
.L1260:
	xorl	%r10d, %r10d
	xorl	%r12d, %r12d
	xorl	%r8d, %r8d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	jmp	.L1245
.L1269:
	movq	%r11, %rdx
	jmp	.L1251
unroll9x9a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	subq	$16, %rsp
	movq	%rsi, 8(%rsp)
	call	vec_length
	leaq	-8(%rax), %r13
	movq	%rbx, %rdi
	movq	%rax, %r14
	call	get_vec_start
	testq	%r13, %r13
	jle	.L1303
	movq	%rax, %rdx
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r12d, %r12d
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	xorl	%ecx, %ecx
	xorl	%esi, %esi
.L1290:
	addq	$9, %rsi
	addl	(%rdx), %ecx
	addl	4(%rdx), %edi
	addl	8(%rdx), %r15d
	addl	12(%rdx), %r12d
	addq	$36, %rdx
	addl	-20(%rdx), %ebx
	addl	-16(%rdx), %r11d
	addl	-12(%rdx), %r10d
	addl	-8(%rdx), %r9d
	addl	-4(%rdx), %r8d
	cmpq	%rsi, %r13
	jg	.L1290
	addl	%ebx, %r11d
	addl	%r10d, %r9d
	addl	%r15d, %r12d
	addl	%r11d, %r9d
	addl	%r9d, %r8d
.L1288:
	cmpq	%rsi, %r14
	jle	.L1291
	leaq	(%rax,%rsi,4), %r15
	movq	%r14, %r13
	subq	%rsi, %r13
	movq	%r15, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%r13, %rdx
	cmova	%r13, %rdx
	cmpq	$17, %r13
	cmovbe	%r13, %rdx
	testq	%rdx, %rdx
	je	.L1293
	addl	(%r15), %ecx
	cmpq	$1, %rdx
	leaq	1(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rsi), %r10
	jbe	.L1312
	addl	(%rax,%r10,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rsi), %r9
	jbe	.L1320
	addl	(%rax,%r9,4), %ecx
	addq	$17, %rsi
.L1294:
	cmpq	%rdx, %r13
	je	.L1291
.L1293:
	subq	%rdx, %r13
	movq	%r13, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L1296
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r15,%rdx,4), %r11
	xorl	%edx, %edx
.L1302:
	addq	$1, %rdx
	vpaddd	(%r11), %ymm0, %ymm0
	addq	$32, %r11
	cmpq	%r10, %rdx
	jb	.L1302
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rsi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$0, %xmm1, %edx
	addl	%r10d, %edx
	vpextrd	$2, %xmm1, %r10d
	addl	%r10d, %edx
	vpextrd	$3, %xmm1, %r10d
	addl	%r10d, %edx
	vpextrd	$0, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$1, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$2, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$3, %xmm0, %r10d
	addl	%r10d, %edx
	addl	%edx, %ecx
	cmpq	%r13, %r9
	je	.L1328
	vzeroupper
.L1296:
	leaq	1(%rsi), %rdx
	addl	(%rax,%rsi,4), %ecx
	cmpq	%rdx, %r14
	jle	.L1291
	addl	(%rax,%rdx,4), %ecx
	leaq	2(%rsi), %rdx
	cmpq	%rdx, %r14
	jle	.L1291
	addl	(%rax,%rdx,4), %ecx
	leaq	3(%rsi), %rdx
	cmpq	%rdx, %r14
	jle	.L1291
	addl	(%rax,%rdx,4), %ecx
	leaq	4(%rsi), %rdx
	cmpq	%rdx, %r14
	jle	.L1291
	addl	(%rax,%rdx,4), %ecx
	leaq	5(%rsi), %rdx
	cmpq	%rdx, %r14
	jle	.L1291
	addq	$6, %rsi
	addl	(%rax,%rdx,4), %ecx
	cmpq	%rsi, %r14
	jle	.L1291
	addl	(%rax,%rsi,4), %ecx
.L1291:
	movq	8(%rsp), %rax
	addl	%edi, %ecx
	addl	%ecx, %r12d
	addl	%r12d, %r8d
	movl	%r8d, (%rax)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1320:
	movq	%r9, %rsi
	jmp	.L1294
.L1328:
	vzeroupper
	jmp	.L1291
.L1303:
	xorl	%r8d, %r8d
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	jmp	.L1288
.L1312:
	movq	%r10, %rsi
	jmp	.L1294
unroll10x10a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	subq	$16, %rsp
	movq	%rsi, (%rsp)
	call	vec_length
	leaq	-9(%rax), %r14
	movq	%rbx, %rdi
	movq	%rax, 8(%rsp)
	call	get_vec_start
	testq	%r14, %r14
	jle	.L1346
	movq	%rax, %rdx
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%r11d, %r11d
	xorl	%ebx, %ebx
	xorl	%r12d, %r12d
	xorl	%r13d, %r13d
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	xorl	%ecx, %ecx
	xorl	%esi, %esi
.L1333:
	addq	$10, %rsi
	addl	(%rdx), %ecx
	addl	4(%rdx), %edi
	addl	8(%rdx), %r15d
	addl	12(%rdx), %r13d
	addq	$40, %rdx
	addl	-24(%rdx), %r12d
	addl	-20(%rdx), %ebx
	addl	-16(%rdx), %r11d
	addl	-12(%rdx), %r10d
	addl	-8(%rdx), %r9d
	addl	-4(%rdx), %r8d
	cmpq	%rsi, %r14
	jg	.L1333
	addl	%r12d, %ebx
	addl	%r11d, %r10d
	addl	%r15d, %r13d
	addl	%ebx, %r10d
	addl	%r9d, %r8d
.L1331:
	movq	8(%rsp), %r14
	cmpq	%rsi, %r14
	jle	.L1334
	leaq	(%rax,%rsi,4), %r15
	subq	%rsi, %r14
	movq	%r15, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%r14, %rdx
	cmova	%r14, %rdx
	cmpq	$17, %r14
	cmovbe	%r14, %rdx
	testq	%rdx, %rdx
	je	.L1336
	addl	(%r15), %ecx
	cmpq	$1, %rdx
	leaq	1(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$2, %rdx
	leaq	2(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$3, %rdx
	leaq	3(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$4, %rdx
	leaq	4(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$5, %rdx
	leaq	5(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$6, %rdx
	leaq	6(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$7, %rdx
	leaq	7(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$8, %rdx
	leaq	8(%rsi), %r11
	jbe	.L1355
	addl	(%rax,%r11,4), %ecx
	cmpq	$9, %rdx
	leaq	9(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$10, %rdx
	leaq	10(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$11, %rdx
	leaq	11(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$12, %rdx
	leaq	12(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$13, %rdx
	leaq	13(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$14, %rdx
	leaq	14(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$15, %rdx
	leaq	15(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	cmpq	$16, %rdx
	leaq	16(%rsi), %r9
	jbe	.L1363
	addl	(%rax,%r9,4), %ecx
	addq	$17, %rsi
.L1337:
	cmpq	%rdx, %r14
	je	.L1334
.L1336:
	subq	%rdx, %r14
	movq	%r14, %r9
	shrq	$3, %r9
	leaq	0(,%r9,8), %r11
	testq	%r11, %r11
	je	.L1339
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r15,%rdx,4), %rbx
	xorl	%edx, %edx
.L1345:
	addq	$1, %rdx
	vpaddd	(%rbx), %ymm0, %ymm0
	addq	$32, %rbx
	cmpq	%r9, %rdx
	jb	.L1345
	vmovdqa	%xmm0, %xmm1
	addq	%r11, %rsi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r9d
	vpextrd	$0, %xmm1, %edx
	addl	%r9d, %edx
	vpextrd	$2, %xmm1, %r9d
	addl	%r9d, %edx
	vpextrd	$3, %xmm1, %r9d
	addl	%r9d, %edx
	vpextrd	$0, %xmm0, %r9d
	addl	%r9d, %edx
	vpextrd	$1, %xmm0, %r9d
	addl	%r9d, %edx
	vpextrd	$2, %xmm0, %r9d
	addl	%r9d, %edx
	vpextrd	$3, %xmm0, %r9d
	addl	%r9d, %edx
	addl	%edx, %ecx
	cmpq	%r14, %r11
	je	.L1372
	vzeroupper
.L1339:
	movq	8(%rsp), %rbx
	leaq	1(%rsi), %rdx
	addl	(%rax,%rsi,4), %ecx
	cmpq	%rdx, %rbx
	jle	.L1334
	addl	(%rax,%rdx,4), %ecx
	leaq	2(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1334
	addl	(%rax,%rdx,4), %ecx
	leaq	3(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1334
	addl	(%rax,%rdx,4), %ecx
	leaq	4(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1334
	addl	(%rax,%rdx,4), %ecx
	leaq	5(%rsi), %rdx
	cmpq	%rdx, %rbx
	jle	.L1334
	addq	$6, %rsi
	addl	(%rax,%rdx,4), %ecx
	cmpq	%rsi, %rbx
	jle	.L1334
	addl	(%rax,%rsi,4), %ecx
.L1334:
	addl	%edi, %ecx
	movq	(%rsp), %rax
	addl	%ecx, %r13d
	addl	%r13d, %r10d
	addl	%r10d, %r8d
	movl	%r8d, (%rax)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1363:
	movq	%r9, %rsi
	jmp	.L1337
.L1346:
	xorl	%r8d, %r8d
	xorl	%r10d, %r10d
	xorl	%r13d, %r13d
	xorl	%edi, %edi
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	jmp	.L1331
.L1355:
	movq	%r11, %rsi
	jmp	.L1337
.L1372:
	vzeroupper
	jmp	.L1334
unrollx2as_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	call	vec_length
	movq	%rax, %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	shrq	$63, %rbx
	addq	%rax, %rbx
	sarq	%rbx
	call	get_vec_start
	leaq	0(,%rbx,4), %r10
	testq	%rbx, %rbx
	movq	%rax, %rcx
	leaq	(%rax,%r10), %r8
	jle	.L1399
	andl	$31, %eax
	movq	%rbx, %r15
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpq	%rax, %rbx
	cmovbe	%rbx, %rax
	cmpq	$17, %rbx
	ja	.L1449
.L1376:
	cmpq	$1, %r15
	movl	(%rcx), %edi
	movl	(%r8), %edx
	jbe	.L1402
	addl	4(%rcx), %edi
	addl	4(%r8), %edx
	cmpq	$2, %r15
	jbe	.L1403
	addl	8(%rcx), %edi
	addl	8(%r8), %edx
	cmpq	$3, %r15
	jbe	.L1404
	addl	12(%rcx), %edi
	addl	12(%r8), %edx
	cmpq	$4, %r15
	jbe	.L1405
	addl	16(%rcx), %edi
	addl	16(%r8), %edx
	cmpq	$5, %r15
	jbe	.L1406
	addl	20(%rcx), %edi
	addl	20(%r8), %edx
	cmpq	$6, %r15
	jbe	.L1407
	addl	24(%rcx), %edi
	addl	24(%r8), %edx
	cmpq	$7, %r15
	jbe	.L1408
	addl	28(%rcx), %edi
	addl	28(%r8), %edx
	cmpq	$8, %r15
	jbe	.L1409
	addl	32(%rcx), %edi
	addl	32(%r8), %edx
	cmpq	$9, %r15
	jbe	.L1410
	movl	36(%r8), %eax
	addl	36(%rcx), %edi
	addl	%edx, %eax
	cmpq	$10, %r15
	movl	%eax, %edx
	jbe	.L1411
	addl	40(%r8), %eax
	addl	40(%rcx), %edi
	cmpq	$11, %r15
	movl	%eax, %edx
	jbe	.L1412
	addl	44(%rcx), %edi
	addl	44(%r8), %edx
	cmpq	$12, %r15
	jbe	.L1413
	addl	48(%rcx), %edi
	addl	48(%r8), %edx
	cmpq	$13, %r15
	jbe	.L1414
	addl	52(%rcx), %edi
	addl	52(%r8), %edx
	cmpq	$14, %r15
	jbe	.L1415
	addl	56(%rcx), %edi
	addl	56(%r8), %edx
	cmpq	$15, %r15
	jbe	.L1416
	addl	60(%rcx), %edi
	addl	60(%r8), %edx
	cmpq	$16, %r15
	jbe	.L1417
	addl	64(%rcx), %edi
	addl	64(%r8), %edx
	movl	$17, %eax
.L1378:
	cmpq	%r15, %rbx
	je	.L1375
.L1377:
	movq	%rbx, %rsi
	subq	%r15, %rsi
	movq	%rsi, 24(%rsp)
	shrq	$3, %rsi
	leaq	0(,%rsi,8), %r13
	testq	%r13, %r13
	je	.L1380
	salq	$2, %r15
	vpxor	%xmm1, %xmm1, %xmm1
	leaq	(%rcx,%r15), %r11
	movq	%rax, 8(%rsp)
	addq	%r8, %r15
	xorl	%r9d, %r9d
	movq	%r11, 16(%rsp)
	movq	16(%rsp), %rax
	vmovdqa	%ymm1, %ymm0
	xorl	%r11d, %r11d
.L1386:
	vmovdqu	(%r15,%r9), %xmm2
	addq	$1, %r11
	vpaddd	(%rax,%r9), %ymm0, %ymm0
	vinserti128	$0x1, 16(%r15,%r9), %ymm2, %ymm2
	addq	$32, %r9
	cmpq	%rsi, %r11
	vpaddd	%ymm2, %ymm1, %ymm1
	jb	.L1386
	vmovdqa	%xmm1, %xmm2
	movq	8(%rsp), %rax
	vextracti128	$0x1, %ymm1, %xmm1
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$0, %xmm2, %esi
	addq	%r13, %rax
	addl	%r9d, %esi
	vpextrd	$2, %xmm2, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm2, %r9d
	addl	%r9d, %esi
	vpextrd	$0, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$1, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$2, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm1, %r9d
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	addl	%r9d, %esi
	vpextrd	$1, %xmm1, %r9d
	addl	%esi, %edx
	vpextrd	$0, %xmm1, %esi
	addl	%r9d, %esi
	vpextrd	$2, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$0, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$1, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$2, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm0, %r9d
	addl	%r9d, %esi
	addl	%esi, %edi
	cmpq	%r13, 24(%rsp)
	je	.L1375
.L1380:
	leaq	1(%rax), %rsi
	addl	(%rcx,%rax,4), %edi
	addl	(%r8,%rax,4), %edx
	cmpq	%rsi, %rbx
	jle	.L1375
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	2(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L1375
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	3(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L1375
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	4(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L1375
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	5(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L1375
	addq	$6, %rax
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	cmpq	%rax, %rbx
	jle	.L1375
	addl	(%rcx,%rax,4), %edi
	addl	(%r8,%rax,4), %edx
.L1375:
	addq	%rbx, %rbx
	cmpq	%rbx, %r12
	jle	.L1387
	addq	%r10, %r8
	movq	%r12, %rsi
	movq	%r8, %rax
	subq	%rbx, %rsi
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpq	%rsi, %rax
	cmova	%rsi, %rax
	cmpq	$17, %rsi
	cmovbe	%rsi, %rax
	testq	%rax, %rax
	je	.L1389
	addl	(%rcx,%rbx,4), %edx
	cmpq	$1, %rax
	leaq	1(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %rax
	leaq	2(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %rax
	leaq	3(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %rax
	leaq	4(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %rax
	leaq	5(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %rax
	leaq	6(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %rax
	leaq	7(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %rax
	leaq	8(%rbx), %r10
	jbe	.L1426
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %rax
	leaq	9(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %rax
	leaq	10(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %rax
	leaq	11(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %rax
	leaq	12(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %rax
	leaq	13(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %rax
	leaq	14(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %rax
	leaq	15(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %rax
	leaq	16(%rbx), %r9
	jbe	.L1434
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rbx
.L1390:
	cmpq	%rsi, %rax
	je	.L1387
.L1389:
	subq	%rax, %rsi
	movq	%rsi, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L1392
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r8,%rax,4), %r8
	xorl	%eax, %eax
.L1398:
	addq	$1, %rax
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rax, %r10
	ja	.L1398
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rbx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %eax
	addl	%r8d, %eax
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %eax
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %eax
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %eax
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %eax
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %eax
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %eax
	addl	%eax, %edx
	cmpq	%r9, %rsi
	je	.L1387
.L1392:
	leaq	1(%rbx), %rax
	addl	(%rcx,%rbx,4), %edx
	cmpq	%rax, %r12
	jle	.L1387
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L1387
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L1387
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L1387
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L1387
	addq	$6, %rbx
	addl	(%rcx,%rax,4), %edx
	cmpq	%rbx, %r12
	jle	.L1387
	addl	(%rcx,%rbx,4), %edx
.L1387:
	addl	%edi, %edx
	movl	%edx, (%r14)
	vzeroupper
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1434:
	movq	%r9, %rbx
	jmp	.L1390
.L1449:
	testq	%rax, %rax
	jne	.L1450
	xorl	%r15d, %r15d
	xorl	%edx, %edx
	xorl	%edi, %edi
	xorl	%eax, %eax
	jmp	.L1377
.L1399:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L1375
.L1409:
	movl	$8, %eax
	jmp	.L1378
.L1410:
	movl	$9, %eax
	jmp	.L1378
.L1408:
	movl	$7, %eax
	jmp	.L1378
.L1417:
	movl	$16, %eax
	jmp	.L1378
.L1426:
	movq	%r10, %rbx
	jmp	.L1390
.L1407:
	movl	$6, %eax
	jmp	.L1378
.L1402:
	movl	$1, %eax
	jmp	.L1378
.L1403:
	movl	$2, %eax
	jmp	.L1378
.L1416:
	movl	$15, %eax
	jmp	.L1378
.L1411:
	movl	$10, %eax
	jmp	.L1378
.L1412:
	movl	$11, %eax
	jmp	.L1378
.L1413:
	movl	$12, %eax
	jmp	.L1378
.L1414:
	movl	$13, %eax
	jmp	.L1378
.L1415:
	movl	$14, %eax
	jmp	.L1378
.L1404:
	movl	$3, %eax
	jmp	.L1378
.L1405:
	movl	$4, %eax
	jmp	.L1378
.L1406:
	movl	$5, %eax
	jmp	.L1378
.L1450:
	movq	%rax, %r15
	jmp	.L1376
unroll8x2_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	movq	%rsi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %r13
	call	get_vec_start
	movq	%rax, %r8
	leaq	-28(%rax,%r13,4), %rax
	cmpq	%rax, %r8
	jae	.L1467
	movq	%r8, %rdx
	xorl	%edi, %edi
	xorl	%ecx, %ecx
.L1454:
	addl	(%rdx), %ecx
	addl	4(%rdx), %edi
	addq	$32, %rdx
	addl	-24(%rdx), %ecx
	addl	-20(%rdx), %edi
	addl	-16(%rdx), %ecx
	addl	-12(%rdx), %edi
	addl	-8(%rdx), %ecx
	addl	-4(%rdx), %edi
	cmpq	%rdx, %rax
	ja	.L1454
	movq	%r8, %rdx
	notq	%rdx
	addq	%rax, %rdx
	andq	$-32, %rdx
	leaq	32(%r8,%rdx), %r8
.L1452:
	leaq	28(%rax), %r10
	cmpq	%r8, %r10
	jbe	.L1455
	addq	$27, %rax
	movq	%r8, %rdx
	subq	%r8, %rax
	andl	$31, %edx
	shrq	$2, %rax
	shrq	$2, %rdx
	leaq	1(%rax), %r9
	negq	%rdx
	andl	$7, %edx
	cmpq	%rdx, %r9
	movq	%rdx, %rax
	movq	%r9, %rdx
	cmovbe	%r9, %rax
	cmpq	$17, %r9
	ja	.L1476
.L1456:
	addl	(%r8), %ecx
	cmpq	$1, %rdx
	leaq	4(%r8), %rax
	jbe	.L1458
	addl	4(%r8), %ecx
	cmpq	$2, %rdx
	leaq	8(%r8), %rax
	jbe	.L1458
	addl	8(%r8), %ecx
	cmpq	$3, %rdx
	leaq	12(%r8), %rax
	jbe	.L1458
	addl	12(%r8), %ecx
	cmpq	$4, %rdx
	leaq	16(%r8), %rax
	jbe	.L1458
	addl	16(%r8), %ecx
	cmpq	$5, %rdx
	leaq	20(%r8), %rax
	jbe	.L1458
	addl	20(%r8), %ecx
	cmpq	$6, %rdx
	leaq	24(%r8), %rax
	jbe	.L1458
	addl	24(%r8), %ecx
	cmpq	$7, %rdx
	leaq	28(%r8), %rax
	jbe	.L1458
	addl	28(%r8), %ecx
	cmpq	$8, %rdx
	leaq	32(%r8), %rax
	jbe	.L1458
	addl	32(%r8), %ecx
	cmpq	$9, %rdx
	leaq	36(%r8), %rax
	jbe	.L1458
	movl	36(%r8), %esi
	leaq	40(%r8), %rax
	addl	%ecx, %esi
	cmpq	$10, %rdx
	movl	%esi, %ecx
	jbe	.L1458
	addl	40(%r8), %esi
	cmpq	$11, %rdx
	leaq	44(%r8), %rax
	movl	%esi, %ecx
	jbe	.L1458
	addl	44(%r8), %ecx
	cmpq	$12, %rdx
	leaq	48(%r8), %rax
	jbe	.L1458
	addl	48(%r8), %ecx
	cmpq	$13, %rdx
	leaq	52(%r8), %rax
	jbe	.L1458
	addl	52(%r8), %ecx
	cmpq	$14, %rdx
	leaq	56(%r8), %rax
	jbe	.L1458
	addl	56(%r8), %ecx
	cmpq	$15, %rdx
	leaq	60(%r8), %rax
	jbe	.L1458
	addl	60(%r8), %ecx
	cmpq	$16, %rdx
	leaq	64(%r8), %rax
	jbe	.L1458
	addl	64(%r8), %ecx
	leaq	68(%r8), %rax
.L1458:
	cmpq	%rdx, %r9
	je	.L1455
.L1457:
	subq	%rdx, %r9
	movq	%r9, %r12
	shrq	$3, %r12
	leaq	0(,%r12,8), %r11
	testq	%r11, %r11
	je	.L1460
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r8,%rdx,4), %rsi
	xorl	%edx, %edx
.L1466:
	addq	$1, %rdx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rdx, %r12
	ja	.L1466
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r11,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %edx
	addl	%esi, %edx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %edx
	addl	%edx, %ecx
	cmpq	%r11, %r9
	je	.L1474
	vzeroupper
.L1460:
	leaq	4(%rax), %rdx
	addl	(%rax), %ecx
	cmpq	%rdx, %r10
	jbe	.L1455
	leaq	8(%rax), %rdx
	addl	4(%rax), %ecx
	cmpq	%rdx, %r10
	jbe	.L1455
	leaq	12(%rax), %rdx
	addl	8(%rax), %ecx
	cmpq	%rdx, %r10
	jbe	.L1455
	leaq	16(%rax), %rdx
	addl	12(%rax), %ecx
	cmpq	%rdx, %r10
	jbe	.L1455
	leaq	20(%rax), %rdx
	addl	16(%rax), %ecx
	cmpq	%rdx, %r10
	jbe	.L1455
	leaq	24(%rax), %rdx
	addl	20(%rax), %ecx
	cmpq	%rdx, %r10
	jbe	.L1455
	addl	24(%rax), %ecx
.L1455:
	addl	%edi, %ecx
	movl	%ecx, (%rbx)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L1476:
	testq	%rax, %rax
	jne	.L1477
	xorl	%edx, %edx
	movq	%r8, %rax
	jmp	.L1457
.L1474:
	vzeroupper
	addl	%edi, %ecx
	movl	%ecx, (%rbx)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L1467:
	xorl	%edi, %edi
	xorl	%ecx, %ecx
	jmp	.L1452
.L1477:
	movq	%rax, %rdx
	jmp	.L1456
unroll9x3_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	movq	%rsi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %r13
	call	get_vec_start
	leaq	-32(%rax,%r13,4), %r8
	movq	%rax, %rdx
	xorl	%ecx, %ecx
	xorl	%edi, %edi
	xorl	%eax, %eax
	cmpq	%r8, %rdx
	jae	.L1479
.L1480:
	addl	(%rdx), %eax
	addl	4(%rdx), %edi
	addq	$36, %rdx
	addl	-28(%rdx), %ecx
	addl	-24(%rdx), %eax
	addl	-20(%rdx), %edi
	addl	-16(%rdx), %ecx
	addl	-12(%rdx), %eax
	addl	-8(%rdx), %edi
	addl	-4(%rdx), %ecx
	cmpq	%rdx, %r8
	ja	.L1480
.L1479:
	leaq	32(%r8), %r11
	cmpq	%rdx, %r11
	jbe	.L1481
	addq	$31, %r8
	movq	%rdx, %r9
	subq	%rdx, %r8
	andl	$31, %r9d
	shrq	$2, %r8
	shrq	$2, %r9
	leaq	1(%r8), %r10
	negq	%r9
	andl	$7, %r9d
	cmpq	%r9, %r10
	movq	%r9, %rsi
	movq	%r10, %r9
	cmovbe	%r10, %rsi
	cmpq	$17, %r10
	ja	.L1502
.L1482:
	addl	(%rdx), %eax
	cmpq	$1, %r9
	leaq	4(%rdx), %r8
	jbe	.L1484
	addl	4(%rdx), %eax
	cmpq	$2, %r9
	leaq	8(%rdx), %r8
	jbe	.L1484
	addl	8(%rdx), %eax
	cmpq	$3, %r9
	leaq	12(%rdx), %r8
	jbe	.L1484
	addl	12(%rdx), %eax
	cmpq	$4, %r9
	leaq	16(%rdx), %r8
	jbe	.L1484
	addl	16(%rdx), %eax
	cmpq	$5, %r9
	leaq	20(%rdx), %r8
	jbe	.L1484
	addl	20(%rdx), %eax
	cmpq	$6, %r9
	leaq	24(%rdx), %r8
	jbe	.L1484
	addl	24(%rdx), %eax
	cmpq	$7, %r9
	leaq	28(%rdx), %r8
	jbe	.L1484
	addl	28(%rdx), %eax
	cmpq	$8, %r9
	leaq	32(%rdx), %r8
	jbe	.L1484
	addl	32(%rdx), %eax
	cmpq	$9, %r9
	leaq	36(%rdx), %r8
	jbe	.L1484
	addl	36(%rdx), %eax
	cmpq	$10, %r9
	leaq	40(%rdx), %r8
	jbe	.L1484
	addl	40(%rdx), %eax
	cmpq	$11, %r9
	leaq	44(%rdx), %r8
	jbe	.L1484
	addl	44(%rdx), %eax
	cmpq	$12, %r9
	leaq	48(%rdx), %r8
	jbe	.L1484
	addl	48(%rdx), %eax
	cmpq	$13, %r9
	leaq	52(%rdx), %r8
	jbe	.L1484
	addl	52(%rdx), %eax
	cmpq	$14, %r9
	leaq	56(%rdx), %r8
	jbe	.L1484
	addl	56(%rdx), %eax
	cmpq	$15, %r9
	leaq	60(%rdx), %r8
	jbe	.L1484
	addl	60(%rdx), %eax
	cmpq	$16, %r9
	leaq	64(%rdx), %r8
	jbe	.L1484
	addl	64(%rdx), %eax
	leaq	68(%rdx), %r8
.L1484:
	cmpq	%r10, %r9
	je	.L1481
.L1483:
	subq	%r9, %r10
	movq	%r10, %r13
	shrq	$3, %r13
	leaq	0(,%r13,8), %r12
	testq	%r12, %r12
	je	.L1486
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rdx,%r9,4), %rsi
	xorl	%edx, %edx
.L1492:
	addq	$1, %rdx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rdx, %r13
	ja	.L1492
	vmovdqa	%xmm0, %xmm1
	leaq	(%r8,%r12,4), %r8
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %edx
	addl	%esi, %edx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %edx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %edx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %edx
	addl	%edx, %eax
	cmpq	%r12, %r10
	je	.L1500
	vzeroupper
.L1486:
	leaq	4(%r8), %rdx
	addl	(%r8), %eax
	cmpq	%rdx, %r11
	jbe	.L1481
	leaq	8(%r8), %rdx
	addl	4(%r8), %eax
	cmpq	%rdx, %r11
	jbe	.L1481
	leaq	12(%r8), %rdx
	addl	8(%r8), %eax
	cmpq	%rdx, %r11
	jbe	.L1481
	leaq	16(%r8), %rdx
	addl	12(%r8), %eax
	cmpq	%rdx, %r11
	jbe	.L1481
	leaq	20(%r8), %rdx
	addl	16(%r8), %eax
	cmpq	%rdx, %r11
	jbe	.L1481
	leaq	24(%r8), %rdx
	addl	20(%r8), %eax
	cmpq	%rdx, %r11
	jbe	.L1481
	addl	24(%r8), %eax
.L1481:
	addl	%edi, %eax
	addl	%eax, %ecx
	movl	%ecx, (%rbx)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L1502:
	testq	%rsi, %rsi
	jne	.L1503
	xorl	%r9d, %r9d
	movq	%rdx, %r8
	jmp	.L1483
.L1500:
	vzeroupper
	jmp	.L1481
.L1503:
	movq	%rsi, %r9
	jmp	.L1482
unroll8x4_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	movq	%rsi, %rbx
	andq	$-32, %rsp
	call	vec_length
	movq	%r12, %rdi
	movq	%rax, %r13
	call	get_vec_start
	movq	%rax, %r10
	leaq	-28(%rax,%r13,4), %rax
	cmpq	%rax, %r10
	jae	.L1520
	movq	%r10, %rdx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%ecx, %ecx
.L1507:
	addl	(%rdx), %ecx
	addl	4(%rdx), %r9d
	addq	$32, %rdx
	addl	-24(%rdx), %r8d
	addl	-20(%rdx), %edi
	addl	-16(%rdx), %ecx
	addl	-12(%rdx), %r9d
	addl	-8(%rdx), %r8d
	addl	-4(%rdx), %edi
	cmpq	%rdx, %rax
	ja	.L1507
	movq	%r10, %rdx
	notq	%rdx
	addq	%rax, %rdx
	andq	$-32, %rdx
	leaq	32(%r10,%rdx), %r10
.L1505:
	leaq	28(%rax), %rsi
	cmpq	%r10, %rsi
	jbe	.L1508
	addq	$27, %rax
	movq	%r10, %rdx
	subq	%r10, %rax
	andl	$31, %edx
	shrq	$2, %rax
	shrq	$2, %rdx
	leaq	1(%rax), %r11
	negq	%rdx
	andl	$7, %edx
	cmpq	%rdx, %r11
	movq	%rdx, %rax
	movq	%r11, %rdx
	cmovbe	%r11, %rax
	cmpq	$17, %r11
	ja	.L1529
.L1509:
	addl	(%r10), %ecx
	cmpq	$1, %rdx
	leaq	4(%r10), %rax
	jbe	.L1511
	addl	4(%r10), %ecx
	cmpq	$2, %rdx
	leaq	8(%r10), %rax
	jbe	.L1511
	addl	8(%r10), %ecx
	cmpq	$3, %rdx
	leaq	12(%r10), %rax
	jbe	.L1511
	addl	12(%r10), %ecx
	cmpq	$4, %rdx
	leaq	16(%r10), %rax
	jbe	.L1511
	addl	16(%r10), %ecx
	cmpq	$5, %rdx
	leaq	20(%r10), %rax
	jbe	.L1511
	addl	20(%r10), %ecx
	cmpq	$6, %rdx
	leaq	24(%r10), %rax
	jbe	.L1511
	addl	24(%r10), %ecx
	cmpq	$7, %rdx
	leaq	28(%r10), %rax
	jbe	.L1511
	addl	28(%r10), %ecx
	cmpq	$8, %rdx
	leaq	32(%r10), %rax
	jbe	.L1511
	addl	32(%r10), %ecx
	cmpq	$9, %rdx
	leaq	36(%r10), %rax
	jbe	.L1511
	addl	36(%r10), %ecx
	cmpq	$10, %rdx
	leaq	40(%r10), %rax
	jbe	.L1511
	addl	40(%r10), %ecx
	cmpq	$11, %rdx
	leaq	44(%r10), %rax
	jbe	.L1511
	addl	44(%r10), %ecx
	cmpq	$12, %rdx
	leaq	48(%r10), %rax
	jbe	.L1511
	addl	48(%r10), %ecx
	cmpq	$13, %rdx
	leaq	52(%r10), %rax
	jbe	.L1511
	addl	52(%r10), %ecx
	cmpq	$14, %rdx
	leaq	56(%r10), %rax
	jbe	.L1511
	addl	56(%r10), %ecx
	cmpq	$15, %rdx
	leaq	60(%r10), %rax
	jbe	.L1511
	addl	60(%r10), %ecx
	cmpq	$16, %rdx
	leaq	64(%r10), %rax
	jbe	.L1511
	addl	64(%r10), %ecx
	leaq	68(%r10), %rax
.L1511:
	cmpq	%r11, %rdx
	je	.L1508
.L1510:
	subq	%rdx, %r11
	movq	%r11, %r13
	shrq	$3, %r13
	leaq	0(,%r13,8), %r12
	testq	%r12, %r12
	je	.L1513
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r10,%rdx,4), %r10
	xorl	%edx, %edx
.L1519:
	addq	$1, %rdx
	vpaddd	(%r10), %ymm0, %ymm0
	addq	$32, %r10
	cmpq	%rdx, %r13
	ja	.L1519
	vmovdqa	%xmm0, %xmm1
	leaq	(%rax,%r12,4), %rax
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r10d
	vpextrd	$0, %xmm1, %edx
	addl	%r10d, %edx
	vpextrd	$2, %xmm1, %r10d
	addl	%r10d, %edx
	vpextrd	$3, %xmm1, %r10d
	addl	%r10d, %edx
	vpextrd	$0, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$1, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$2, %xmm0, %r10d
	addl	%r10d, %edx
	vpextrd	$3, %xmm0, %r10d
	addl	%r10d, %edx
	addl	%edx, %ecx
	cmpq	%r12, %r11
	je	.L1527
	vzeroupper
.L1513:
	leaq	4(%rax), %rdx
	addl	(%rax), %ecx
	cmpq	%rdx, %rsi
	jbe	.L1508
	leaq	8(%rax), %rdx
	addl	4(%rax), %ecx
	cmpq	%rdx, %rsi
	jbe	.L1508
	leaq	12(%rax), %rdx
	addl	8(%rax), %ecx
	cmpq	%rdx, %rsi
	jbe	.L1508
	leaq	16(%rax), %rdx
	addl	12(%rax), %ecx
	cmpq	%rdx, %rsi
	jbe	.L1508
	leaq	20(%rax), %rdx
	addl	16(%rax), %ecx
	cmpq	%rdx, %rsi
	jbe	.L1508
	leaq	24(%rax), %rdx
	addl	20(%rax), %ecx
	cmpq	%rdx, %rsi
	jbe	.L1508
	addl	24(%rax), %ecx
.L1508:
	addl	%r9d, %ecx
	addl	%ecx, %r8d
	addl	%r8d, %edi
	movl	%edi, (%rbx)
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L1529:
	testq	%rax, %rax
	jne	.L1530
	xorl	%edx, %edx
	movq	%r10, %rax
	jmp	.L1510
.L1527:
	vzeroupper
	jmp	.L1508
.L1520:
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	xorl	%ecx, %ecx
	jmp	.L1505
.L1530:
	movq	%rax, %rdx
	jmp	.L1509
unroll8x8_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdi, %rbx
	andq	$-32, %rsp
	subq	$16, %rsp
	call	vec_length
	movq	%rbx, %rdi
	movq	%rax, %r12
	call	get_vec_start
	leaq	-28(%rax,%r12,4), %r11
	movq	%rax, %rcx
	cmpq	%r11, %rax
	jae	.L1547
	movq	%rax, %rsi
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%edx, %edx
	xorl	%r8d, %r8d
	xorl	%eax, %eax
	xorl	%edi, %edi
.L1534:
	addl	(%rsi), %edx
	addl	4(%rsi), %r10d
	addq	$32, %rsi
	addl	-24(%rsi), %r9d
	addl	-20(%rsi), %r8d
	addl	-16(%rsi), %edi
	movl	-12(%rsi), %r15d
	movl	-8(%rsi), %ebx
	addl	-4(%rsi), %eax
	cmpq	%rsi, %r11
	ja	.L1534
	movq	%rcx, %rsi
	addl	%ebx, %r15d
	notq	%rsi
	addq	%r11, %rsi
	andq	$-32, %rsi
	leaq	32(%rcx,%rsi), %rcx
.L1532:
	leaq	28(%r11), %r12
	cmpq	%rcx, %r12
	jbe	.L1535
	addq	$27, %r11
	movq	%rcx, %rsi
	subq	%rcx, %r11
	andl	$31, %esi
	shrq	$2, %r11
	shrq	$2, %rsi
	leaq	1(%r11), %r13
	negq	%rsi
	andl	$7, %esi
	cmpq	%rsi, %r13
	movq	%r13, %rbx
	cmovbe	%r13, %rsi
	cmpq	$17, %r13
	ja	.L1556
.L1536:
	addl	(%rcx), %edx
	cmpq	$1, %rbx
	leaq	4(%rcx), %r11
	jbe	.L1538
	addl	4(%rcx), %edx
	cmpq	$2, %rbx
	leaq	8(%rcx), %r11
	jbe	.L1538
	addl	8(%rcx), %edx
	cmpq	$3, %rbx
	leaq	12(%rcx), %r11
	jbe	.L1538
	addl	12(%rcx), %edx
	cmpq	$4, %rbx
	leaq	16(%rcx), %r11
	jbe	.L1538
	addl	16(%rcx), %edx
	cmpq	$5, %rbx
	leaq	20(%rcx), %r11
	jbe	.L1538
	addl	20(%rcx), %edx
	cmpq	$6, %rbx
	leaq	24(%rcx), %r11
	jbe	.L1538
	addl	24(%rcx), %edx
	cmpq	$7, %rbx
	leaq	28(%rcx), %r11
	jbe	.L1538
	addl	28(%rcx), %edx
	cmpq	$8, %rbx
	leaq	32(%rcx), %r11
	jbe	.L1538
	addl	32(%rcx), %edx
	cmpq	$9, %rbx
	leaq	36(%rcx), %r11
	jbe	.L1538
	addl	36(%rcx), %edx
	cmpq	$10, %rbx
	leaq	40(%rcx), %r11
	jbe	.L1538
	addl	40(%rcx), %edx
	cmpq	$11, %rbx
	leaq	44(%rcx), %r11
	jbe	.L1538
	addl	44(%rcx), %edx
	cmpq	$12, %rbx
	leaq	48(%rcx), %r11
	jbe	.L1538
	addl	48(%rcx), %edx
	cmpq	$13, %rbx
	leaq	52(%rcx), %r11
	jbe	.L1538
	addl	52(%rcx), %edx
	cmpq	$14, %rbx
	leaq	56(%rcx), %r11
	jbe	.L1538
	addl	56(%rcx), %edx
	cmpq	$15, %rbx
	leaq	60(%rcx), %r11
	jbe	.L1538
	addl	60(%rcx), %edx
	cmpq	$16, %rbx
	leaq	64(%rcx), %r11
	jbe	.L1538
	addl	64(%rcx), %edx
	leaq	68(%rcx), %r11
.L1538:
	cmpq	%r13, %rbx
	je	.L1535
.L1537:
	subq	%rbx, %r13
	movq	%r13, %rsi
	movq	%r13, 8(%rsp)
	shrq	$3, %rsi
	leaq	0(,%rsi,8), %r13
	testq	%r13, %r13
	je	.L1540
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rbx,4), %rbx
	xorl	%ecx, %ecx
.L1546:
	addq	$1, %rcx
	vpaddd	(%rbx), %ymm0, %ymm0
	addq	$32, %rbx
	cmpq	%rcx, %rsi
	ja	.L1546
	vmovdqa	%xmm0, %xmm1
	leaq	(%r11,%r13,4), %r11
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edx
	cmpq	%r13, 8(%rsp)
	je	.L1554
	vzeroupper
.L1540:
	leaq	4(%r11), %rcx
	addl	(%r11), %edx
	cmpq	%rcx, %r12
	jbe	.L1535
	leaq	8(%r11), %rcx
	addl	4(%r11), %edx
	cmpq	%rcx, %r12
	jbe	.L1535
	leaq	12(%r11), %rcx
	addl	8(%r11), %edx
	cmpq	%rcx, %r12
	jbe	.L1535
	leaq	16(%r11), %rcx
	addl	12(%r11), %edx
	cmpq	%rcx, %r12
	jbe	.L1535
	leaq	20(%r11), %rcx
	addl	16(%r11), %edx
	cmpq	%rcx, %r12
	jbe	.L1535
	leaq	24(%r11), %rcx
	addl	20(%r11), %edx
	cmpq	%rcx, %r12
	jbe	.L1535
	addl	24(%r11), %edx
.L1535:
	addl	%r10d, %edx
	addl	%edx, %r9d
	addl	%r9d, %r8d
	addl	%r8d, %edi
	addl	%edi, %r15d
	addl	%r15d, %eax
	movl	%eax, (%r14)
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L1556:
	testq	%rsi, %rsi
	jne	.L1557
	xorl	%ebx, %ebx
	movq	%rcx, %r11
	jmp	.L1537
.L1554:
	vzeroupper
	jmp	.L1535
.L1547:
	xorl	%r9d, %r9d
	xorl	%r10d, %r10d
	xorl	%edx, %edx
	xorl	%r8d, %r8d
	xorl	%eax, %eax
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	jmp	.L1532
.L1557:
	movq	%rsi, %rbx
	jmp	.L1536
combine7:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-1(%rax), %r12
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r12, %r12
	movq	%rax, %rcx
	jle	.L1579
	leaq	-2(%rbx), %rdi
	shrq	%rdi
	addq	$1, %rdi
	movq	%rdi, %r8
	shrq	$3, %r8
	leaq	0(,%r8,8), %rsi
	testq	%rsi, %rsi
	je	.L1580
	cmpq	$9, %rdi
	jbe	.L1580
	vpxor	%xmm2, %xmm2, %xmm2
	xorl	%edx, %edx
.L1566:
	vmovdqu	(%rax), %xmm0
	addq	$1, %rdx
	addq	$64, %rax
	vmovdqu	-32(%rax), %xmm1
	vinserti128	$0x1, -48(%rax), %ymm0, %ymm0
	vinserti128	$0x1, -16(%rax), %ymm1, %ymm1
	cmpq	%r8, %rdx
	vperm2i128	$32, %ymm1, %ymm0, %ymm4
	vperm2i128	$49, %ymm1, %ymm0, %ymm3
	vpshufd	$216, %ymm4, %ymm4
	vpshufd	$216, %ymm3, %ymm3
	vpunpcklqdq	%ymm3, %ymm4, %ymm4
	vperm2i128	$32, %ymm1, %ymm0, %ymm3
	vperm2i128	$49, %ymm1, %ymm0, %ymm0
	vpshufd	$216, %ymm3, %ymm3
	vpshufd	$216, %ymm0, %ymm0
	vpunpckhqdq	%ymm0, %ymm3, %ymm0
	vpaddd	%ymm0, %ymm4, %ymm3
	vpaddd	%ymm3, %ymm2, %ymm2
	jb	.L1566
	vmovdqa	%xmm2, %xmm0
	vextracti128	$0x1, %ymm2, %xmm2
	vpextrd	$1, %xmm0, %edx
	vpextrd	$0, %xmm0, %eax
	addl	%edx, %eax
	vpextrd	$2, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm0, %edx
	addl	%edx, %eax
	vpextrd	$0, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$1, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$2, %xmm2, %edx
	addl	%edx, %eax
	vpextrd	$3, %xmm2, %edx
	addl	%eax, %edx
	cmpq	%rsi, %rdi
	leaq	(%rsi,%rsi), %rax
	je	.L1564
.L1560:
	leaq	2(%rax), %rsi
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rax,4), %edx
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	4(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	6(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	8(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	10(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	12(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addl	4(%rcx,%rsi,4), %edx
	leaq	14(%rax), %rsi
	cmpq	%rsi, %r12
	jle	.L1564
	addl	(%rcx,%rsi,4), %edx
	addq	$16, %rax
	addl	4(%rcx,%rsi,4), %edx
	cmpq	%rax, %r12
	jle	.L1564
	addl	(%rcx,%rax,4), %edx
	addl	4(%rcx,%rax,4), %edx
.L1564:
	addq	%rdi, %rdi
.L1559:
	cmpq	%rdi, %rbx
	jle	.L1567
	leaq	(%rcx,%rdi,4), %rsi
	movq	%rbx, %rax
	subq	%rdi, %rax
	movq	%rsi, %r8
	andl	$31, %r8d
	shrq	$2, %r8
	negq	%r8
	andl	$7, %r8d
	cmpq	%rax, %r8
	cmova	%rax, %r8
	cmpq	$17, %rax
	cmovbe	%rax, %r8
	testq	%r8, %r8
	je	.L1569
	addl	(%rsi), %edx
	cmpq	$1, %r8
	leaq	1(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %r8
	leaq	2(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %r8
	leaq	3(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %r8
	leaq	4(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %r8
	leaq	5(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %r8
	leaq	6(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %r8
	leaq	7(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %r8
	leaq	8(%rdi), %r10
	jbe	.L1589
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %r8
	leaq	9(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %r8
	leaq	10(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %r8
	leaq	11(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %r8
	leaq	12(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %r8
	leaq	13(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %r8
	leaq	14(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %r8
	leaq	15(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %r8
	leaq	16(%rdi), %r9
	jbe	.L1597
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rdi
.L1570:
	cmpq	%rax, %r8
	je	.L1567
.L1569:
	subq	%r8, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L1572
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%r8,4), %r8
	xorl	%esi, %esi
.L1578:
	addq	$1, %rsi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rsi, %r10
	ja	.L1578
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rdi
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %esi
	addl	%esi, %edx
	cmpq	%r9, %rax
	je	.L1567
.L1572:
	leaq	1(%rdi), %rax
	addl	(%rcx,%rdi,4), %edx
	cmpq	%rax, %rbx
	jle	.L1567
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L1567
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L1567
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L1567
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rdi), %rax
	cmpq	%rax, %rbx
	jle	.L1567
	addq	$6, %rdi
	addl	(%rcx,%rax,4), %edx
	cmpq	%rdi, %rbx
	jle	.L1567
	addl	(%rcx,%rdi,4), %edx
.L1567:
	movl	%edx, 0(%r13)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1597:
	movq	%r9, %rdi
	jmp	.L1570
.L1580:
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.L1560
.L1579:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L1559
.L1589:
	movq	%r10, %rdi
	jmp	.L1570
unroll3aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-2(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	xorl	%edi, %edi
	xorl	%edx, %edx
	testq	%rbx, %rbx
	movq	%rax, %r8
	jle	.L1608
.L1609:
	movl	(%r8,%rdx,4), %ecx
	addl	4(%r8,%rdx,4), %ecx
	addl	8(%r8,%rdx,4), %ecx
	addq	$3, %rdx
	addl	%ecx, %edi
	cmpq	%rdx, %rbx
	jg	.L1609
.L1608:
	cmpq	%rdx, %r12
	jle	.L1610
	leaq	(%r8,%rdx,4), %r9
	movq	%r12, %rax
	subq	%rdx, %rax
	movq	%r9, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L1612
	addl	(%r9), %edi
	cmpq	$1, %rcx
	leaq	1(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$2, %rcx
	leaq	2(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$3, %rcx
	leaq	3(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$4, %rcx
	leaq	4(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$5, %rcx
	leaq	5(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$6, %rcx
	leaq	6(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$7, %rcx
	leaq	7(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$8, %rcx
	leaq	8(%rdx), %r10
	movl	%edi, %esi
	jbe	.L1631
	addl	(%r8,%r10,4), %esi
	cmpq	$9, %rcx
	movl	%esi, %edi
	leaq	9(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$10, %rcx
	leaq	10(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$11, %rcx
	leaq	11(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$12, %rcx
	leaq	12(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$13, %rcx
	leaq	13(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$14, %rcx
	leaq	14(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$15, %rcx
	leaq	15(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	cmpq	$16, %rcx
	leaq	16(%rdx), %rsi
	jbe	.L1639
	addl	(%r8,%rsi,4), %edi
	addq	$17, %rdx
.L1613:
	cmpq	%rax, %rcx
	je	.L1610
.L1612:
	subq	%rcx, %rax
	movq	%rax, %r11
	shrq	$3, %r11
	leaq	0(,%r11,8), %r10
	testq	%r10, %r10
	je	.L1615
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r9,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L1621:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%rcx, %r11
	ja	.L1621
	vmovdqa	%xmm0, %xmm1
	addq	%r10, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %edi
	cmpq	%rax, %r10
	je	.L1647
	vzeroupper
.L1615:
	leaq	1(%rdx), %rax
	addl	(%r8,%rdx,4), %edi
	cmpq	%rax, %r12
	jle	.L1610
	addl	(%r8,%rax,4), %edi
	leaq	2(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L1610
	addl	(%r8,%rax,4), %edi
	leaq	3(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L1610
	addl	(%r8,%rax,4), %edi
	leaq	4(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L1610
	addl	(%r8,%rax,4), %edi
	leaq	5(%rdx), %rax
	cmpq	%rax, %r12
	jle	.L1610
	addq	$6, %rdx
	addl	(%r8,%rax,4), %edi
	cmpq	%rdx, %r12
	jle	.L1610
	addl	(%r8,%rdx,4), %edi
.L1610:
	movl	%edi, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1639:
	movq	%rsi, %rdx
	jmp	.L1613
.L1647:
	vzeroupper
	movl	%edi, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1631:
	movq	%r10, %rdx
	jmp	.L1613
unroll4aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-3(%rax), %r12
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r12, %r12
	movq	%rax, %r9
	jle	.L1669
	leaq	-4(%rbx), %rsi
	shrq	$2, %rsi
	addq	$1, %rsi
	movq	%rsi, %rcx
	shrq	$3, %rcx
	leaq	0(,%rcx,8), %rdx
	testq	%rdx, %rdx
	je	.L1670
	cmpq	$7, %rsi
	jbe	.L1670
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%edi, %edi
.L1656:
	vmovdqu	(%rax), %xmm2
	addq	$1, %rdi
	subq	$-128, %rax
	vmovdqu	-96(%rax), %xmm4
	vinserti128	$0x1, -112(%rax), %ymm2, %ymm2
	vmovdqu	-64(%rax), %xmm5
	vinserti128	$0x1, -80(%rax), %ymm4, %ymm4
	vmovdqu	-32(%rax), %xmm6
	vinserti128	$0x1, -48(%rax), %ymm5, %ymm5
	vinserti128	$0x1, -16(%rax), %ymm6, %ymm6
	cmpq	%rdi, %rcx
	vperm2i128	$32, %ymm4, %ymm2, %ymm3
	vperm2i128	$49, %ymm4, %ymm2, %ymm1
	vpshufd	$216, %ymm3, %ymm3
	vpshufd	$216, %ymm1, %ymm1
	vpunpcklqdq	%ymm1, %ymm3, %ymm3
	vperm2i128	$32, %ymm4, %ymm2, %ymm1
	vperm2i128	$49, %ymm4, %ymm2, %ymm2
	vperm2i128	$32, %ymm6, %ymm5, %ymm4
	vpshufd	$216, %ymm1, %ymm1
	vpshufd	$216, %ymm2, %ymm2
	vpshufd	$216, %ymm4, %ymm4
	vpunpckhqdq	%ymm2, %ymm1, %ymm1
	vperm2i128	$49, %ymm6, %ymm5, %ymm2
	vpshufd	$216, %ymm2, %ymm2
	vpunpcklqdq	%ymm2, %ymm4, %ymm4
	vperm2i128	$32, %ymm6, %ymm5, %ymm2
	vperm2i128	$49, %ymm6, %ymm5, %ymm5
	vperm2i128	$32, %ymm4, %ymm3, %ymm6
	vpshufd	$216, %ymm2, %ymm2
	vpshufd	$216, %ymm5, %ymm5
	vpshufd	$216, %ymm6, %ymm6
	vpunpckhqdq	%ymm5, %ymm2, %ymm2
	vperm2i128	$49, %ymm4, %ymm3, %ymm5
	vpshufd	$216, %ymm5, %ymm5
	vpunpcklqdq	%ymm5, %ymm6, %ymm7
	vperm2i128	$32, %ymm2, %ymm1, %ymm6
	vperm2i128	$49, %ymm2, %ymm1, %ymm5
	vpshufd	$216, %ymm6, %ymm6
	vpshufd	$216, %ymm5, %ymm5
	vpunpcklqdq	%ymm5, %ymm6, %ymm5
	vpaddd	%ymm5, %ymm7, %ymm6
	vperm2i128	$32, %ymm4, %ymm3, %ymm5
	vperm2i128	$49, %ymm4, %ymm3, %ymm3
	vpshufd	$216, %ymm5, %ymm5
	vpshufd	$216, %ymm3, %ymm3
	vpunpckhqdq	%ymm3, %ymm5, %ymm3
	vperm2i128	$32, %ymm2, %ymm1, %ymm5
	vperm2i128	$49, %ymm2, %ymm1, %ymm1
	vpshufd	$216, %ymm5, %ymm5
	vpshufd	$216, %ymm1, %ymm1
	vpunpckhqdq	%ymm1, %ymm5, %ymm1
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm1, %ymm6, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L1656
	vmovdqa	%xmm0, %xmm1
	leaq	0(,%rdx,4), %r8
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %eax
	addl	%ecx, %eax
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %eax
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %eax
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %eax
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %eax
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %eax
	vpextrd	$3, %xmm0, %ecx
	addl	%eax, %ecx
	cmpq	%rdx, %rsi
	je	.L1654
.L1651:
	leaq	(%r9,%r8,4), %rdi
.L1655:
	movl	(%rdi), %edx
	addl	4(%rdi), %edx
	addq	$4, %r8
	addl	8(%rdi), %edx
	addq	$16, %rdi
	addl	-4(%rdi), %edx
	addl	%edx, %ecx
	cmpq	%r8, %r12
	jg	.L1655
.L1654:
	leaq	0(,%rsi,4), %rdx
.L1650:
	cmpq	%rdx, %rbx
	jle	.L1657
	leaq	(%r9,%rdx,4), %rsi
	movq	%rbx, %rax
	subq	%rdx, %rax
	movq	%rsi, %rdi
	andl	$31, %edi
	shrq	$2, %rdi
	negq	%rdi
	andl	$7, %edi
	cmpq	%rax, %rdi
	cmova	%rax, %rdi
	cmpq	$17, %rax
	cmovbe	%rax, %rdi
	testq	%rdi, %rdi
	je	.L1659
	addl	(%rsi), %ecx
	cmpq	$1, %rdi
	leaq	1(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$2, %rdi
	leaq	2(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$3, %rdi
	leaq	3(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$4, %rdi
	leaq	4(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$5, %rdi
	leaq	5(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$6, %rdi
	leaq	6(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$7, %rdi
	leaq	7(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$8, %rdi
	leaq	8(%rdx), %r10
	jbe	.L1679
	addl	(%r9,%r10,4), %ecx
	cmpq	$9, %rdi
	leaq	9(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$10, %rdi
	leaq	10(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$11, %rdi
	leaq	11(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$12, %rdi
	leaq	12(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$13, %rdi
	leaq	13(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$14, %rdi
	leaq	14(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$15, %rdi
	leaq	15(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	cmpq	$16, %rdi
	leaq	16(%rdx), %r8
	jbe	.L1687
	addl	(%r9,%r8,4), %ecx
	addq	$17, %rdx
.L1660:
	cmpq	%rax, %rdi
	je	.L1657
.L1659:
	subq	%rdi, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r8
	testq	%r8, %r8
	je	.L1662
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%rdi,4), %rdi
	xorl	%esi, %esi
.L1668:
	addq	$1, %rsi
	vpaddd	(%rdi), %ymm0, %ymm0
	addq	$32, %rdi
	cmpq	%r10, %rsi
	jb	.L1668
	vmovdqa	%xmm0, %xmm1
	addq	%r8, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %edi
	vpextrd	$0, %xmm1, %esi
	addl	%edi, %esi
	vpextrd	$2, %xmm1, %edi
	addl	%edi, %esi
	vpextrd	$3, %xmm1, %edi
	addl	%edi, %esi
	vpextrd	$0, %xmm0, %edi
	addl	%edi, %esi
	vpextrd	$1, %xmm0, %edi
	addl	%edi, %esi
	vpextrd	$2, %xmm0, %edi
	addl	%edi, %esi
	vpextrd	$3, %xmm0, %edi
	addl	%edi, %esi
	addl	%esi, %ecx
	cmpq	%rax, %r8
	je	.L1657
.L1662:
	leaq	1(%rdx), %rax
	addl	(%r9,%rdx,4), %ecx
	cmpq	%rax, %rbx
	jle	.L1657
	addl	(%r9,%rax,4), %ecx
	leaq	2(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1657
	addl	(%r9,%rax,4), %ecx
	leaq	3(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1657
	addl	(%r9,%rax,4), %ecx
	leaq	4(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1657
	addl	(%r9,%rax,4), %ecx
	leaq	5(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1657
	addq	$6, %rdx
	addl	(%r9,%rax,4), %ecx
	cmpq	%rdx, %rbx
	jle	.L1657
	addl	(%r9,%rdx,4), %ecx
.L1657:
	movl	%ecx, 0(%r13)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1687:
	movq	%r8, %rdx
	jmp	.L1660
.L1670:
	xorl	%ecx, %ecx
	xorl	%r8d, %r8d
	jmp	.L1651
.L1669:
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	jmp	.L1650
.L1679:
	movq	%r10, %rdx
	jmp	.L1660
unroll5aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-4(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r9
	jle	.L1712
	movq	%rax, %rcx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
.L1699:
	movl	(%rcx), %edx
	addl	4(%rcx), %edx
	addq	$5, %r8
	addl	8(%rcx), %edx
	addq	$20, %rcx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	addl	%edx, %edi
	cmpq	%r8, %rbx
	jg	.L1699
.L1698:
	cmpq	%r8, %r12
	jle	.L1700
	leaq	(%r9,%r8,4), %rcx
	movq	%r12, %rax
	subq	%r8, %rax
	movq	%rcx, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1702
	addl	(%rcx), %edi
	cmpq	$1, %rdx
	leaq	1(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$2, %rdx
	leaq	2(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$3, %rdx
	leaq	3(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$4, %rdx
	leaq	4(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$5, %rdx
	leaq	5(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$6, %rdx
	leaq	6(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$7, %rdx
	leaq	7(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$8, %rdx
	leaq	8(%r8), %r10
	movl	%edi, %esi
	jbe	.L1721
	addl	(%r9,%r10,4), %esi
	cmpq	$9, %rdx
	movl	%esi, %edi
	leaq	9(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$10, %rdx
	leaq	10(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$11, %rdx
	leaq	11(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$12, %rdx
	leaq	12(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$13, %rdx
	leaq	13(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$14, %rdx
	leaq	14(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$15, %rdx
	leaq	15(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	cmpq	$16, %rdx
	leaq	16(%r8), %rsi
	jbe	.L1729
	addl	(%r9,%rsi,4), %edi
	addq	$17, %r8
.L1703:
	cmpq	%rax, %rdx
	je	.L1700
.L1702:
	subq	%rdx, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %rsi
	testq	%rsi, %rsi
	je	.L1705
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdx,4), %rcx
	xorl	%edx, %edx
.L1711:
	addq	$1, %rdx
	vpaddd	(%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%r10, %rdx
	jb	.L1711
	vmovdqa	%xmm0, %xmm1
	addq	%rsi, %r8
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %edx
	addl	%ecx, %edx
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm0, %ecx
	addl	%ecx, %edx
	addl	%edx, %edi
	cmpq	%rax, %rsi
	je	.L1737
	vzeroupper
.L1705:
	leaq	1(%r8), %rax
	addl	(%r9,%r8,4), %edi
	cmpq	%rax, %r12
	jle	.L1700
	addl	(%r9,%rax,4), %edi
	leaq	2(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1700
	addl	(%r9,%rax,4), %edi
	leaq	3(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1700
	addl	(%r9,%rax,4), %edi
	leaq	4(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1700
	addl	(%r9,%rax,4), %edi
	leaq	5(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1700
	addq	$6, %r8
	addl	(%r9,%rax,4), %edi
	cmpq	%r8, %r12
	jle	.L1700
	addl	(%r9,%r8,4), %edi
.L1700:
	movl	%edi, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1729:
	movq	%rsi, %r8
	jmp	.L1703
.L1737:
	vzeroupper
	movl	%edi, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1712:
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	jmp	.L1698
.L1721:
	movq	%r10, %r8
	jmp	.L1703
unroll6aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-5(%rax), %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r9
	jle	.L1754
	movq	%rax, %rcx
	xorl	%edi, %edi
	xorl	%r8d, %r8d
.L1741:
	movl	(%rcx), %edx
	addl	4(%rcx), %edx
	addq	$6, %r8
	addl	8(%rcx), %edx
	addq	$24, %rcx
	addl	-12(%rcx), %edx
	addl	-8(%rcx), %edx
	addl	-4(%rcx), %edx
	addl	%edx, %edi
	cmpq	%r8, %rbx
	jg	.L1741
.L1740:
	cmpq	%r8, %r12
	jle	.L1742
	leaq	(%r9,%r8,4), %rcx
	movq	%r12, %rax
	subq	%r8, %rax
	movq	%rcx, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1744
	addl	(%rcx), %edi
	cmpq	$1, %rdx
	leaq	1(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$2, %rdx
	leaq	2(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$3, %rdx
	leaq	3(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$4, %rdx
	leaq	4(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$5, %rdx
	leaq	5(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$6, %rdx
	leaq	6(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$7, %rdx
	leaq	7(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$8, %rdx
	leaq	8(%r8), %r10
	movl	%edi, %esi
	jbe	.L1763
	addl	(%r9,%r10,4), %esi
	cmpq	$9, %rdx
	movl	%esi, %edi
	leaq	9(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$10, %rdx
	leaq	10(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$11, %rdx
	leaq	11(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$12, %rdx
	leaq	12(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$13, %rdx
	leaq	13(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$14, %rdx
	leaq	14(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$15, %rdx
	leaq	15(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	cmpq	$16, %rdx
	leaq	16(%r8), %rsi
	jbe	.L1771
	addl	(%r9,%rsi,4), %edi
	addq	$17, %r8
.L1745:
	cmpq	%rax, %rdx
	je	.L1742
.L1744:
	subq	%rdx, %rax
	movq	%rax, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %rsi
	testq	%rsi, %rsi
	je	.L1747
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdx,4), %rcx
	xorl	%edx, %edx
.L1753:
	addq	$1, %rdx
	vpaddd	(%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%r10, %rdx
	jb	.L1753
	vmovdqa	%xmm0, %xmm1
	addq	%rsi, %r8
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %edx
	addl	%ecx, %edx
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm0, %ecx
	addl	%ecx, %edx
	addl	%edx, %edi
	cmpq	%rax, %rsi
	je	.L1779
	vzeroupper
.L1747:
	leaq	1(%r8), %rax
	addl	(%r9,%r8,4), %edi
	cmpq	%rax, %r12
	jle	.L1742
	addl	(%r9,%rax,4), %edi
	leaq	2(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1742
	addl	(%r9,%rax,4), %edi
	leaq	3(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1742
	addl	(%r9,%rax,4), %edi
	leaq	4(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1742
	addl	(%r9,%rax,4), %edi
	leaq	5(%r8), %rax
	cmpq	%rax, %r12
	jle	.L1742
	addq	$6, %r8
	addl	(%r9,%rax,4), %edi
	cmpq	%r8, %r12
	jle	.L1742
	addl	(%r9,%r8,4), %edi
.L1742:
	movl	%edi, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1771:
	movq	%rsi, %r8
	jmp	.L1745
.L1779:
	vzeroupper
	movl	%edi, (%r14)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1754:
	xorl	%edi, %edi
	xorl	%r8d, %r8d
	jmp	.L1740
.L1763:
	movq	%r10, %r8
	jmp	.L1745
unroll7aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-6(%rax), %rbx
	movq	%r14, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r10
	jle	.L1796
	movq	%rax, %rdx
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
.L1783:
	movl	(%rdx), %ecx
	addl	4(%rdx), %ecx
	addq	$7, %r9
	addl	8(%rdx), %ecx
	movl	16(%rdx), %edi
	addq	$28, %rdx
	addl	-8(%rdx), %edi
	addl	-16(%rdx), %ecx
	addl	-4(%rdx), %edi
	addl	%edi, %ecx
	addl	%ecx, %r8d
	cmpq	%r9, %rbx
	jg	.L1783
.L1782:
	cmpq	%r9, %r12
	jle	.L1784
	leaq	(%r10,%r9,4), %rcx
	movq	%r12, %rax
	subq	%r9, %rax
	movq	%rcx, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1786
	addl	(%rcx), %r8d
	cmpq	$1, %rdx
	leaq	1(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$2, %rdx
	leaq	2(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$3, %rdx
	leaq	3(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$4, %rdx
	leaq	4(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$5, %rdx
	leaq	5(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$6, %rdx
	leaq	6(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$7, %rdx
	leaq	7(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$8, %rdx
	leaq	8(%r9), %rdi
	movl	%r8d, %esi
	jbe	.L1805
	addl	(%r10,%rdi,4), %esi
	cmpq	$9, %rdx
	movl	%esi, %r8d
	leaq	9(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$10, %rdx
	leaq	10(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$11, %rdx
	leaq	11(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$12, %rdx
	leaq	12(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$13, %rdx
	leaq	13(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$14, %rdx
	leaq	14(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$15, %rdx
	leaq	15(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	cmpq	$16, %rdx
	leaq	16(%r9), %rsi
	jbe	.L1813
	addl	(%r10,%rsi,4), %r8d
	addq	$17, %r9
.L1787:
	cmpq	%rax, %rdx
	je	.L1784
.L1786:
	subq	%rdx, %rax
	movq	%rax, %rdi
	shrq	$3, %rdi
	leaq	0(,%rdi,8), %rsi
	testq	%rsi, %rsi
	je	.L1789
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdx,4), %rcx
	xorl	%edx, %edx
.L1795:
	addq	$1, %rdx
	vpaddd	(%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%rdi, %rdx
	jb	.L1795
	vmovdqa	%xmm0, %xmm1
	addq	%rsi, %r9
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %edx
	addl	%ecx, %edx
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm0, %ecx
	addl	%ecx, %edx
	addl	%edx, %r8d
	cmpq	%rax, %rsi
	je	.L1821
	vzeroupper
.L1789:
	leaq	1(%r9), %rax
	addl	(%r10,%r9,4), %r8d
	cmpq	%rax, %r12
	jle	.L1784
	addl	(%r10,%rax,4), %r8d
	leaq	2(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1784
	addl	(%r10,%rax,4), %r8d
	leaq	3(%r9), %rax
	cmpq	%r12, %rax
	jge	.L1784
	addl	(%r10,%rax,4), %r8d
	leaq	4(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1784
	addl	(%r10,%rax,4), %r8d
	leaq	5(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1784
	addq	$6, %r9
	addl	(%r10,%rax,4), %r8d
	cmpq	%r9, %r12
	jle	.L1784
	addl	(%r10,%r9,4), %r8d
.L1784:
	movl	%r8d, 0(%r13)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1813:
	movq	%rsi, %r9
	jmp	.L1787
.L1821:
	vzeroupper
	jmp	.L1784
.L1796:
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	jmp	.L1782
.L1805:
	movq	%rdi, %r9
	jmp	.L1787
unroll8aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-7(%rax), %r13
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r13, %r13
	movq	%rax, %r10
	jle	.L1843
	leaq	-8(%rbx), %rax
	shrq	$3, %rax
	addq	$1, %rax
	movq	%rax, %rsi
	shrq	$3, %rsi
	leaq	0(,%rsi,8), %rdx
	testq	%rdx, %rdx
	je	.L1844
	cmpq	$7, %rax
	jbe	.L1844
	vpxor	%xmm0, %xmm0, %xmm0
	movq	%r10, %rcx
	xorl	%edi, %edi
.L1830:
	vmovdqu	(%rcx), %xmm6
	addq	$1, %rdi
	addq	$256, %rcx
	vmovdqu	-224(%rcx), %xmm8
	vinserti128	$0x1, -240(%rcx), %ymm6, %ymm6
	vmovdqu	-192(%rcx), %xmm1
	vinserti128	$0x1, -208(%rcx), %ymm8, %ymm8
	vmovdqu	-160(%rcx), %xmm9
	vinserti128	$0x1, -176(%rcx), %ymm1, %ymm1
	vmovdqu	-128(%rcx), %xmm5
	vinserti128	$0x1, -144(%rcx), %ymm9, %ymm9
	vmovdqu	-96(%rcx), %xmm10
	vperm2i128	$32, %ymm8, %ymm6, %ymm4
	vperm2i128	$49, %ymm8, %ymm6, %ymm2
	vmovdqu	-64(%rcx), %xmm3
	vinserti128	$0x1, -80(%rcx), %ymm10, %ymm10
	vmovdqu	-32(%rcx), %xmm7
	vpshufd	$216, %ymm2, %ymm2
	vpshufd	$216, %ymm4, %ymm4
	vinserti128	$0x1, -112(%rcx), %ymm5, %ymm5
	vpunpcklqdq	%ymm2, %ymm4, %ymm4
	vperm2i128	$32, %ymm8, %ymm6, %ymm2
	vperm2i128	$49, %ymm8, %ymm6, %ymm6
	vperm2i128	$32, %ymm9, %ymm1, %ymm8
	vpshufd	$216, %ymm2, %ymm2
	vpshufd	$216, %ymm6, %ymm6
	vpshufd	$216, %ymm8, %ymm8
	vinserti128	$0x1, -16(%rcx), %ymm7, %ymm7
	vpunpckhqdq	%ymm6, %ymm2, %ymm2
	vperm2i128	$49, %ymm9, %ymm1, %ymm6
	vinserti128	$0x1, -48(%rcx), %ymm3, %ymm3
	cmpq	%rdi, %rsi
	vpshufd	$216, %ymm6, %ymm6
	vpunpcklqdq	%ymm6, %ymm8, %ymm8
	vperm2i128	$32, %ymm9, %ymm1, %ymm6
	vperm2i128	$49, %ymm9, %ymm1, %ymm1
	vperm2i128	$32, %ymm10, %ymm5, %ymm9
	vpshufd	$216, %ymm6, %ymm6
	vpshufd	$216, %ymm1, %ymm1
	vpshufd	$216, %ymm9, %ymm9
	vpunpckhqdq	%ymm1, %ymm6, %ymm6
	vperm2i128	$49, %ymm10, %ymm5, %ymm1
	vpshufd	$216, %ymm1, %ymm1
	vpunpcklqdq	%ymm1, %ymm9, %ymm1
	vperm2i128	$32, %ymm10, %ymm5, %ymm9
	vperm2i128	$49, %ymm10, %ymm5, %ymm5
	vperm2i128	$32, %ymm7, %ymm3, %ymm10
	vpshufd	$216, %ymm9, %ymm9
	vpshufd	$216, %ymm5, %ymm5
	vpshufd	$216, %ymm10, %ymm10
	vpunpckhqdq	%ymm5, %ymm9, %ymm9
	vperm2i128	$49, %ymm7, %ymm3, %ymm5
	vpshufd	$216, %ymm5, %ymm5
	vpunpcklqdq	%ymm5, %ymm10, %ymm5
	vperm2i128	$32, %ymm7, %ymm3, %ymm10
	vperm2i128	$49, %ymm7, %ymm3, %ymm3
	vperm2i128	$32, %ymm8, %ymm4, %ymm7
	vpshufd	$216, %ymm10, %ymm10
	vpshufd	$216, %ymm3, %ymm3
	vpshufd	$216, %ymm7, %ymm7
	vpunpckhqdq	%ymm3, %ymm10, %ymm10
	vperm2i128	$49, %ymm8, %ymm4, %ymm3
	vpshufd	$216, %ymm3, %ymm3
	vpunpcklqdq	%ymm3, %ymm7, %ymm7
	vperm2i128	$32, %ymm8, %ymm4, %ymm3
	vperm2i128	$49, %ymm8, %ymm4, %ymm4
	vperm2i128	$32, %ymm5, %ymm1, %ymm8
	vpshufd	$216, %ymm3, %ymm3
	vpshufd	$216, %ymm4, %ymm4
	vpshufd	$216, %ymm8, %ymm8
	vpunpckhqdq	%ymm4, %ymm3, %ymm3
	vperm2i128	$49, %ymm5, %ymm1, %ymm4
	vpshufd	$216, %ymm4, %ymm4
	vpunpcklqdq	%ymm4, %ymm8, %ymm8
	vperm2i128	$32, %ymm5, %ymm1, %ymm4
	vperm2i128	$49, %ymm5, %ymm1, %ymm1
	vperm2i128	$32, %ymm6, %ymm2, %ymm5
	vpshufd	$216, %ymm4, %ymm4
	vpshufd	$216, %ymm1, %ymm1
	vpshufd	$216, %ymm5, %ymm5
	vpunpckhqdq	%ymm1, %ymm4, %ymm4
	vperm2i128	$49, %ymm6, %ymm2, %ymm1
	vpshufd	$216, %ymm1, %ymm1
	vpunpcklqdq	%ymm1, %ymm5, %ymm5
	vperm2i128	$32, %ymm6, %ymm2, %ymm1
	vperm2i128	$49, %ymm6, %ymm2, %ymm2
	vperm2i128	$32, %ymm10, %ymm9, %ymm6
	vpshufd	$216, %ymm1, %ymm1
	vpshufd	$216, %ymm2, %ymm2
	vpshufd	$216, %ymm6, %ymm6
	vpunpckhqdq	%ymm2, %ymm1, %ymm1
	vperm2i128	$49, %ymm10, %ymm9, %ymm2
	vpshufd	$216, %ymm2, %ymm2
	vpunpcklqdq	%ymm2, %ymm6, %ymm6
	vperm2i128	$32, %ymm10, %ymm9, %ymm2
	vperm2i128	$49, %ymm10, %ymm9, %ymm9
	vperm2i128	$32, %ymm8, %ymm7, %ymm10
	vpshufd	$216, %ymm2, %ymm2
	vpshufd	$216, %ymm9, %ymm9
	vpshufd	$216, %ymm10, %ymm10
	vperm2i128	$32, %ymm6, %ymm5, %ymm11
	vpunpckhqdq	%ymm9, %ymm2, %ymm2
	vperm2i128	$49, %ymm8, %ymm7, %ymm9
	vpshufd	$216, %ymm11, %ymm11
	vpshufd	$216, %ymm9, %ymm9
	vpunpcklqdq	%ymm9, %ymm10, %ymm10
	vperm2i128	$49, %ymm6, %ymm5, %ymm9
	vpshufd	$216, %ymm9, %ymm9
	vpunpcklqdq	%ymm9, %ymm11, %ymm11
	vperm2i128	$49, %ymm4, %ymm3, %ymm9
	vpaddd	%ymm11, %ymm10, %ymm10
	vperm2i128	$32, %ymm4, %ymm3, %ymm11
	vpshufd	$216, %ymm9, %ymm9
	vpshufd	$216, %ymm11, %ymm11
	vpunpcklqdq	%ymm9, %ymm11, %ymm12
	vperm2i128	$32, %ymm2, %ymm1, %ymm11
	vperm2i128	$49, %ymm2, %ymm1, %ymm9
	vpshufd	$216, %ymm11, %ymm11
	vpshufd	$216, %ymm9, %ymm9
	vpunpcklqdq	%ymm9, %ymm11, %ymm9
	vpaddd	%ymm9, %ymm12, %ymm11
	vperm2i128	$32, %ymm8, %ymm7, %ymm9
	vpaddd	%ymm11, %ymm10, %ymm10
	vperm2i128	$49, %ymm8, %ymm7, %ymm7
	vpshufd	$216, %ymm9, %ymm9
	vpshufd	$216, %ymm7, %ymm7
	vpunpckhqdq	%ymm7, %ymm9, %ymm7
	vperm2i128	$32, %ymm6, %ymm5, %ymm9
	vperm2i128	$49, %ymm6, %ymm5, %ymm5
	vpshufd	$216, %ymm9, %ymm9
	vpshufd	$216, %ymm5, %ymm5
	vpunpckhqdq	%ymm5, %ymm9, %ymm5
	vpaddd	%ymm5, %ymm7, %ymm9
	vperm2i128	$32, %ymm4, %ymm3, %ymm5
	vperm2i128	$49, %ymm4, %ymm3, %ymm3
	vpshufd	$216, %ymm5, %ymm5
	vpshufd	$216, %ymm3, %ymm3
	vpunpckhqdq	%ymm3, %ymm5, %ymm3
	vperm2i128	$32, %ymm2, %ymm1, %ymm5
	vperm2i128	$49, %ymm2, %ymm1, %ymm1
	vpshufd	$216, %ymm5, %ymm5
	vpshufd	$216, %ymm1, %ymm1
	vpunpckhqdq	%ymm1, %ymm5, %ymm1
	vpaddd	%ymm1, %ymm3, %ymm1
	vpaddd	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm1, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm0, %ymm0
	ja	.L1830
	vmovdqa	%xmm0, %xmm1
	leaq	0(,%rdx,8), %r9
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	vpextrd	$2, %xmm0, %r8d
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	addl	%r8d, %ecx
	vpextrd	$3, %xmm0, %r8d
	addl	%ecx, %r8d
	cmpq	%rax, %rdx
	je	.L1828
.L1825:
	leaq	(%r10,%r9,4), %rdx
.L1829:
	movl	(%rdx), %edi
	movl	16(%rdx), %ecx
	addq	$8, %r9
	addl	4(%rdx), %edi
	addl	20(%rdx), %ecx
	addq	$32, %rdx
	addl	-24(%rdx), %edi
	addl	-8(%rdx), %ecx
	addl	-20(%rdx), %edi
	addl	-4(%rdx), %ecx
	addl	%edi, %ecx
	addl	%ecx, %r8d
	cmpq	%r9, %r13
	jg	.L1829
.L1828:
	leaq	0(,%rax,8), %rdx
.L1824:
	cmpq	%rdx, %rbx
	jle	.L1831
	leaq	(%r10,%rdx,4), %rsi
	movq	%rbx, %rax
	subq	%rdx, %rax
	movq	%rsi, %rcx
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpq	%rax, %rcx
	cmova	%rax, %rcx
	cmpq	$17, %rax
	cmovbe	%rax, %rcx
	testq	%rcx, %rcx
	je	.L1833
	addl	(%rsi), %r8d
	cmpq	$1, %rcx
	leaq	1(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$2, %rcx
	leaq	2(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$3, %rcx
	leaq	3(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$4, %rcx
	leaq	4(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$5, %rcx
	leaq	5(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$6, %rcx
	leaq	6(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$7, %rcx
	leaq	7(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$8, %rcx
	leaq	8(%rdx), %r9
	movl	%r8d, %edi
	jbe	.L1853
	addl	(%r10,%r9,4), %edi
	cmpq	$9, %rcx
	movl	%edi, %r8d
	leaq	9(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$10, %rcx
	leaq	10(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$11, %rcx
	leaq	11(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$12, %rcx
	leaq	12(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$13, %rcx
	leaq	13(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$14, %rcx
	leaq	14(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$15, %rcx
	leaq	15(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	cmpq	$16, %rcx
	leaq	16(%rdx), %rdi
	jbe	.L1861
	addl	(%r10,%rdi,4), %r8d
	addq	$17, %rdx
.L1834:
	cmpq	%rax, %rcx
	je	.L1831
.L1833:
	subq	%rcx, %rax
	movq	%rax, %r9
	shrq	$3, %r9
	leaq	0(,%r9,8), %rdi
	testq	%rdi, %rdi
	je	.L1836
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rsi,%rcx,4), %rsi
	xorl	%ecx, %ecx
.L1842:
	addq	$1, %rcx
	vpaddd	(%rsi), %ymm0, %ymm0
	addq	$32, %rsi
	cmpq	%r9, %rcx
	jb	.L1842
	vmovdqa	%xmm0, %xmm1
	addq	%rdi, %rdx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %esi
	vpextrd	$0, %xmm1, %ecx
	addl	%esi, %ecx
	vpextrd	$2, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm1, %esi
	addl	%esi, %ecx
	vpextrd	$0, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$1, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$2, %xmm0, %esi
	addl	%esi, %ecx
	vpextrd	$3, %xmm0, %esi
	addl	%esi, %ecx
	addl	%ecx, %r8d
	cmpq	%rax, %rdi
	je	.L1831
.L1836:
	leaq	1(%rdx), %rax
	addl	(%r10,%rdx,4), %r8d
	cmpq	%rax, %rbx
	jle	.L1831
	addl	(%r10,%rax,4), %r8d
	leaq	2(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1831
	addl	(%r10,%rax,4), %r8d
	leaq	3(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1831
	addl	(%r10,%rax,4), %r8d
	leaq	4(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1831
	addl	(%r10,%rax,4), %r8d
	leaq	5(%rdx), %rax
	cmpq	%rax, %rbx
	jle	.L1831
	addq	$6, %rdx
	addl	(%r10,%rax,4), %r8d
	cmpq	%rdx, %rbx
	jle	.L1831
	addl	(%r10,%rdx,4), %r8d
.L1831:
	movl	%r8d, (%r12)
	vzeroupper
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1861:
	movq	%rdi, %rdx
	jmp	.L1834
.L1844:
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	jmp	.L1825
.L1843:
	xorl	%r8d, %r8d
	xorl	%edx, %edx
	jmp	.L1824
.L1853:
	movq	%r9, %rdx
	jmp	.L1834
unroll9aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-8(%rax), %rbx
	movq	%r14, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r10
	jle	.L1886
	movq	%rax, %rdx
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
.L1873:
	movl	16(%rdx), %ecx
	addl	20(%rdx), %ecx
	addq	$9, %r9
	movl	(%rdx), %edi
	addl	24(%rdx), %ecx
	addq	$36, %rdx
	addl	-32(%rdx), %edi
	addl	-8(%rdx), %ecx
	addl	-28(%rdx), %edi
	addl	-4(%rdx), %ecx
	addl	-24(%rdx), %edi
	addl	%edi, %ecx
	addl	%ecx, %r8d
	cmpq	%r9, %rbx
	jg	.L1873
.L1872:
	cmpq	%r9, %r12
	jle	.L1874
	leaq	(%r10,%r9,4), %rcx
	movq	%r12, %rax
	subq	%r9, %rax
	movq	%rcx, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1876
	addl	(%rcx), %r8d
	cmpq	$1, %rdx
	leaq	1(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$2, %rdx
	leaq	2(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$3, %rdx
	leaq	3(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$4, %rdx
	leaq	4(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$5, %rdx
	leaq	5(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$6, %rdx
	leaq	6(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$7, %rdx
	leaq	7(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$8, %rdx
	leaq	8(%r9), %rdi
	movl	%r8d, %esi
	jbe	.L1895
	addl	(%r10,%rdi,4), %esi
	cmpq	$9, %rdx
	movl	%esi, %r8d
	leaq	9(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$10, %rdx
	leaq	10(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$11, %rdx
	leaq	11(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$12, %rdx
	leaq	12(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$13, %rdx
	leaq	13(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$14, %rdx
	leaq	14(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$15, %rdx
	leaq	15(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	cmpq	$16, %rdx
	leaq	16(%r9), %rsi
	jbe	.L1903
	addl	(%r10,%rsi,4), %r8d
	addq	$17, %r9
.L1877:
	cmpq	%rax, %rdx
	je	.L1874
.L1876:
	subq	%rdx, %rax
	movq	%rax, %rdi
	shrq	$3, %rdi
	leaq	0(,%rdi,8), %rsi
	testq	%rsi, %rsi
	je	.L1879
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdx,4), %rcx
	xorl	%edx, %edx
.L1885:
	addq	$1, %rdx
	vpaddd	(%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%rdi, %rdx
	jb	.L1885
	vmovdqa	%xmm0, %xmm1
	addq	%rsi, %r9
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %edx
	addl	%ecx, %edx
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm0, %ecx
	addl	%ecx, %edx
	addl	%edx, %r8d
	cmpq	%rax, %rsi
	je	.L1911
	vzeroupper
.L1879:
	leaq	1(%r9), %rax
	addl	(%r10,%r9,4), %r8d
	cmpq	%rax, %r12
	jle	.L1874
	addl	(%r10,%rax,4), %r8d
	leaq	2(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1874
	addl	(%r10,%rax,4), %r8d
	leaq	3(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1874
	addl	(%r10,%rax,4), %r8d
	leaq	4(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1874
	addl	(%r10,%rax,4), %r8d
	leaq	5(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1874
	addq	$6, %r9
	addl	(%r10,%rax,4), %r8d
	cmpq	%r12, %r9
	jge	.L1874
	addl	(%r10,%r9,4), %r8d
.L1874:
	movl	%r8d, 0(%r13)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1903:
	movq	%rsi, %r9
	jmp	.L1877
.L1911:
	vzeroupper
	jmp	.L1874
.L1886:
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	jmp	.L1872
.L1895:
	movq	%rdi, %r9
	jmp	.L1877
unroll10aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-9(%rax), %rbx
	movq	%r14, %rdi
	movq	%rax, %r12
	call	get_vec_start
	testq	%rbx, %rbx
	movq	%rax, %r10
	jle	.L1928
	movq	%rax, %rdx
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
.L1915:
	movl	16(%rdx), %ecx
	addl	20(%rdx), %ecx
	addq	$10, %r9
	addl	24(%rdx), %ecx
	movl	(%rdx), %edi
	addq	$40, %rdx
	addl	-36(%rdx), %edi
	addl	-12(%rdx), %ecx
	addl	-32(%rdx), %edi
	addl	-8(%rdx), %ecx
	addl	-28(%rdx), %edi
	addl	-4(%rdx), %ecx
	addl	%edi, %ecx
	addl	%ecx, %r8d
	cmpq	%r9, %rbx
	jg	.L1915
.L1914:
	cmpq	%r9, %r12
	jle	.L1916
	leaq	(%r10,%r9,4), %rcx
	movq	%r12, %rax
	subq	%r9, %rax
	movq	%rcx, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1918
	addl	(%rcx), %r8d
	cmpq	$1, %rdx
	leaq	1(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$2, %rdx
	leaq	2(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$3, %rdx
	leaq	3(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$4, %rdx
	leaq	4(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$5, %rdx
	leaq	5(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$6, %rdx
	leaq	6(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$7, %rdx
	leaq	7(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$8, %rdx
	leaq	8(%r9), %rdi
	movl	%r8d, %esi
	jbe	.L1937
	addl	(%r10,%rdi,4), %esi
	cmpq	$9, %rdx
	movl	%esi, %r8d
	leaq	9(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$10, %rdx
	leaq	10(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$11, %rdx
	leaq	11(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$12, %rdx
	leaq	12(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$13, %rdx
	leaq	13(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$14, %rdx
	leaq	14(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$15, %rdx
	leaq	15(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	cmpq	$16, %rdx
	leaq	16(%r9), %rsi
	jbe	.L1945
	addl	(%r10,%rsi,4), %r8d
	addq	$17, %r9
.L1919:
	cmpq	%rax, %rdx
	je	.L1916
.L1918:
	subq	%rdx, %rax
	movq	%rax, %rdi
	shrq	$3, %rdi
	leaq	0(,%rdi,8), %rsi
	testq	%rsi, %rsi
	je	.L1921
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdx,4), %rcx
	xorl	%edx, %edx
.L1927:
	addq	$1, %rdx
	vpaddd	(%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%rdi, %rdx
	jb	.L1927
	vmovdqa	%xmm0, %xmm1
	addq	%rsi, %r9
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %edx
	addl	%ecx, %edx
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm0, %ecx
	addl	%ecx, %edx
	addl	%edx, %r8d
	cmpq	%rax, %rsi
	je	.L1953
	vzeroupper
.L1921:
	leaq	1(%r9), %rax
	addl	(%r10,%r9,4), %r8d
	cmpq	%rax, %r12
	jle	.L1916
	addl	(%r10,%rax,4), %r8d
	leaq	2(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1916
	addl	(%r10,%rax,4), %r8d
	leaq	3(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1916
	addl	(%r10,%rax,4), %r8d
	leaq	4(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1916
	addl	(%r10,%rax,4), %r8d
	leaq	5(%r9), %rax
	cmpq	%rax, %r12
	jle	.L1916
	addq	$6, %r9
	addl	(%r10,%rax,4), %r8d
	cmpq	%r9, %r12
	jle	.L1916
	addl	(%r10,%r9,4), %r8d
.L1916:
	movl	%r8d, 0(%r13)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1945:
	movq	%rsi, %r9
	jmp	.L1919
.L1953:
	vzeroupper
	jmp	.L1916
.L1928:
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	jmp	.L1914
.L1937:
	movq	%rdi, %r9
	jmp	.L1919
unroll12aa_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r14
	movq	%rdi, %r14
	pushq	%r13
	pushq	%r12
	movq	%rsi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	call	vec_length
	leaq	-11(%rax), %r13
	movq	%r14, %rdi
	movq	%rax, %rbx
	call	get_vec_start
	testq	%r13, %r13
	movq	%rax, %r10
	jle	.L1970
	movq	%rax, %rdx
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
.L1957:
	movl	(%rdx), %edi
	movl	16(%rdx), %ecx
	addq	$12, %r9
	addl	4(%rdx), %edi
	addl	20(%rdx), %ecx
	addq	$48, %rdx
	addl	-40(%rdx), %edi
	addl	-24(%rdx), %ecx
	addl	-20(%rdx), %ecx
	addl	-36(%rdx), %edi
	addl	%ecx, %edi
	movl	-16(%rdx), %ecx
	addl	-12(%rdx), %ecx
	addl	-8(%rdx), %ecx
	addl	-4(%rdx), %ecx
	addl	%edi, %ecx
	addl	%ecx, %r8d
	cmpq	%r9, %r13
	jg	.L1957
.L1956:
	cmpq	%r9, %rbx
	jle	.L1958
	leaq	(%r10,%r9,4), %rcx
	movq	%rbx, %rax
	subq	%r9, %rax
	movq	%rcx, %rdx
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpq	%rax, %rdx
	cmova	%rax, %rdx
	cmpq	$17, %rax
	cmovbe	%rax, %rdx
	testq	%rdx, %rdx
	je	.L1960
	addl	(%rcx), %r8d
	cmpq	$1, %rdx
	leaq	1(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$2, %rdx
	leaq	2(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$3, %rdx
	leaq	3(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$4, %rdx
	leaq	4(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$5, %rdx
	leaq	5(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$6, %rdx
	leaq	6(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$7, %rdx
	leaq	7(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$8, %rdx
	leaq	8(%r9), %rdi
	movl	%r8d, %esi
	jbe	.L1979
	addl	(%r10,%rdi,4), %esi
	cmpq	$9, %rdx
	movl	%esi, %r8d
	leaq	9(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$10, %rdx
	leaq	10(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$11, %rdx
	leaq	11(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$12, %rdx
	leaq	12(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$13, %rdx
	leaq	13(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$14, %rdx
	leaq	14(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$15, %rdx
	leaq	15(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	cmpq	$16, %rdx
	leaq	16(%r9), %rsi
	jbe	.L1987
	addl	(%r10,%rsi,4), %r8d
	addq	$17, %r9
.L1961:
	cmpq	%rax, %rdx
	je	.L1958
.L1960:
	subq	%rdx, %rax
	movq	%rax, %rdi
	shrq	$3, %rdi
	leaq	0(,%rdi,8), %rsi
	testq	%rsi, %rsi
	je	.L1963
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rcx,%rdx,4), %rcx
	xorl	%edx, %edx
.L1969:
	addq	$1, %rdx
	vpaddd	(%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%rdi, %rdx
	jb	.L1969
	vmovdqa	%xmm0, %xmm1
	addq	%rsi, %r9
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %ecx
	vpextrd	$0, %xmm1, %edx
	addl	%ecx, %edx
	vpextrd	$2, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm1, %ecx
	addl	%ecx, %edx
	vpextrd	$0, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$1, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$2, %xmm0, %ecx
	addl	%ecx, %edx
	vpextrd	$3, %xmm0, %ecx
	addl	%ecx, %edx
	addl	%edx, %r8d
	cmpq	%rax, %rsi
	je	.L1995
	vzeroupper
.L1963:
	leaq	1(%r9), %rax
	addl	(%r10,%r9,4), %r8d
	cmpq	%rax, %rbx
	jle	.L1958
	addl	(%r10,%rax,4), %r8d
	leaq	2(%r9), %rax
	cmpq	%rax, %rbx
	jle	.L1958
	addl	(%r10,%rax,4), %r8d
	leaq	3(%r9), %rax
	cmpq	%rax, %rbx
	jle	.L1958
	addl	(%r10,%rax,4), %r8d
	leaq	4(%r9), %rax
	cmpq	%rax, %rbx
	jle	.L1958
	addl	(%r10,%rax,4), %r8d
	leaq	5(%r9), %rax
	cmpq	%rax, %rbx
	jle	.L1958
	addq	$6, %r9
	addl	(%r10,%rax,4), %r8d
	cmpq	%r9, %rbx
	jle	.L1958
	addl	(%r10,%r9,4), %r8d
.L1958:
	movl	%r8d, (%r12)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%rbp
	ret

.L1987:
	movq	%rsi, %r9
	jmp	.L1961
.L1995:
	vzeroupper
	jmp	.L1958
.L1970:
	xorl	%r8d, %r8d
	xorl	%r9d, %r9d
	jmp	.L1956
.L1979:
	movq	%rdi, %r9
	jmp	.L1961
simd_v1_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$64, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, 32(%rsp)
	movq	$0, 40(%rsp)
	testb	$31, %bl
	movq	$0, 48(%rsp)
	movq	$0, 56(%rsp)
	movl	%eax, %edx
	vmovdqa	32(%rsp), %ymm4
	vmovdqa	%ymm4, (%rsp)
	je	.L2018
	testl	%eax, %eax
	movl	$0, %eax
	jne	.L2003
	jmp	.L2006
.L2000:
	testl	%edx, %edx
	je	.L2006
.L2003:
	addq	$4, %rbx
	addl	-4(%rbx), %eax
	subl	$1, %edx
	testb	$31, %bl
	jne	.L2000
.L1998:
	cmpl	$7, %edx
	jbe	.L2002
	movl	%edx, %edi
	movq	%rbx, %rcx
.L2005:
	vmovdqa	(%rsp), %ymm3
	subl	$8, %edi
	addq	$32, %rcx
	vpaddd	-32(%rcx), %ymm3, %ymm2
	cmpl	$7, %edi
	vmovdqa	%ymm2, (%rsp)
	ja	.L2005
	subl	$8, %edx
	movl	%edx, %esi
	shrl	$3, %esi
	movl	%esi, %ecx
	negl	%esi
	addq	$1, %rcx
	leal	(%rdx,%rsi,8), %edx
	salq	$5, %rcx
	addq	%rcx, %rbx
.L2002:
	testl	%edx, %edx
	je	.L2006
	movq	%rbx, %rcx
	movl	%edx, %esi
	movl	%edx, %edi
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpl	%edx, %ecx
	cmova	%edx, %ecx
	cmpl	$17, %edx
	ja	.L2064
.L2007:
	addl	(%rbx), %eax
	cmpl	$1, %edi
	leaq	4(%rbx), %rcx
	leal	-1(%rdx), %r8d
	jbe	.L2037
	addl	4(%rbx), %eax
	cmpl	$2, %edi
	leaq	8(%rbx), %rcx
	leal	-2(%rdx), %r8d
	jbe	.L2037
	addl	8(%rbx), %eax
	cmpl	$3, %edi
	leaq	12(%rbx), %rcx
	leal	-3(%rdx), %r8d
	jbe	.L2037
	addl	12(%rbx), %eax
	cmpl	$4, %edi
	leaq	16(%rbx), %rcx
	leal	-4(%rdx), %r8d
	jbe	.L2037
	addl	16(%rbx), %eax
	cmpl	$5, %edi
	leaq	20(%rbx), %rcx
	leal	-5(%rdx), %r8d
	jbe	.L2037
	addl	20(%rbx), %eax
	cmpl	$6, %edi
	leaq	24(%rbx), %rcx
	leal	-6(%rdx), %r8d
	jbe	.L2037
	addl	24(%rbx), %eax
	cmpl	$7, %edi
	leaq	28(%rbx), %rcx
	leal	-7(%rdx), %r8d
	jbe	.L2037
	addl	28(%rbx), %eax
	cmpl	$8, %edi
	leaq	32(%rbx), %rcx
	leal	-8(%rdx), %r8d
	jbe	.L2037
	addl	32(%rbx), %eax
	cmpl	$9, %edi
	leaq	36(%rbx), %rcx
	leal	-9(%rdx), %r8d
	jbe	.L2037
	addl	36(%rbx), %eax
	cmpl	$10, %edi
	leaq	40(%rbx), %rcx
	leal	-10(%rdx), %r9d
	jbe	.L2031
	addl	40(%rbx), %eax
	cmpl	$11, %edi
	leaq	44(%rbx), %rcx
	leal	-11(%rdx), %r8d
	jbe	.L2037
	addl	44(%rbx), %eax
	cmpl	$12, %edi
	leaq	48(%rbx), %rcx
	leal	-12(%rdx), %r8d
	jbe	.L2037
	addl	48(%rbx), %eax
	cmpl	$13, %edi
	leaq	52(%rbx), %rcx
	leal	-13(%rdx), %r8d
	jbe	.L2037
	addl	52(%rbx), %eax
	cmpl	$14, %edi
	leaq	56(%rbx), %rcx
	leal	-14(%rdx), %r8d
	jbe	.L2037
	addl	56(%rbx), %eax
	cmpl	$15, %edi
	leaq	60(%rbx), %rcx
	leal	-15(%rdx), %r8d
	jbe	.L2037
	addl	60(%rbx), %eax
	cmpl	$16, %edi
	leaq	64(%rbx), %rcx
	leal	-16(%rdx), %r8d
	jbe	.L2037
	addl	64(%rbx), %eax
	leaq	68(%rbx), %rcx
	subl	$17, %edx
.L2009:
	cmpl	%edi, %esi
	je	.L2006
.L2008:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2011
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2017:
	addl	$1, %edi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpl	%r9d, %edi
	jb	.L2017
	vmovdqa	%xmm0, %xmm1
	subl	%esi, %edx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %edi
	addl	%edi, %eax
	movl	%esi, %edi
	cmpl	%esi, %r10d
	leaq	(%rcx,%rdi,4), %rcx
	je	.L2006
.L2011:
	addl	(%rcx), %eax
	cmpl	$1, %edx
	je	.L2006
	addl	4(%rcx), %eax
	cmpl	$2, %edx
	je	.L2006
	addl	8(%rcx), %eax
	cmpl	$3, %edx
	je	.L2006
	addl	12(%rcx), %eax
	cmpl	$4, %edx
	je	.L2006
	addl	16(%rcx), %eax
	cmpl	$5, %edx
	je	.L2006
	addl	20(%rcx), %eax
	cmpl	$6, %edx
	je	.L2006
	addl	24(%rcx), %eax
.L2006:
	vmovdqa	(%rsp), %ymm5
	vmovdqa	(%rsp), %xmm0
	vmovdqa	%ymm5, 32(%rsp)
	vpaddd	48(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	movl	%eax, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2037:
	movl	%r8d, %edx
	jmp	.L2009
.L2064:
	testl	%ecx, %ecx
	jne	.L2065
	xorl	%edi, %edi
	movq	%rbx, %rcx
	jmp	.L2008
.L2031:
	movl	%r9d, %edx
	jmp	.L2009
.L2018:
	xorl	%eax, %eax
	jmp	.L1998
.L2065:
	movl	%ecx, %edi
	jmp	.L2007
simd_v2_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$64, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, 32(%rsp)
	movq	$0, 40(%rsp)
	testb	$31, %bl
	movq	$0, 48(%rsp)
	movq	$0, 56(%rsp)
	movl	%eax, %edx
	vmovdqa	32(%rsp), %ymm0
	je	.L2087
	testl	%eax, %eax
	movl	$0, %eax
	jne	.L2072
	jmp	.L2068
.L2069:
	testl	%edx, %edx
	je	.L2068
.L2072:
	addq	$4, %rbx
	addl	-4(%rbx), %eax
	subl	$1, %edx
	testb	$31, %bl
	jne	.L2069
.L2067:
	cmpl	$15, %edx
	jbe	.L2133
	vmovdqa	%ymm0, %ymm1
	movl	%edx, %edi
	movq	%rbx, %rcx
.L2074:
	subl	$16, %edi
	vpaddd	(%rcx), %ymm0, %ymm0
	vpaddd	32(%rcx), %ymm1, %ymm1
	addq	$64, %rcx
	cmpl	$15, %edi
	ja	.L2074
	subl	$16, %edx
	movl	%edx, %esi
	shrl	$4, %esi
	movl	%esi, %ecx
	sall	$4, %esi
	addq	$1, %rcx
	subl	%esi, %edx
	salq	$6, %rcx
	addq	%rcx, %rbx
.L2071:
	testl	%edx, %edx
	je	.L2075
	movq	%rbx, %rcx
	movl	%edx, %esi
	movl	%edx, %edi
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpl	%edx, %ecx
	cmova	%edx, %ecx
	cmpl	$17, %edx
	ja	.L2134
.L2076:
	addl	(%rbx), %eax
	cmpl	$1, %edi
	leaq	4(%rbx), %rcx
	leal	-1(%rdx), %r8d
	jbe	.L2106
	addl	4(%rbx), %eax
	cmpl	$2, %edi
	leaq	8(%rbx), %rcx
	leal	-2(%rdx), %r8d
	jbe	.L2106
	addl	8(%rbx), %eax
	cmpl	$3, %edi
	leaq	12(%rbx), %rcx
	leal	-3(%rdx), %r8d
	jbe	.L2106
	addl	12(%rbx), %eax
	cmpl	$4, %edi
	leaq	16(%rbx), %rcx
	leal	-4(%rdx), %r8d
	jbe	.L2106
	addl	16(%rbx), %eax
	cmpl	$5, %edi
	leaq	20(%rbx), %rcx
	leal	-5(%rdx), %r8d
	jbe	.L2106
	addl	20(%rbx), %eax
	cmpl	$6, %edi
	leaq	24(%rbx), %rcx
	leal	-6(%rdx), %r8d
	jbe	.L2106
	addl	24(%rbx), %eax
	cmpl	$7, %edi
	leaq	28(%rbx), %rcx
	leal	-7(%rdx), %r8d
	jbe	.L2106
	addl	28(%rbx), %eax
	cmpl	$8, %edi
	leaq	32(%rbx), %rcx
	leal	-8(%rdx), %r8d
	jbe	.L2106
	addl	32(%rbx), %eax
	cmpl	$9, %edi
	leaq	36(%rbx), %rcx
	leal	-9(%rdx), %r8d
	jbe	.L2106
	addl	36(%rbx), %eax
	cmpl	$10, %edi
	leaq	40(%rbx), %rcx
	leal	-10(%rdx), %r9d
	jbe	.L2100
	addl	40(%rbx), %eax
	cmpl	$11, %edi
	leaq	44(%rbx), %rcx
	leal	-11(%rdx), %r8d
	jbe	.L2106
	addl	44(%rbx), %eax
	cmpl	$12, %edi
	leaq	48(%rbx), %rcx
	leal	-12(%rdx), %r8d
	jbe	.L2106
	addl	48(%rbx), %eax
	cmpl	$13, %edi
	leaq	52(%rbx), %rcx
	leal	-13(%rdx), %r8d
	jbe	.L2106
	addl	52(%rbx), %eax
	cmpl	$14, %edi
	leaq	56(%rbx), %rcx
	leal	-14(%rdx), %r8d
	jbe	.L2106
	addl	56(%rbx), %eax
	cmpl	$15, %edi
	leaq	60(%rbx), %rcx
	leal	-15(%rdx), %r8d
	jbe	.L2106
	addl	60(%rbx), %eax
	cmpl	$16, %edi
	leaq	64(%rbx), %rcx
	leal	-16(%rdx), %r8d
	jbe	.L2106
	addl	64(%rbx), %eax
	leaq	68(%rbx), %rcx
	subl	$17, %edx
.L2078:
	cmpl	%edi, %esi
	je	.L2075
.L2077:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2080
	vpxor	%xmm2, %xmm2, %xmm2
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2086:
	addl	$1, %edi
	vpaddd	(%r8), %ymm2, %ymm2
	addq	$32, %r8
	cmpl	%r9d, %edi
	jb	.L2086
	vmovdqa	%xmm2, %xmm3
	subl	%esi, %edx
	vextracti128	$0x1, %ymm2, %xmm2
	vpextrd	$1, %xmm3, %r8d
	vpextrd	$0, %xmm3, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm3, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm3, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm2, %r8d
	addl	%r8d, %edi
	addl	%edi, %eax
	movl	%esi, %edi
	cmpl	%esi, %r10d
	leaq	(%rcx,%rdi,4), %rcx
	je	.L2075
.L2080:
	addl	(%rcx), %eax
	cmpl	$1, %edx
	je	.L2075
	addl	4(%rcx), %eax
	cmpl	$2, %edx
	je	.L2075
	addl	8(%rcx), %eax
	cmpl	$3, %edx
	je	.L2075
	addl	12(%rcx), %eax
	cmpl	$4, %edx
	je	.L2075
	addl	16(%rcx), %eax
	cmpl	$5, %edx
	je	.L2075
	addl	20(%rcx), %eax
	cmpl	$6, %edx
	je	.L2075
	addl	24(%rcx), %eax
.L2075:
	vpaddd	%ymm1, %ymm0, %ymm4
	vmovdqa	%ymm4, (%rsp)
	vmovdqa	%ymm4, 32(%rsp)
	vmovdqa	(%rsp), %xmm0
	vpaddd	48(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	movl	%eax, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2106:
	movl	%r8d, %edx
	jmp	.L2078
.L2068:
	vmovdqa	%ymm0, %ymm1
	jmp	.L2075
.L2134:
	testl	%ecx, %ecx
	jne	.L2135
	xorl	%edi, %edi
	movq	%rbx, %rcx
	jmp	.L2077
.L2100:
	movl	%r9d, %edx
	jmp	.L2078
.L2087:
	xorl	%eax, %eax
	jmp	.L2067
.L2133:
	vmovdqa	%ymm0, %ymm1
	jmp	.L2071
.L2135:
	movl	%ecx, %edi
	jmp	.L2076
simd_v4_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$64, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, 32(%rsp)
	movq	$0, 40(%rsp)
	xorl	%ecx, %ecx
	movq	$0, 48(%rsp)
	movq	$0, 56(%rsp)
	testb	$31, %bl
	movl	%eax, %edx
	vmovdqa	32(%rsp), %ymm0
	je	.L2137
	testl	%eax, %eax
	jne	.L2142
	jmp	.L2138
.L2139:
	testl	%edx, %edx
	je	.L2138
.L2142:
	addq	$4, %rbx
	addl	-4(%rbx), %ecx
	subl	$1, %edx
	testb	$31, %bl
	jne	.L2139
.L2137:
	cmpl	$31, %edx
	jbe	.L2203
	vmovdqa	%ymm0, %ymm1
	vmovdqa	%ymm0, %ymm2
	movl	%edx, %edi
	vmovdqa	%ymm0, %ymm3
	movq	%rbx, %rax
.L2144:
	subl	$32, %edi
	vpaddd	(%rax), %ymm0, %ymm0
	vpaddd	32(%rax), %ymm3, %ymm3
	vpaddd	64(%rax), %ymm2, %ymm2
	vpaddd	96(%rax), %ymm1, %ymm1
	subq	$-128, %rax
	cmpl	$31, %edi
	ja	.L2144
	subl	$32, %edx
	movl	%edx, %esi
	shrl	$5, %esi
	movl	%esi, %eax
	sall	$5, %esi
	addq	$1, %rax
	subl	%esi, %edx
	salq	$7, %rax
	addq	%rax, %rbx
.L2141:
	testl	%edx, %edx
	je	.L2145
	movq	%rbx, %rax
	movl	%edx, %esi
	movl	%edx, %edi
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpl	%edx, %eax
	cmova	%edx, %eax
	cmpl	$17, %edx
	ja	.L2204
.L2146:
	addl	(%rbx), %ecx
	cmpl	$1, %edi
	leaq	4(%rbx), %rax
	leal	-1(%rdx), %r8d
	jbe	.L2176
	addl	4(%rbx), %ecx
	cmpl	$2, %edi
	leaq	8(%rbx), %rax
	leal	-2(%rdx), %r8d
	jbe	.L2176
	addl	8(%rbx), %ecx
	cmpl	$3, %edi
	leaq	12(%rbx), %rax
	leal	-3(%rdx), %r8d
	jbe	.L2176
	addl	12(%rbx), %ecx
	cmpl	$4, %edi
	leaq	16(%rbx), %rax
	leal	-4(%rdx), %r8d
	jbe	.L2176
	addl	16(%rbx), %ecx
	cmpl	$5, %edi
	leaq	20(%rbx), %rax
	leal	-5(%rdx), %r8d
	jbe	.L2176
	addl	20(%rbx), %ecx
	cmpl	$6, %edi
	leaq	24(%rbx), %rax
	leal	-6(%rdx), %r8d
	jbe	.L2176
	addl	24(%rbx), %ecx
	cmpl	$7, %edi
	leaq	28(%rbx), %rax
	leal	-7(%rdx), %r8d
	jbe	.L2176
	addl	28(%rbx), %ecx
	cmpl	$8, %edi
	leaq	32(%rbx), %rax
	leal	-8(%rdx), %r8d
	jbe	.L2176
	addl	32(%rbx), %ecx
	cmpl	$9, %edi
	leaq	36(%rbx), %rax
	leal	-9(%rdx), %r8d
	jbe	.L2176
	addl	36(%rbx), %ecx
	cmpl	$10, %edi
	leaq	40(%rbx), %rax
	leal	-10(%rdx), %r9d
	jbe	.L2170
	addl	40(%rbx), %ecx
	cmpl	$11, %edi
	leaq	44(%rbx), %rax
	leal	-11(%rdx), %r8d
	jbe	.L2176
	addl	44(%rbx), %ecx
	cmpl	$12, %edi
	leaq	48(%rbx), %rax
	leal	-12(%rdx), %r8d
	jbe	.L2176
	addl	48(%rbx), %ecx
	cmpl	$13, %edi
	leaq	52(%rbx), %rax
	leal	-13(%rdx), %r8d
	jbe	.L2176
	addl	52(%rbx), %ecx
	cmpl	$14, %edi
	leaq	56(%rbx), %rax
	leal	-14(%rdx), %r8d
	jbe	.L2176
	addl	56(%rbx), %ecx
	cmpl	$15, %edi
	leaq	60(%rbx), %rax
	leal	-15(%rdx), %r8d
	jbe	.L2176
	addl	60(%rbx), %ecx
	cmpl	$16, %edi
	leaq	64(%rbx), %rax
	leal	-16(%rdx), %r8d
	jbe	.L2176
	addl	64(%rbx), %ecx
	leaq	68(%rbx), %rax
	subl	$17, %edx
.L2148:
	cmpl	%esi, %edi
	je	.L2145
.L2147:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2150
	vpxor	%xmm4, %xmm4, %xmm4
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2156:
	addl	$1, %edi
	vpaddd	(%r8), %ymm4, %ymm4
	addq	$32, %r8
	cmpl	%r9d, %edi
	jb	.L2156
	vmovdqa	%xmm4, %xmm5
	subl	%esi, %edx
	vextracti128	$0x1, %ymm4, %xmm4
	vpextrd	$1, %xmm5, %r8d
	vpextrd	$0, %xmm5, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm5, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm5, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm4, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm4, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm4, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm4, %r8d
	addl	%r8d, %edi
	addl	%edi, %ecx
	movl	%esi, %edi
	cmpl	%esi, %r10d
	leaq	(%rax,%rdi,4), %rax
	je	.L2145
.L2150:
	addl	(%rax), %ecx
	cmpl	$1, %edx
	je	.L2145
	addl	4(%rax), %ecx
	cmpl	$2, %edx
	je	.L2145
	addl	8(%rax), %ecx
	cmpl	$3, %edx
	je	.L2145
	addl	12(%rax), %ecx
	cmpl	$4, %edx
	je	.L2145
	addl	16(%rax), %ecx
	cmpl	$5, %edx
	je	.L2145
	addl	20(%rax), %ecx
	cmpl	$6, %edx
	je	.L2145
	addl	24(%rax), %ecx
.L2145:
	vpaddd	%ymm3, %ymm0, %ymm0
	vpaddd	%ymm1, %ymm2, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm6
	vmovdqa	%ymm6, (%rsp)
	vmovdqa	%ymm6, 32(%rsp)
	vmovdqa	(%rsp), %xmm0
	vpaddd	48(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %eax
	addl	%eax, %ecx
	movl	%ecx, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2176:
	movl	%r8d, %edx
	jmp	.L2148
.L2138:
	vmovdqa	%ymm0, %ymm1
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm3
	jmp	.L2145
.L2204:
	testl	%eax, %eax
	jne	.L2205
	xorl	%edi, %edi
	movq	%rbx, %rax
	jmp	.L2147
.L2170:
	movl	%r9d, %edx
	jmp	.L2148
.L2205:
	movl	%eax, %edi
	jmp	.L2146
.L2203:
	vmovdqa	%ymm0, %ymm1
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm3
	jmp	.L2141
simd_v8_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, (%rsp)
	movq	$0, 8(%rsp)
	testb	$31, %bl
	movq	$0, 16(%rsp)
	movq	$0, 24(%rsp)
	movl	%eax, %ecx
	vmovdqa	(%rsp), %ymm0
	je	.L2227
	testl	%eax, %eax
	movl	$0, %eax
	jne	.L2212
	jmp	.L2208
.L2209:
	testl	%ecx, %ecx
	je	.L2208
.L2212:
	addq	$4, %rbx
	addl	-4(%rbx), %eax
	subl	$1, %ecx
	testb	$31, %bl
	jne	.L2209
.L2207:
	cmpl	$63, %ecx
	jbe	.L2273
	vmovdqa	%ymm0, %ymm1
	vmovdqa	%ymm0, %ymm2
	movl	%ecx, %edi
	vmovdqa	%ymm0, %ymm3
	vmovdqa	%ymm0, %ymm4
	movq	%rbx, %rdx
	vmovdqa	%ymm0, %ymm5
	vmovdqa	%ymm0, %ymm6
	vmovdqa	%ymm0, %ymm7
.L2214:
	subl	$64, %edi
	vpaddd	(%rdx), %ymm0, %ymm0
	vpaddd	32(%rdx), %ymm7, %ymm7
	vpaddd	64(%rdx), %ymm6, %ymm6
	vpaddd	96(%rdx), %ymm5, %ymm5
	vpaddd	128(%rdx), %ymm4, %ymm4
	vpaddd	160(%rdx), %ymm3, %ymm3
	vpaddd	192(%rdx), %ymm2, %ymm2
	vpaddd	224(%rdx), %ymm1, %ymm1
	addq	$256, %rdx
	cmpl	$63, %edi
	ja	.L2214
	subl	$64, %ecx
	movl	%ecx, %esi
	shrl	$6, %esi
	movl	%esi, %edx
	sall	$6, %esi
	addq	$1, %rdx
	subl	%esi, %ecx
	salq	$8, %rdx
	addq	%rdx, %rbx
.L2211:
	testl	%ecx, %ecx
	je	.L2215
	movq	%rbx, %rdx
	movl	%ecx, %esi
	movl	%ecx, %edi
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpl	%ecx, %edx
	cmova	%ecx, %edx
	cmpl	$17, %ecx
	ja	.L2274
.L2216:
	addl	(%rbx), %eax
	cmpl	$1, %edi
	leaq	4(%rbx), %rdx
	leal	-1(%rcx), %r8d
	jbe	.L2246
	addl	4(%rbx), %eax
	cmpl	$2, %edi
	leaq	8(%rbx), %rdx
	leal	-2(%rcx), %r8d
	jbe	.L2246
	addl	8(%rbx), %eax
	cmpl	$3, %edi
	leaq	12(%rbx), %rdx
	leal	-3(%rcx), %r8d
	jbe	.L2246
	addl	12(%rbx), %eax
	cmpl	$4, %edi
	leaq	16(%rbx), %rdx
	leal	-4(%rcx), %r8d
	jbe	.L2246
	addl	16(%rbx), %eax
	cmpl	$5, %edi
	leaq	20(%rbx), %rdx
	leal	-5(%rcx), %r8d
	jbe	.L2246
	addl	20(%rbx), %eax
	cmpl	$6, %edi
	leaq	24(%rbx), %rdx
	leal	-6(%rcx), %r8d
	jbe	.L2246
	addl	24(%rbx), %eax
	cmpl	$7, %edi
	leaq	28(%rbx), %rdx
	leal	-7(%rcx), %r8d
	jbe	.L2246
	addl	28(%rbx), %eax
	cmpl	$8, %edi
	leaq	32(%rbx), %rdx
	leal	-8(%rcx), %r8d
	jbe	.L2246
	addl	32(%rbx), %eax
	cmpl	$9, %edi
	leaq	36(%rbx), %rdx
	leal	-9(%rcx), %r8d
	jbe	.L2246
	addl	36(%rbx), %eax
	cmpl	$10, %edi
	leaq	40(%rbx), %rdx
	leal	-10(%rcx), %r9d
	jbe	.L2240
	addl	40(%rbx), %eax
	cmpl	$11, %edi
	leaq	44(%rbx), %rdx
	leal	-11(%rcx), %r8d
	jbe	.L2246
	addl	44(%rbx), %eax
	cmpl	$12, %edi
	leaq	48(%rbx), %rdx
	leal	-12(%rcx), %r8d
	jbe	.L2246
	addl	48(%rbx), %eax
	cmpl	$13, %edi
	leaq	52(%rbx), %rdx
	leal	-13(%rcx), %r8d
	jbe	.L2246
	addl	52(%rbx), %eax
	cmpl	$14, %edi
	leaq	56(%rbx), %rdx
	leal	-14(%rcx), %r8d
	jbe	.L2246
	addl	56(%rbx), %eax
	cmpl	$15, %edi
	leaq	60(%rbx), %rdx
	leal	-15(%rcx), %r8d
	jbe	.L2246
	addl	60(%rbx), %eax
	cmpl	$16, %edi
	leaq	64(%rbx), %rdx
	leal	-16(%rcx), %r8d
	jbe	.L2246
	addl	64(%rbx), %eax
	leaq	68(%rbx), %rdx
	subl	$17, %ecx
.L2218:
	cmpl	%esi, %edi
	je	.L2215
.L2217:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2220
	vpxor	%xmm8, %xmm8, %xmm8
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2226:
	addl	$1, %edi
	vpaddd	(%r8), %ymm8, %ymm8
	addq	$32, %r8
	cmpl	%edi, %r9d
	ja	.L2226
	vmovdqa	%xmm8, %xmm9
	subl	%esi, %ecx
	vextracti128	$0x1, %ymm8, %xmm8
	vpextrd	$1, %xmm9, %r8d
	vpextrd	$0, %xmm9, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm9, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm9, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm8, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm8, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm8, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm8, %r8d
	addl	%r8d, %edi
	addl	%edi, %eax
	movl	%esi, %edi
	cmpl	%r10d, %esi
	leaq	(%rdx,%rdi,4), %rdx
	je	.L2215
.L2220:
	addl	(%rdx), %eax
	cmpl	$1, %ecx
	je	.L2215
	addl	4(%rdx), %eax
	cmpl	$2, %ecx
	je	.L2215
	addl	8(%rdx), %eax
	cmpl	$3, %ecx
	je	.L2215
	addl	12(%rdx), %eax
	cmpl	$4, %ecx
	je	.L2215
	addl	16(%rdx), %eax
	cmpl	$5, %ecx
	je	.L2215
	addl	20(%rdx), %eax
	cmpl	$6, %ecx
	je	.L2215
	addl	24(%rdx), %eax
.L2215:
	vpaddd	%ymm7, %ymm0, %ymm0
	vpaddd	%ymm5, %ymm6, %ymm5
	vpaddd	%ymm3, %ymm4, %ymm4
	vpaddd	%ymm5, %ymm0, %ymm0
	vpaddd	%ymm1, %ymm2, %ymm2
	vpaddd	%ymm4, %ymm0, %ymm3
	vpaddd	%ymm2, %ymm3, %ymm1
	vmovdqa	%ymm1, (%rsp)
	vmovdqa	(%rsp), %xmm0
	vpaddd	16(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	movl	%eax, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2246:
	movl	%r8d, %ecx
	jmp	.L2218
.L2208:
	vmovdqa	%ymm0, %ymm1
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm3
	vmovdqa	%ymm0, %ymm4
	vmovdqa	%ymm0, %ymm5
	vmovdqa	%ymm0, %ymm6
	vmovdqa	%ymm0, %ymm7
	jmp	.L2215
.L2274:
	testl	%edx, %edx
	jne	.L2275
	xorl	%edi, %edi
	movq	%rbx, %rdx
	jmp	.L2217
.L2240:
	movl	%r9d, %ecx
	jmp	.L2218
.L2227:
	xorl	%eax, %eax
	jmp	.L2207
.L2273:
	vmovdqa	%ymm0, %ymm1
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm3
	vmovdqa	%ymm0, %ymm4
	vmovdqa	%ymm0, %ymm5
	vmovdqa	%ymm0, %ymm6
	vmovdqa	%ymm0, %ymm7
	jmp	.L2211
.L2275:
	movl	%edx, %edi
	jmp	.L2216
simd_v12_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, (%rsp)
	movq	$0, 8(%rsp)
	xorl	%ecx, %ecx
	movq	$0, 16(%rsp)
	movq	$0, 24(%rsp)
	testb	$31, %bl
	movl	%eax, %edx
	vmovdqa	(%rsp), %ymm0
	je	.L2278
	testl	%eax, %eax
	jne	.L2283
	jmp	.L2279
.L2280:
	testl	%edx, %edx
	je	.L2279
.L2283:
	addq	$4, %rbx
	addl	-4(%rbx), %ecx
	subl	$1, %edx
	testb	$31, %bl
	jne	.L2280
	movl	%edx, %eax
.L2278:
	cmpl	$95, %eax
	jbe	.L2345
	vmovdqa	%ymm0, %ymm11
	vmovdqa	%ymm0, %ymm10
	movl	%eax, %edx
	vmovdqa	%ymm0, %ymm9
	vmovdqa	%ymm0, %ymm8
	vmovdqa	%ymm0, %ymm7
	vmovdqa	%ymm0, %ymm6
	vmovdqa	%ymm0, %ymm5
	vmovdqa	%ymm0, %ymm4
	vmovdqa	%ymm0, %ymm3
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm1
.L2285:
	subl	$96, %edx
	vpaddd	(%rbx), %ymm0, %ymm0
	vpaddd	32(%rbx), %ymm1, %ymm1
	vpaddd	64(%rbx), %ymm2, %ymm2
	vpaddd	96(%rbx), %ymm3, %ymm3
	vpaddd	128(%rbx), %ymm4, %ymm4
	vpaddd	160(%rbx), %ymm5, %ymm5
	vpaddd	192(%rbx), %ymm6, %ymm6
	vpaddd	224(%rbx), %ymm7, %ymm7
	vpaddd	256(%rbx), %ymm8, %ymm8
	vpaddd	288(%rbx), %ymm9, %ymm9
	vpaddd	320(%rbx), %ymm10, %ymm10
	vpaddd	352(%rbx), %ymm11, %ymm11
	addq	$384, %rbx
	cmpl	$95, %edx
	ja	.L2285
.L2282:
	testl	%edx, %edx
	je	.L2286
	movq	%rbx, %rax
	movl	%edx, %esi
	movl	%edx, %edi
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpl	%edx, %eax
	cmova	%edx, %eax
	cmpl	$17, %edx
	ja	.L2346
.L2287:
	addl	(%rbx), %ecx
	cmpl	$1, %edi
	leaq	4(%rbx), %rax
	leal	-1(%rdx), %r8d
	jbe	.L2316
	addl	4(%rbx), %ecx
	cmpl	$2, %edi
	leaq	8(%rbx), %rax
	leal	-2(%rdx), %r8d
	jbe	.L2316
	addl	8(%rbx), %ecx
	cmpl	$3, %edi
	leaq	12(%rbx), %rax
	leal	-3(%rdx), %r8d
	jbe	.L2316
	addl	12(%rbx), %ecx
	cmpl	$4, %edi
	leaq	16(%rbx), %rax
	leal	-4(%rdx), %r8d
	jbe	.L2316
	addl	16(%rbx), %ecx
	cmpl	$5, %edi
	leaq	20(%rbx), %rax
	leal	-5(%rdx), %r8d
	jbe	.L2316
	addl	20(%rbx), %ecx
	cmpl	$6, %edi
	leaq	24(%rbx), %rax
	leal	-6(%rdx), %r8d
	jbe	.L2316
	addl	24(%rbx), %ecx
	cmpl	$7, %edi
	leaq	28(%rbx), %rax
	leal	-7(%rdx), %r8d
	jbe	.L2316
	addl	28(%rbx), %ecx
	cmpl	$8, %edi
	leaq	32(%rbx), %rax
	leal	-8(%rdx), %r8d
	jbe	.L2316
	addl	32(%rbx), %ecx
	cmpl	$9, %edi
	leaq	36(%rbx), %rax
	leal	-9(%rdx), %r8d
	jbe	.L2316
	addl	36(%rbx), %ecx
	cmpl	$10, %edi
	leaq	40(%rbx), %rax
	leal	-10(%rdx), %r9d
	jbe	.L2310
	addl	40(%rbx), %ecx
	cmpl	$11, %edi
	leaq	44(%rbx), %rax
	leal	-11(%rdx), %r8d
	jbe	.L2316
	addl	44(%rbx), %ecx
	cmpl	$12, %edi
	leaq	48(%rbx), %rax
	leal	-12(%rdx), %r8d
	jbe	.L2316
	addl	48(%rbx), %ecx
	cmpl	$13, %edi
	leaq	52(%rbx), %rax
	leal	-13(%rdx), %r8d
	jbe	.L2316
	addl	52(%rbx), %ecx
	cmpl	$14, %edi
	leaq	56(%rbx), %rax
	leal	-14(%rdx), %r8d
	jbe	.L2316
	addl	56(%rbx), %ecx
	cmpl	$15, %edi
	leaq	60(%rbx), %rax
	leal	-15(%rdx), %r8d
	jbe	.L2316
	addl	60(%rbx), %ecx
	cmpl	$16, %edi
	leaq	64(%rbx), %rax
	leal	-16(%rdx), %r8d
	jbe	.L2316
	addl	64(%rbx), %ecx
	leaq	68(%rbx), %rax
	subl	$17, %edx
.L2289:
	cmpl	%esi, %edi
	je	.L2286
.L2288:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %edi
	testl	%edi, %edi
	je	.L2291
	vpxor	%xmm12, %xmm12, %xmm12
	leaq	(%rbx,%r8,4), %r8
	xorl	%esi, %esi
.L2297:
	addl	$1, %esi
	vpaddd	(%r8), %ymm12, %ymm12
	addq	$32, %r8
	cmpl	%esi, %r9d
	ja	.L2297
	vmovdqa	%xmm12, %xmm13
	subl	%edi, %edx
	vextracti128	$0x1, %ymm12, %xmm12
	vpextrd	$1, %xmm13, %r8d
	vpextrd	$0, %xmm13, %esi
	addl	%r8d, %esi
	vpextrd	$2, %xmm13, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm13, %r8d
	addl	%r8d, %esi
	vpextrd	$0, %xmm12, %r8d
	addl	%r8d, %esi
	vpextrd	$1, %xmm12, %r8d
	addl	%r8d, %esi
	vpextrd	$2, %xmm12, %r8d
	addl	%r8d, %esi
	vpextrd	$3, %xmm12, %r8d
	addl	%r8d, %esi
	addl	%esi, %ecx
	movl	%edi, %esi
	cmpl	%r10d, %edi
	leaq	(%rax,%rsi,4), %rax
	je	.L2286
.L2291:
	addl	(%rax), %ecx
	cmpl	$1, %edx
	je	.L2286
	addl	4(%rax), %ecx
	cmpl	$2, %edx
	je	.L2286
	addl	8(%rax), %ecx
	cmpl	$3, %edx
	je	.L2286
	addl	12(%rax), %ecx
	cmpl	$4, %edx
	je	.L2286
	addl	16(%rax), %ecx
	cmpl	$5, %edx
	je	.L2286
	addl	20(%rax), %ecx
	cmpl	$6, %edx
	je	.L2286
	addl	24(%rax), %ecx
.L2286:
	vpaddd	%ymm1, %ymm0, %ymm0
	vpaddd	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm5, %ymm4, %ymm5
	vpaddd	%ymm2, %ymm0, %ymm0
	vpaddd	%ymm7, %ymm6, %ymm7
	vpaddd	%ymm5, %ymm0, %ymm4
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	%ymm7, %ymm4, %ymm6
	vpaddd	%ymm11, %ymm10, %ymm11
	vpaddd	%ymm9, %ymm6, %ymm8
	vpaddd	%ymm11, %ymm8, %ymm10
	vmovdqa	%ymm10, (%rsp)
	vmovdqa	(%rsp), %xmm0
	vpaddd	16(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %eax
	addl	%eax, %ecx
	movl	%ecx, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2316:
	movl	%r8d, %edx
	jmp	.L2289
.L2279:
	vmovdqa	%ymm0, %ymm11
	vmovdqa	%ymm0, %ymm10
	vmovdqa	%ymm0, %ymm9
	vmovdqa	%ymm0, %ymm8
	vmovdqa	%ymm0, %ymm7
	vmovdqa	%ymm0, %ymm6
	vmovdqa	%ymm0, %ymm5
	vmovdqa	%ymm0, %ymm4
	vmovdqa	%ymm0, %ymm3
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm1
	jmp	.L2286
.L2346:
	testl	%eax, %eax
	jne	.L2347
	xorl	%edi, %edi
	movq	%rbx, %rax
	jmp	.L2288
.L2310:
	movl	%r9d, %edx
	jmp	.L2289
.L2347:
	movl	%eax, %edi
	jmp	.L2287
.L2345:
	vmovdqa	%ymm0, %ymm11
	vmovdqa	%ymm0, %ymm10
	vmovdqa	%ymm0, %ymm9
	vmovdqa	%ymm0, %ymm8
	vmovdqa	%ymm0, %ymm7
	vmovdqa	%ymm0, %ymm6
	vmovdqa	%ymm0, %ymm5
	vmovdqa	%ymm0, %ymm4
	vmovdqa	%ymm0, %ymm3
	vmovdqa	%ymm0, %ymm2
	vmovdqa	%ymm0, %ymm1
	jmp	.L2282
simd_v2a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$64, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, 32(%rsp)
	movq	$0, 40(%rsp)
	testb	$31, %bl
	movq	$0, 48(%rsp)
	movq	$0, 56(%rsp)
	movl	%eax, %edx
	vmovdqa	32(%rsp), %ymm4
	vmovdqa	%ymm4, (%rsp)
	je	.L2369
	testl	%eax, %eax
	movl	$0, %eax
	jne	.L2354
	jmp	.L2357
.L2351:
	testl	%edx, %edx
	je	.L2357
.L2354:
	addq	$4, %rbx
	addl	-4(%rbx), %eax
	subl	$1, %edx
	testb	$31, %bl
	jne	.L2351
.L2349:
	cmpl	$15, %edx
	jbe	.L2353
	movl	%edx, %edi
	movq	%rbx, %rcx
.L2356:
	vmovdqa	(%rsp), %ymm2
	subl	$16, %edi
	addq	$64, %rcx
	vpaddd	-64(%rcx), %ymm2, %ymm0
	vpaddd	-32(%rcx), %ymm0, %ymm3
	cmpl	$15, %edi
	vmovdqa	%ymm3, (%rsp)
	ja	.L2356
	subl	$16, %edx
	movl	%edx, %esi
	shrl	$4, %esi
	movl	%esi, %ecx
	sall	$4, %esi
	addq	$1, %rcx
	subl	%esi, %edx
	salq	$6, %rcx
	addq	%rcx, %rbx
.L2353:
	testl	%edx, %edx
	je	.L2357
	movq	%rbx, %rcx
	movl	%edx, %esi
	movl	%edx, %edi
	andl	$31, %ecx
	shrq	$2, %rcx
	negq	%rcx
	andl	$7, %ecx
	cmpl	%edx, %ecx
	cmova	%edx, %ecx
	cmpl	$17, %edx
	ja	.L2415
.L2358:
	addl	(%rbx), %eax
	cmpl	$1, %edi
	leaq	4(%rbx), %rcx
	leal	-1(%rdx), %r8d
	jbe	.L2388
	addl	4(%rbx), %eax
	cmpl	$2, %edi
	leaq	8(%rbx), %rcx
	leal	-2(%rdx), %r8d
	jbe	.L2388
	addl	8(%rbx), %eax
	cmpl	$3, %edi
	leaq	12(%rbx), %rcx
	leal	-3(%rdx), %r8d
	jbe	.L2388
	addl	12(%rbx), %eax
	cmpl	$4, %edi
	leaq	16(%rbx), %rcx
	leal	-4(%rdx), %r8d
	jbe	.L2388
	addl	16(%rbx), %eax
	cmpl	$5, %edi
	leaq	20(%rbx), %rcx
	leal	-5(%rdx), %r8d
	jbe	.L2388
	addl	20(%rbx), %eax
	cmpl	$6, %edi
	leaq	24(%rbx), %rcx
	leal	-6(%rdx), %r8d
	jbe	.L2388
	addl	24(%rbx), %eax
	cmpl	$7, %edi
	leaq	28(%rbx), %rcx
	leal	-7(%rdx), %r8d
	jbe	.L2388
	addl	28(%rbx), %eax
	cmpl	$8, %edi
	leaq	32(%rbx), %rcx
	leal	-8(%rdx), %r8d
	jbe	.L2388
	addl	32(%rbx), %eax
	cmpl	$9, %edi
	leaq	36(%rbx), %rcx
	leal	-9(%rdx), %r8d
	jbe	.L2388
	addl	36(%rbx), %eax
	cmpl	$10, %edi
	leaq	40(%rbx), %rcx
	leal	-10(%rdx), %r9d
	jbe	.L2382
	addl	40(%rbx), %eax
	cmpl	$11, %edi
	leaq	44(%rbx), %rcx
	leal	-11(%rdx), %r8d
	jbe	.L2388
	addl	44(%rbx), %eax
	cmpl	$12, %edi
	leaq	48(%rbx), %rcx
	leal	-12(%rdx), %r8d
	jbe	.L2388
	addl	48(%rbx), %eax
	cmpl	$13, %edi
	leaq	52(%rbx), %rcx
	leal	-13(%rdx), %r8d
	jbe	.L2388
	addl	52(%rbx), %eax
	cmpl	$14, %edi
	leaq	56(%rbx), %rcx
	leal	-14(%rdx), %r8d
	jbe	.L2388
	addl	56(%rbx), %eax
	cmpl	$15, %edi
	leaq	60(%rbx), %rcx
	leal	-15(%rdx), %r8d
	jbe	.L2388
	addl	60(%rbx), %eax
	cmpl	$16, %edi
	leaq	64(%rbx), %rcx
	leal	-16(%rdx), %r8d
	jbe	.L2388
	addl	64(%rbx), %eax
	leaq	68(%rbx), %rcx
	subl	$17, %edx
.L2360:
	cmpl	%edi, %esi
	je	.L2357
.L2359:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2362
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2368:
	addl	$1, %edi
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpl	%r9d, %edi
	jb	.L2368
	vmovdqa	%xmm0, %xmm1
	subl	%esi, %edx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %edi
	addl	%edi, %eax
	movl	%esi, %edi
	cmpl	%esi, %r10d
	leaq	(%rcx,%rdi,4), %rcx
	je	.L2357
.L2362:
	addl	(%rcx), %eax
	cmpl	$1, %edx
	je	.L2357
	addl	4(%rcx), %eax
	cmpl	$2, %edx
	je	.L2357
	addl	8(%rcx), %eax
	cmpl	$3, %edx
	je	.L2357
	addl	12(%rcx), %eax
	cmpl	$4, %edx
	je	.L2357
	addl	16(%rcx), %eax
	cmpl	$5, %edx
	je	.L2357
	addl	20(%rcx), %eax
	cmpl	$6, %edx
	je	.L2357
	addl	24(%rcx), %eax
.L2357:
	vmovdqa	(%rsp), %ymm5
	vmovdqa	(%rsp), %xmm0
	vmovdqa	%ymm5, 32(%rsp)
	vpaddd	48(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	movl	%eax, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2388:
	movl	%r8d, %edx
	jmp	.L2360
.L2415:
	testl	%ecx, %ecx
	jne	.L2416
	xorl	%edi, %edi
	movq	%rbx, %rcx
	jmp	.L2359
.L2382:
	movl	%r9d, %edx
	jmp	.L2360
.L2369:
	xorl	%eax, %eax
	jmp	.L2349
.L2416:
	movl	%ecx, %edi
	jmp	.L2358
simd_v4a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, (%rsp)
	movq	$0, 8(%rsp)
	xorl	%ecx, %ecx
	movq	$0, 16(%rsp)
	movq	$0, 24(%rsp)
	testb	$31, %bl
	movl	%eax, %edx
	vmovdqa	(%rsp), %ymm0
	je	.L2418
	testl	%eax, %eax
	jne	.L2423
	jmp	.L2426
.L2420:
	testl	%edx, %edx
	je	.L2426
.L2423:
	addq	$4, %rbx
	addl	-4(%rbx), %ecx
	subl	$1, %edx
	testb	$31, %bl
	jne	.L2420
.L2418:
	cmpl	$31, %edx
	jbe	.L2422
	movl	%edx, %edi
	movq	%rbx, %rax
.L2425:
	vmovdqa	(%rax), %ymm1
	subl	$32, %edi
	subq	$-128, %rax
	vpaddd	-96(%rax), %ymm1, %ymm1
	vpaddd	-64(%rax), %ymm1, %ymm1
	vpaddd	-32(%rax), %ymm1, %ymm1
	cmpl	$31, %edi
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L2425
	subl	$32, %edx
	movl	%edx, %esi
	shrl	$5, %esi
	movl	%esi, %eax
	sall	$5, %esi
	addq	$1, %rax
	subl	%esi, %edx
	salq	$7, %rax
	addq	%rax, %rbx
.L2422:
	testl	%edx, %edx
	je	.L2426
	movq	%rbx, %rax
	movl	%edx, %esi
	movl	%edx, %edi
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpl	%edx, %eax
	cmova	%edx, %eax
	cmpl	$17, %edx
	ja	.L2484
.L2427:
	addl	(%rbx), %ecx
	cmpl	$1, %edi
	leaq	4(%rbx), %rax
	leal	-1(%rdx), %r8d
	jbe	.L2457
	addl	4(%rbx), %ecx
	cmpl	$2, %edi
	leaq	8(%rbx), %rax
	leal	-2(%rdx), %r8d
	jbe	.L2457
	addl	8(%rbx), %ecx
	cmpl	$3, %edi
	leaq	12(%rbx), %rax
	leal	-3(%rdx), %r8d
	jbe	.L2457
	addl	12(%rbx), %ecx
	cmpl	$4, %edi
	leaq	16(%rbx), %rax
	leal	-4(%rdx), %r8d
	jbe	.L2457
	addl	16(%rbx), %ecx
	cmpl	$5, %edi
	leaq	20(%rbx), %rax
	leal	-5(%rdx), %r8d
	jbe	.L2457
	addl	20(%rbx), %ecx
	cmpl	$6, %edi
	leaq	24(%rbx), %rax
	leal	-6(%rdx), %r8d
	jbe	.L2457
	addl	24(%rbx), %ecx
	cmpl	$7, %edi
	leaq	28(%rbx), %rax
	leal	-7(%rdx), %r8d
	jbe	.L2457
	addl	28(%rbx), %ecx
	cmpl	$8, %edi
	leaq	32(%rbx), %rax
	leal	-8(%rdx), %r8d
	jbe	.L2457
	addl	32(%rbx), %ecx
	cmpl	$9, %edi
	leaq	36(%rbx), %rax
	leal	-9(%rdx), %r8d
	jbe	.L2457
	addl	36(%rbx), %ecx
	cmpl	$10, %edi
	leaq	40(%rbx), %rax
	leal	-10(%rdx), %r9d
	jbe	.L2451
	addl	40(%rbx), %ecx
	cmpl	$11, %edi
	leaq	44(%rbx), %rax
	leal	-11(%rdx), %r8d
	jbe	.L2457
	addl	44(%rbx), %ecx
	cmpl	$12, %edi
	leaq	48(%rbx), %rax
	leal	-12(%rdx), %r8d
	jbe	.L2457
	addl	48(%rbx), %ecx
	cmpl	$13, %edi
	leaq	52(%rbx), %rax
	leal	-13(%rdx), %r8d
	jbe	.L2457
	addl	52(%rbx), %ecx
	cmpl	$14, %edi
	leaq	56(%rbx), %rax
	leal	-14(%rdx), %r8d
	jbe	.L2457
	addl	56(%rbx), %ecx
	cmpl	$15, %edi
	leaq	60(%rbx), %rax
	leal	-15(%rdx), %r8d
	jbe	.L2457
	addl	60(%rbx), %ecx
	cmpl	$16, %edi
	leaq	64(%rbx), %rax
	leal	-16(%rdx), %r8d
	jbe	.L2457
	addl	64(%rbx), %ecx
	leaq	68(%rbx), %rax
	subl	$17, %edx
.L2429:
	cmpl	%esi, %edi
	je	.L2426
.L2428:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2431
	vpxor	%xmm1, %xmm1, %xmm1
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2437:
	addl	$1, %edi
	vpaddd	(%r8), %ymm1, %ymm1
	addq	$32, %r8
	cmpl	%r9d, %edi
	jb	.L2437
	vmovdqa	%xmm1, %xmm2
	subl	%esi, %edx
	vextracti128	$0x1, %ymm1, %xmm1
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$0, %xmm2, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edi
	addl	%edi, %ecx
	movl	%esi, %edi
	cmpl	%esi, %r10d
	leaq	(%rax,%rdi,4), %rax
	je	.L2426
.L2431:
	addl	(%rax), %ecx
	cmpl	$1, %edx
	je	.L2426
	addl	4(%rax), %ecx
	cmpl	$2, %edx
	je	.L2426
	addl	8(%rax), %ecx
	cmpl	$3, %edx
	je	.L2426
	addl	12(%rax), %ecx
	cmpl	$4, %edx
	je	.L2426
	addl	16(%rax), %ecx
	cmpl	$5, %edx
	je	.L2426
	addl	20(%rax), %ecx
	cmpl	$6, %edx
	je	.L2426
	addl	24(%rax), %ecx
.L2426:
	vmovdqa	%ymm0, (%rsp)
	vmovdqa	(%rsp), %xmm0
	vpaddd	16(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %eax
	addl	%eax, %ecx
	movl	%ecx, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2457:
	movl	%r8d, %edx
	jmp	.L2429
.L2484:
	testl	%eax, %eax
	jne	.L2485
	xorl	%edi, %edi
	movq	%rbx, %rax
	jmp	.L2428
.L2451:
	movl	%r9d, %edx
	jmp	.L2429
.L2485:
	movl	%eax, %edi
	jmp	.L2427
simd_v8a_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r13
	movq	%rsi, %r13
	pushq	%r12
	movq	%rdi, %r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	call	get_vec_start
	movq	%r12, %rdi
	movq	%rax, %rbx
	call	vec_length
	movq	$0, (%rsp)
	movq	$0, 8(%rsp)
	testb	$31, %bl
	movq	$0, 16(%rsp)
	movq	$0, 24(%rsp)
	movl	%eax, %ecx
	vmovdqa	(%rsp), %ymm0
	je	.L2507
	testl	%eax, %eax
	movl	$0, %eax
	jne	.L2492
	jmp	.L2495
.L2489:
	testl	%ecx, %ecx
	je	.L2495
.L2492:
	addq	$4, %rbx
	addl	-4(%rbx), %eax
	subl	$1, %ecx
	testb	$31, %bl
	jne	.L2489
.L2487:
	cmpl	$63, %ecx
	jbe	.L2491
	movl	%ecx, %edi
	movq	%rbx, %rdx
.L2494:
	vmovdqa	(%rdx), %ymm2
	subl	$64, %edi
	addq	$256, %rdx
	vmovdqa	-128(%rdx), %ymm1
	vpaddd	-224(%rdx), %ymm2, %ymm2
	vpaddd	-192(%rdx), %ymm2, %ymm2
	vpaddd	-160(%rdx), %ymm2, %ymm2
	vpaddd	-96(%rdx), %ymm1, %ymm1
	vpaddd	-64(%rdx), %ymm1, %ymm1
	vpaddd	-32(%rdx), %ymm1, %ymm1
	cmpl	$63, %edi
	vpaddd	%ymm1, %ymm2, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	ja	.L2494
	subl	$64, %ecx
	movl	%ecx, %esi
	shrl	$6, %esi
	movl	%esi, %edx
	sall	$6, %esi
	addq	$1, %rdx
	subl	%esi, %ecx
	salq	$8, %rdx
	addq	%rdx, %rbx
.L2491:
	testl	%ecx, %ecx
	je	.L2495
	movq	%rbx, %rdx
	movl	%ecx, %esi
	movl	%ecx, %edi
	andl	$31, %edx
	shrq	$2, %rdx
	negq	%rdx
	andl	$7, %edx
	cmpl	%ecx, %edx
	cmova	%ecx, %edx
	cmpl	$17, %ecx
	ja	.L2553
.L2496:
	addl	(%rbx), %eax
	cmpl	$1, %edi
	leaq	4(%rbx), %rdx
	leal	-1(%rcx), %r8d
	jbe	.L2526
	addl	4(%rbx), %eax
	cmpl	$2, %edi
	leaq	8(%rbx), %rdx
	leal	-2(%rcx), %r8d
	jbe	.L2526
	addl	8(%rbx), %eax
	cmpl	$3, %edi
	leaq	12(%rbx), %rdx
	leal	-3(%rcx), %r8d
	jbe	.L2526
	addl	12(%rbx), %eax
	cmpl	$4, %edi
	leaq	16(%rbx), %rdx
	leal	-4(%rcx), %r8d
	jbe	.L2526
	addl	16(%rbx), %eax
	cmpl	$5, %edi
	leaq	20(%rbx), %rdx
	leal	-5(%rcx), %r8d
	jbe	.L2526
	addl	20(%rbx), %eax
	cmpl	$6, %edi
	leaq	24(%rbx), %rdx
	leal	-6(%rcx), %r8d
	jbe	.L2526
	addl	24(%rbx), %eax
	cmpl	$7, %edi
	leaq	28(%rbx), %rdx
	leal	-7(%rcx), %r8d
	jbe	.L2526
	addl	28(%rbx), %eax
	cmpl	$8, %edi
	leaq	32(%rbx), %rdx
	leal	-8(%rcx), %r8d
	jbe	.L2526
	addl	32(%rbx), %eax
	cmpl	$9, %edi
	leaq	36(%rbx), %rdx
	leal	-9(%rcx), %r8d
	jbe	.L2526
	addl	36(%rbx), %eax
	cmpl	$10, %edi
	leaq	40(%rbx), %rdx
	leal	-10(%rcx), %r9d
	jbe	.L2520
	addl	40(%rbx), %eax
	cmpl	$11, %edi
	leaq	44(%rbx), %rdx
	leal	-11(%rcx), %r8d
	jbe	.L2526
	addl	44(%rbx), %eax
	cmpl	$12, %edi
	leaq	48(%rbx), %rdx
	leal	-12(%rcx), %r8d
	jbe	.L2526
	addl	48(%rbx), %eax
	cmpl	$13, %edi
	leaq	52(%rbx), %rdx
	leal	-13(%rcx), %r8d
	jbe	.L2526
	addl	52(%rbx), %eax
	cmpl	$14, %edi
	leaq	56(%rbx), %rdx
	leal	-14(%rcx), %r8d
	jbe	.L2526
	addl	56(%rbx), %eax
	cmpl	$15, %edi
	leaq	60(%rbx), %rdx
	leal	-15(%rcx), %r8d
	jbe	.L2526
	addl	60(%rbx), %eax
	cmpl	$16, %edi
	leaq	64(%rbx), %rdx
	leal	-16(%rcx), %r8d
	jbe	.L2526
	addl	64(%rbx), %eax
	leaq	68(%rbx), %rdx
	subl	$17, %ecx
.L2498:
	cmpl	%esi, %edi
	je	.L2495
.L2497:
	subl	%edi, %esi
	movl	%edi, %r8d
	movl	%esi, %r9d
	movl	%esi, %r10d
	shrl	$3, %r9d
	leal	0(,%r9,8), %esi
	testl	%esi, %esi
	je	.L2500
	vpxor	%xmm1, %xmm1, %xmm1
	leaq	(%rbx,%r8,4), %r8
	xorl	%edi, %edi
.L2506:
	addl	$1, %edi
	vpaddd	(%r8), %ymm1, %ymm1
	addq	$32, %r8
	cmpl	%edi, %r9d
	ja	.L2506
	vmovdqa	%xmm1, %xmm2
	subl	%esi, %ecx
	vextracti128	$0x1, %ymm1, %xmm1
	vpextrd	$1, %xmm2, %r8d
	vpextrd	$0, %xmm2, %edi
	addl	%r8d, %edi
	vpextrd	$2, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm2, %r8d
	addl	%r8d, %edi
	vpextrd	$0, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$1, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %edi
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %edi
	addl	%edi, %eax
	movl	%esi, %edi
	cmpl	%r10d, %esi
	leaq	(%rdx,%rdi,4), %rdx
	je	.L2495
.L2500:
	addl	(%rdx), %eax
	cmpl	$1, %ecx
	je	.L2495
	addl	4(%rdx), %eax
	cmpl	$2, %ecx
	je	.L2495
	addl	8(%rdx), %eax
	cmpl	$3, %ecx
	je	.L2495
	addl	12(%rdx), %eax
	cmpl	$4, %ecx
	je	.L2495
	addl	16(%rdx), %eax
	cmpl	$5, %ecx
	je	.L2495
	addl	20(%rdx), %eax
	cmpl	$6, %ecx
	je	.L2495
	addl	24(%rdx), %eax
.L2495:
	vmovdqa	%ymm0, (%rsp)
	vmovdqa	(%rsp), %xmm0
	vpaddd	16(%rsp), %xmm0, %xmm0
	vpsrldq	$8, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpsrldq	$4, %xmm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpextrd	$0, %xmm0, %edx
	addl	%edx, %eax
	movl	%eax, 0(%r13)
	vzeroupper
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%rbp
	ret

.L2526:
	movl	%r8d, %ecx
	jmp	.L2498
.L2553:
	testl	%edx, %edx
	jne	.L2554
	xorl	%edi, %edi
	movq	%rbx, %rdx
	jmp	.L2497
.L2520:
	movl	%r9d, %ecx
	jmp	.L2498
.L2507:
	xorl	%eax, %eax
	jmp	.L2487
.L2554:
	movl	%edx, %edi
	jmp	.L2496
unroll4x2as_combine:
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	movq	%rsi, %r14
	pushq	%r13
	movq	%rdi, %r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$32, %rsp
	call	vec_length
	movq	%rax, %rbx
	movq	%r13, %rdi
	movq	%rax, %r12
	shrq	$63, %rbx
	addq	%rax, %rbx
	sarq	%rbx
	call	get_vec_start
	leaq	0(,%rbx,4), %r10
	testq	%rbx, %rbx
	movq	%rax, %rcx
	leaq	(%rax,%r10), %r8
	jle	.L2580
	andl	$31, %eax
	movq	%rbx, %r15
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpq	%rax, %rbx
	cmovbe	%rbx, %rax
	cmpq	$17, %rbx
	ja	.L2630
.L2557:
	cmpq	$1, %r15
	movl	(%rcx), %edi
	movl	(%r8), %edx
	jbe	.L2583
	addl	4(%rcx), %edi
	addl	4(%r8), %edx
	cmpq	$2, %r15
	jbe	.L2584
	addl	8(%rcx), %edi
	addl	8(%r8), %edx
	cmpq	$3, %r15
	jbe	.L2585
	addl	12(%rcx), %edi
	addl	12(%r8), %edx
	cmpq	$4, %r15
	jbe	.L2586
	addl	16(%rcx), %edi
	addl	16(%r8), %edx
	cmpq	$5, %r15
	jbe	.L2587
	addl	20(%rcx), %edi
	addl	20(%r8), %edx
	cmpq	$6, %r15
	jbe	.L2588
	addl	24(%rcx), %edi
	addl	24(%r8), %edx
	cmpq	$7, %r15
	jbe	.L2589
	addl	28(%rcx), %edi
	addl	28(%r8), %edx
	cmpq	$8, %r15
	jbe	.L2590
	addl	32(%rcx), %edi
	addl	32(%r8), %edx
	cmpq	$9, %r15
	jbe	.L2591
	movl	36(%r8), %eax
	addl	36(%rcx), %edi
	addl	%edx, %eax
	cmpq	$10, %r15
	movl	%eax, %edx
	jbe	.L2592
	addl	40(%r8), %eax
	addl	40(%rcx), %edi
	cmpq	$11, %r15
	movl	%eax, %edx
	jbe	.L2593
	addl	44(%rcx), %edi
	addl	44(%r8), %edx
	cmpq	$12, %r15
	jbe	.L2594
	addl	48(%rcx), %edi
	addl	48(%r8), %edx
	cmpq	$13, %r15
	jbe	.L2595
	addl	52(%rcx), %edi
	addl	52(%r8), %edx
	cmpq	$14, %r15
	jbe	.L2596
	addl	56(%rcx), %edi
	addl	56(%r8), %edx
	cmpq	$15, %r15
	jbe	.L2597
	addl	60(%rcx), %edi
	addl	60(%r8), %edx
	cmpq	$16, %r15
	jbe	.L2598
	addl	64(%rcx), %edi
	addl	64(%r8), %edx
	movl	$17, %eax
.L2559:
	cmpq	%r15, %rbx
	je	.L2556
.L2558:
	movq	%rbx, %rsi
	subq	%r15, %rsi
	movq	%rsi, 24(%rsp)
	shrq	$3, %rsi
	leaq	0(,%rsi,8), %r13
	testq	%r13, %r13
	je	.L2561
	salq	$2, %r15
	vpxor	%xmm1, %xmm1, %xmm1
	leaq	(%rcx,%r15), %r11
	movq	%rax, 8(%rsp)
	addq	%r8, %r15
	xorl	%r9d, %r9d
	movq	%r11, 16(%rsp)
	movq	16(%rsp), %rax
	vmovdqa	%ymm1, %ymm0
	xorl	%r11d, %r11d
.L2567:
	vmovdqu	(%r15,%r9), %xmm2
	addq	$1, %r11
	vpaddd	(%rax,%r9), %ymm0, %ymm0
	vinserti128	$0x1, 16(%r15,%r9), %ymm2, %ymm2
	addq	$32, %r9
	cmpq	%rsi, %r11
	vpaddd	%ymm2, %ymm1, %ymm1
	jb	.L2567
	vmovdqa	%xmm1, %xmm2
	movq	8(%rsp), %rax
	vextracti128	$0x1, %ymm1, %xmm1
	vpextrd	$1, %xmm2, %r9d
	vpextrd	$0, %xmm2, %esi
	addq	%r13, %rax
	addl	%r9d, %esi
	vpextrd	$2, %xmm2, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm2, %r9d
	addl	%r9d, %esi
	vpextrd	$0, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$1, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$2, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm1, %r9d
	vmovdqa	%xmm0, %xmm1
	vextracti128	$0x1, %ymm0, %xmm0
	addl	%r9d, %esi
	vpextrd	$1, %xmm1, %r9d
	addl	%esi, %edx
	vpextrd	$0, %xmm1, %esi
	addl	%r9d, %esi
	vpextrd	$2, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm1, %r9d
	addl	%r9d, %esi
	vpextrd	$0, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$1, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$2, %xmm0, %r9d
	addl	%r9d, %esi
	vpextrd	$3, %xmm0, %r9d
	addl	%r9d, %esi
	addl	%esi, %edi
	cmpq	%r13, 24(%rsp)
	je	.L2556
.L2561:
	leaq	1(%rax), %rsi
	addl	(%rcx,%rax,4), %edi
	addl	(%r8,%rax,4), %edx
	cmpq	%rsi, %rbx
	jle	.L2556
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	2(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L2556
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	3(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L2556
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	4(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L2556
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	leaq	5(%rax), %rsi
	cmpq	%rsi, %rbx
	jle	.L2556
	addq	$6, %rax
	addl	(%rcx,%rsi,4), %edi
	addl	(%r8,%rsi,4), %edx
	cmpq	%rax, %rbx
	jle	.L2556
	addl	(%rcx,%rax,4), %edi
	addl	(%r8,%rax,4), %edx
.L2556:
	addq	%rbx, %rbx
	cmpq	%rbx, %r12
	jle	.L2568
	addq	%r10, %r8
	movq	%r12, %rsi
	movq	%r8, %rax
	subq	%rbx, %rsi
	andl	$31, %eax
	shrq	$2, %rax
	negq	%rax
	andl	$7, %eax
	cmpq	%rsi, %rax
	cmova	%rsi, %rax
	cmpq	$17, %rsi
	cmovbe	%rsi, %rax
	testq	%rax, %rax
	je	.L2570
	addl	(%rcx,%rbx,4), %edx
	cmpq	$1, %rax
	leaq	1(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$2, %rax
	leaq	2(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$3, %rax
	leaq	3(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$4, %rax
	leaq	4(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$5, %rax
	leaq	5(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$6, %rax
	leaq	6(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$7, %rax
	leaq	7(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$8, %rax
	leaq	8(%rbx), %r10
	jbe	.L2607
	addl	(%rcx,%r10,4), %edx
	cmpq	$9, %rax
	leaq	9(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$10, %rax
	leaq	10(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$11, %rax
	leaq	11(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$12, %rax
	leaq	12(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$13, %rax
	leaq	13(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$14, %rax
	leaq	14(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$15, %rax
	leaq	15(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	cmpq	$16, %rax
	leaq	16(%rbx), %r9
	jbe	.L2615
	addl	(%rcx,%r9,4), %edx
	addq	$17, %rbx
.L2571:
	cmpq	%rsi, %rax
	je	.L2568
.L2570:
	subq	%rax, %rsi
	movq	%rsi, %r10
	shrq	$3, %r10
	leaq	0(,%r10,8), %r9
	testq	%r9, %r9
	je	.L2573
	vpxor	%xmm0, %xmm0, %xmm0
	leaq	(%r8,%rax,4), %r8
	xorl	%eax, %eax
.L2579:
	addq	$1, %rax
	vpaddd	(%r8), %ymm0, %ymm0
	addq	$32, %r8
	cmpq	%rax, %r10
	ja	.L2579
	vmovdqa	%xmm0, %xmm1
	addq	%r9, %rbx
	vextracti128	$0x1, %ymm0, %xmm0
	vpextrd	$1, %xmm1, %r8d
	vpextrd	$0, %xmm1, %eax
	addl	%r8d, %eax
	vpextrd	$2, %xmm1, %r8d
	addl	%r8d, %eax
	vpextrd	$3, %xmm1, %r8d
	addl	%r8d, %eax
	vpextrd	$0, %xmm0, %r8d
	addl	%r8d, %eax
	vpextrd	$1, %xmm0, %r8d
	addl	%r8d, %eax
	vpextrd	$2, %xmm0, %r8d
	addl	%r8d, %eax
	vpextrd	$3, %xmm0, %r8d
	addl	%r8d, %eax
	addl	%eax, %edx
	cmpq	%r9, %rsi
	je	.L2568
.L2573:
	leaq	1(%rbx), %rax
	addl	(%rcx,%rbx,4), %edx
	cmpq	%rax, %r12
	jle	.L2568
	addl	(%rcx,%rax,4), %edx
	leaq	2(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L2568
	addl	(%rcx,%rax,4), %edx
	leaq	3(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L2568
	addl	(%rcx,%rax,4), %edx
	leaq	4(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L2568
	addl	(%rcx,%rax,4), %edx
	leaq	5(%rbx), %rax
	cmpq	%rax, %r12
	jle	.L2568
	addq	$6, %rbx
	addl	(%rcx,%rax,4), %edx
	cmpq	%rbx, %r12
	jle	.L2568
	addl	(%rcx,%rbx,4), %edx
.L2568:
	addl	%edi, %edx
	movl	%edx, (%r14)
	vzeroupper
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	ret

.L2615:
	movq	%r9, %rbx
	jmp	.L2571
.L2630:
	testq	%rax, %rax
	jne	.L2631
	xorl	%r15d, %r15d
	xorl	%edx, %edx
	xorl	%edi, %edi
	xorl	%eax, %eax
	jmp	.L2558
.L2580:
	xorl	%edx, %edx
	xorl	%edi, %edi
	jmp	.L2556
.L2590:
	movl	$8, %eax
	jmp	.L2559
.L2591:
	movl	$9, %eax
	jmp	.L2559
.L2589:
	movl	$7, %eax
	jmp	.L2559
.L2598:
	movl	$16, %eax
	jmp	.L2559
.L2607:
	movq	%r10, %rbx
	jmp	.L2571
.L2588:
	movl	$6, %eax
	jmp	.L2559
.L2583:
	movl	$1, %eax
	jmp	.L2559
.L2584:
	movl	$2, %eax
	jmp	.L2559
.L2597:
	movl	$15, %eax
	jmp	.L2559
.L2592:
	movl	$10, %eax
	jmp	.L2559
.L2593:
	movl	$11, %eax
	jmp	.L2559
.L2594:
	movl	$12, %eax
	jmp	.L2559
.L2595:
	movl	$13, %eax
	jmp	.L2559
.L2596:
	movl	$14, %eax
	jmp	.L2559
.L2585:
	movl	$3, %eax
	jmp	.L2559
.L2586:
	movl	$4, %eax
	jmp	.L2559
.L2587:
	movl	$5, %eax
	jmp	.L2559
.L2631:
	movq	%rax, %r15
	jmp	.L2557
register_combiners:
	movl	$combine1, %esi
	subq	$8, %rsp
	movl	$combine1_descr, %edx
	movq	%rsi, %rdi
	call	add_combiner
	movl	$combine2_descr, %edx
	movl	$combine1, %esi
	movl	$combine2, %edi
	call	add_combiner
	movl	$combine3_descr, %edx
	movl	$combine1, %esi
	movl	$combine3, %edi
	call	add_combiner
	movl	$combine3w_descr, %edx
	movl	$combine1, %esi
	movl	$combine3w, %edi
	call	add_combiner
	movl	$combine4_descr, %edx
	movl	$combine1, %esi
	movl	$combine4, %edi
	call	add_combiner
	movl	$combine4b_descr, %edx
	movl	$combine1, %esi
	movl	$combine4b, %edi
	call	add_combiner
	movl	$combine4p_descr, %edx
	movl	$combine1, %esi
	movl	$combine4p, %edi
	call	add_combiner
	movl	$combine5_descr, %edx
	movl	$combine1, %esi
	movl	$combine5, %edi
	call	add_combiner
	movl	$combine5p_descr, %edx
	movl	$combine1, %esi
	movl	$combine5p, %edi
	call	add_combiner
	movl	$unroll2aw_descr, %edx
	movl	$combine1, %esi
	movl	$unroll2aw_combine, %edi
	call	add_combiner
	movl	$unroll3a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll3a_combine, %edi
	call	add_combiner
	movl	$unroll4a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll4a_combine, %edi
	call	add_combiner
	movl	$unroll5a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll5a_combine, %edi
	call	add_combiner
	movl	$unroll6a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll6a_combine, %edi
	call	add_combiner
	movl	$unroll7a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll7a_combine, %edi
	call	add_combiner
	movl	$unroll8a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8a_combine, %edi
	call	add_combiner
	movl	$unroll9a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll9a_combine, %edi
	call	add_combiner
	movl	$unroll10a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll10a_combine, %edi
	call	add_combiner
	movl	$unroll16a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll16a_combine, %edi
	call	add_combiner
	movl	$unroll2_descr, %edx
	movl	$combine1, %esi
	movl	$unroll2_combine, %edi
	call	add_combiner
	movl	$unroll3_descr, %edx
	movl	$combine1, %esi
	movl	$unroll3_combine, %edi
	call	add_combiner
	movl	$unroll4_descr, %edx
	movl	$combine1, %esi
	movl	$unroll4_combine, %edi
	call	add_combiner
	movl	$unroll8_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8_combine, %edi
	call	add_combiner
	movl	$unroll16_descr, %edx
	movl	$combine1, %esi
	movl	$unroll16_combine, %edi
	call	add_combiner
	movl	$combine6_descr, %edx
	movl	$combine1, %esi
	movl	$combine6, %edi
	call	add_combiner
	movl	$unroll4x2a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll4x2a_combine, %edi
	call	add_combiner
	movl	$unroll8x2a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8x2a_combine, %edi
	call	add_combiner
	movl	$unroll3x3a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll3x3a_combine, %edi
	call	add_combiner
	movl	$unroll4x4a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll4x4a_combine, %edi
	call	add_combiner
	movl	$unroll5x5a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll5x5a_combine, %edi
	call	add_combiner
	movl	$unroll6x6a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll6x6a_combine, %edi
	call	add_combiner
	movl	$unroll7x7a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll7x7a_combine, %edi
	call	add_combiner
	movl	$unroll8x4a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8x4a_combine, %edi
	call	add_combiner
	movl	$unroll8x8a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8x8a_combine, %edi
	call	add_combiner
	movl	$unroll9x9a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll9x9a_combine, %edi
	call	add_combiner
	movl	$unroll10x10a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll10x10a_combine, %edi
	call	add_combiner
	movl	$unroll12x6a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll12x6a_combine, %edi
	call	add_combiner
	movl	$unroll12x12a_descr, %edx
	movl	$combine1, %esi
	movl	$unroll12x12a_combine, %edi
	call	add_combiner
	movl	$unroll8x2_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8x2_combine, %edi
	call	add_combiner
	movl	$unroll8x4_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8x4_combine, %edi
	call	add_combiner
	movl	$unroll8x8_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8x8_combine, %edi
	call	add_combiner
	movl	$unroll9x3_descr, %edx
	movl	$combine1, %esi
	movl	$unroll9x3_combine, %edi
	call	add_combiner
	movl	$unrollx2as_descr, %edx
	movl	$combine1, %esi
	movl	$unrollx2as_combine, %edi
	call	add_combiner
	movl	$combine7_descr, %edx
	movl	$combine1, %esi
	movl	$combine7, %edi
	call	add_combiner
	movl	$unroll3aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll3aa_combine, %edi
	call	add_combiner
	movl	$unroll4aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll4aa_combine, %edi
	call	add_combiner
	movl	$unroll5aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll5aa_combine, %edi
	call	add_combiner
	movl	$unroll6aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll6aa_combine, %edi
	call	add_combiner
	movl	$unroll7aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll7aa_combine, %edi
	call	add_combiner
	movl	$unroll8aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll8aa_combine, %edi
	call	add_combiner
	movl	$unroll9aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll9aa_combine, %edi
	call	add_combiner
	movl	$unroll10aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll10aa_combine, %edi
	call	add_combiner
	movl	$unroll12aa_descr, %edx
	movl	$combine1, %esi
	movl	$unroll12aa_combine, %edi
	call	add_combiner
	movl	$simd_v1_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v1_combine, %edi
	call	add_combiner
	movl	$simd_v2_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v2_combine, %edi
	call	add_combiner
	movl	$simd_v4_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v4_combine, %edi
	call	add_combiner
	movl	$simd_v8_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v8_combine, %edi
	call	add_combiner
	movl	$simd_v12_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v12_combine, %edi
	call	add_combiner
	movl	$simd_v2a_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v2a_combine, %edi
	call	add_combiner
	movl	$simd_v4a_descr, %edx
	movl	$combine1, %esi
	movl	$simd_v4a_combine, %edi
	call	add_combiner
	movl	$simd_v8a_combine, %edi
	movl	$simd_v8a_descr, %edx
	movl	$combine1, %esi
	call	add_combiner
	vmovsd	.LC0(%rip), %xmm1
	movl	$simd_v8a_combine, %edi
	vmovsd	.LC1(%rip), %xmm0
	addq	$8, %rsp
	jmp	log_combiner
simd_v8a_descr:
simd_v4a_descr:
simd_v2a_descr:
simd_v12_descr:
simd_v8_descr:
simd_v4_descr:
simd_v2_descr:
simd_v1_descr:
unroll12aa_descr:
unroll10aa_descr:
unroll9aa_descr:
unroll8aa_descr:
unroll7aa_descr:
unroll6aa_descr:
unroll5aa_descr:
unroll4aa_descr:
unroll3aa_descr:
combine7_descr:
unroll8x8_descr:
unroll8x4_descr:
unroll9x3_descr:
unroll8x2_descr:
unroll4x2as_descr:
unrollx2as_descr:
unroll10x10a_descr:
unroll9x9a_descr:
unroll8x8a_descr:
unroll7x7a_descr:
unroll6x6a_descr:
unroll5x5a_descr:
unroll12x12a_descr:
unroll12x6a_descr:
unroll8x4a_descr:
unroll4x4a_descr:
unroll3x3a_descr:
unroll8x2a_descr:
unroll4x2a_descr:
combine6_descr:
unroll16_descr:
unroll8_descr:
unroll4_descr:
unroll3_descr:
unroll2_descr:
unroll16a_descr:
unroll10a_descr:
unroll9a_descr:
unroll8a_descr:
unroll7a_descr:
unroll6a_descr:
unroll5a_descr:
unroll4a_descr:
unroll2aw_descr:
combine5p_descr:
unroll3a_descr:
combine5_descr:
combine4p_descr:
combine4b_descr:
combine4_descr:
combine3w_descr:
combine3_descr:
combine2_descr:
combine1_descr:
