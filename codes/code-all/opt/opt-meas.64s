.Ltext0:
aprod:
	leaq	-2(%rsi), %rdx
	testq	%rdx, %rdx
	jle	.L6
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L3:
	vmulsd	(%rdi,%rax,8), %xmm0, %xmm0
	vmulsd	8(%rdi,%rax,8), %xmm0, %xmm0
	vmulsd	16(%rdi,%rax,8), %xmm0, %xmm0
	addq	$3, %rax
	cmpq	%rdx, %rax
	jl	.L3
	jmp	.L2
.L6:
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L2:
	cmpq	%rax, %rsi
	jle	.L4
.L5:
	vmulsd	(%rdi,%rax,8), %xmm0, %xmm0
	addq	$1, %rax
	cmpq	%rsi, %rax
	jne	.L5
.L4:
	rep; ret
aprod1:
	testq	%rsi, %rsi
	jle	.L10
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L9:
	vmulsd	(%rdi,%rax,8), %xmm0, %xmm0
	vmulsd	8(%rdi,%rax,8), %xmm0, %xmm0
	vmulsd	16(%rdi,%rax,8), %xmm0, %xmm0
	addq	$3, %rax
	cmpq	%rax, %rsi
	jg	.L9
	rep; ret
.L10:
	vmovsd	.LC0(%rip), %xmm0
	ret

t1:
	movq	%rdi, %rsi
	movl	$a, %edi
	call	aprod1
	vmovsd	%xmm0, sink(%rip)
	ret

aprod2:
	testq	%rsi, %rsi
	jle	.L15
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L14:
	vmovsd	(%rdi,%rax,8), %xmm1
	vmulsd	8(%rdi,%rax,8), %xmm1, %xmm1
	vmulsd	%xmm0, %xmm1, %xmm0
	vmulsd	16(%rdi,%rax,8), %xmm0, %xmm0
	addq	$3, %rax
	cmpq	%rax, %rsi
	jg	.L14
	rep; ret
.L15:
	vmovsd	.LC0(%rip), %xmm0
	ret

t2:
	movq	%rdi, %rsi
	movl	$a, %edi
	call	aprod2
	vmovsd	%xmm0, sink(%rip)
	ret

aprod3:
	testq	%rsi, %rsi
	jle	.L20
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L19:
	vmovsd	(%rdi,%rax,8), %xmm1
	vmulsd	8(%rdi,%rax,8), %xmm1, %xmm1
	vmulsd	16(%rdi,%rax,8), %xmm1, %xmm1
	vmulsd	%xmm1, %xmm0, %xmm0
	addq	$3, %rax
	cmpq	%rax, %rsi
	jg	.L19
	rep; ret
.L20:
	vmovsd	.LC0(%rip), %xmm0
	ret

t3:
	movq	%rdi, %rsi
	movl	$a, %edi
	call	aprod3
	vmovsd	%xmm0, sink(%rip)
	ret

aprod4:
	testq	%rsi, %rsi
	jle	.L25
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L24:
	vmovsd	8(%rdi,%rax,8), %xmm1
	vmulsd	16(%rdi,%rax,8), %xmm1, %xmm1
	vmulsd	(%rdi,%rax,8), %xmm1, %xmm1
	vmulsd	%xmm1, %xmm0, %xmm0
	addq	$3, %rax
	cmpq	%rax, %rsi
	jg	.L24
	rep; ret
.L25:
	vmovsd	.LC0(%rip), %xmm0
	ret

t4:
	movq	%rdi, %rsi
	movl	$a, %edi
	call	aprod4
	vmovsd	%xmm0, sink(%rip)
	ret

aprod5:
	testq	%rsi, %rsi
	jle	.L30
	vmovsd	.LC0(%rip), %xmm0
	movl	$0, %eax
.L29:
	vmulsd	(%rdi,%rax,8), %xmm0, %xmm0
	vmovsd	8(%rdi,%rax,8), %xmm1
	vmulsd	16(%rdi,%rax,8), %xmm1, %xmm1
	vmulsd	%xmm1, %xmm0, %xmm0
	addq	$3, %rax
	cmpq	%rax, %rsi
	jg	.L29
	rep; ret
.L30:
	vmovsd	.LC0(%rip), %xmm0
	ret

t5:
	movq	%rdi, %rsi
	movl	$a, %edi
	call	aprod5
	vmovsd	%xmm0, sink(%rip)
	ret

main:
	subq	$8, %rsp
	movl	$1024, %esi
	movl	$t1, %edi
	call	find_cpe
	movl	$.LC1, %esi
	movl	$1, %edi
	movl	$1, %eax
	call	__printf_chk
	movl	$1024, %esi
	movl	$t2, %edi
	call	find_cpe
	movl	$.LC2, %esi
	movl	$1, %edi
	movl	$1, %eax
	call	__printf_chk
	movl	$1024, %esi
	movl	$t3, %edi
	call	find_cpe
	movl	$.LC3, %esi
	movl	$1, %edi
	movl	$1, %eax
	call	__printf_chk
	movl	$1024, %esi
	movl	$t4, %edi
	call	find_cpe
	movl	$.LC4, %esi
	movl	$1, %edi
	movl	$1, %eax
	call	__printf_chk
	movl	$1024, %esi
	movl	$t5, %edi
	call	find_cpe
	movl	$.LC5, %esi
	movl	$1, %edi
	movl	$1, %eax
	call	__printf_chk
	movl	$0, %eax
	addq	$8, %rsp
	ret

sink:
.Letext0:
.Ldebug_info0:
.Ldebug_abbrev0:
.Ldebug_loc0:
.Ldebug_line0:
